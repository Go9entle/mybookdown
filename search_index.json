[["index.html", "GOGENTLE’s NOTEBOOK 写在前面 为什么要写课程笔记", " GOGENTLE’s NOTEBOOK gogentle 2025-03-06 写在前面 R软件的bookdown扩展包是R Markdown的增强版， 支持自动目录、文献索引、公式编号与引用、定理编号与引用、图表自动编号与引用等功能， 可以作为LaTeX的一种替代解决方案， 在制作用R进行数据分析建模的技术报告时， 可以将报告文字、R程序、文字性结果、表格、图形都自动地融合在最后形成的网页或者PDF文件中。 Bookdown使用的设置比较复杂， 对初学者不够友好。 这里制作了一些模板， 用户只要解压缩打包的文件， 对某个模板进行修改填充就可以变成自己的中文图书或者论文。 Bookdown的详细用法参见https://bookdown.org/yihui/bookdown/， 本笔记的部署参考了北京大学李东风老师的的《统计软件教程》， 将bookdown部署到GitHub page上参考了R沟通|部署 bookdown 文件到 GitHub 上 需要注意的是李老师文件中的日期为中文格式，在部署到GitHub Pages上会报错，建议改为”YYYY-MM-DD”形式或者2025-03-06。 Code bookdown::render_book(&quot;index.Rmd&quot;, output_format=&quot;bookdown::gitbook&quot;, encoding=&quot;UTF-8&quot;) 上述命令编译为gitbook页面。 Code bookdown::publish_book() 上述命令将电子书提交到bookdown,详见谢益辉,前提是要将RStudio连接到posit账户，详见posit的connect文档，也不要忘记在YAML块加入site: bookdown::bookdown_site(并不确定这个是否有用，参考了这里)。 Bookdown如果输出为网页， 其中的数学公式需要MathJax程序库的支持， 用如下数学公式测试浏览器中数学公式显示是否正常： \\[ \\text{定积分} = \\int_a^b f(x) \\,dx \\] 如果显示不正常， 可以在公式上右键单击， 选择“Math Settings–Math Renderer”， 依次使用改成“Common HTML”，“SVG”等是否可以变成正常显示。 本篇笔记一直在慢慢编写当中，且前期一些语法不够规范导致公式显示不正常，正在随缘修改中。另外发现bookdown生成的网页支持公式的长度有限，所以有些公式可能无法全部展示。 定理能否成功显示？？ 定理0.1 (Pythagorean theorem) For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] 定理0.2 (Pythagorean theorem2) For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] 为什么要写课程笔记 很遗憾本科期间没有养成多用电脑多上网的习惯，上个学期本来是新的开始却也没有好好听讲，导致没什么收获。这个学期希望通过做笔记的形式多学一点东西，虽然可能也不会有多少收获，聊以填充这略显枯燥的研究生生活吧！ "],["关于涛哥.html", "关于涛哥", " 关于涛哥 gogentle，As known as 涛哥, 9ZT, GZT…存在于21世纪地球村的一个人类，拥有中华人民共和国国籍，出生并成长于山东省日照市东港区。 幼儿园毕业于日照三小幼儿园（2005-2008）中班留级一年； 小学毕业于山东省日照市实验小学（2008-2014）； 初中毕业于山东省日照市北京路中学（2014-2017）； 高中毕业于山东省日照实验高级中学（2017-2020）； 本科毕业于山东大学数学学院（2020-2024）， 现就读于中国人民大学统计学院，是一名统计学硕士生（2024-） Email: gzt0912@foxmail.com Blog: https://go9entle.github.io(已经许久未更新) GitHub: https://github.com/Go9entle "],["rlforfin.html", "1 强化学习在金融中的应用 1.1 Markov过程 1.2 Markov决策过程 1.3 动态规划算法 1.4 动态资产配置和消费", " 1 强化学习在金融中的应用 1.1 Markov过程 本书的主题是“序列不确定下的序列决策”，在本章中将暂时忽略“序列决策”方面而只关注”序列不确定性“。 1.1.1 过程中的状态概念 \\(S_t\\)是过程在时间\\(t\\)时的状态。特别地，我们对于下一时刻的状态\\(S_{t+1}\\)的概率感兴趣，如果已知现在的状态\\(S_t\\)和过去的状态\\(S_0,S_1,...,S_{t-1}\\)，我们对\\(P\\{S_{t+1}|S_t,S_{t-1},...,S_0\\}\\)感兴趣。 1.1.2 通过股票价格的例子理解Markov性 为了帮助理解，我们假设股票价格只取整数值，并且零或负股票价格是可以接受的。我们将时间\\(t\\)的股票价格表示为\\(X_t\\).假设从时间\\(t\\) 到下一个时间步骤 \\(t + 1\\),股票价格可以上涨\\(1\\)或下跌\\(1\\),即\\(X_{t+1}\\)的唯一两个结果是\\(X_t + 1\\)或\\(X_t − 1\\).要了解股票价格随时间的随机演变，我们只需要量化上涨的概率 \\(P[X_{t+1} =X_t+ 1]\\).我们将考虑股票价格演变的 3 个不同过程。 \\(P[X_{t+1}=X_t+1]=\\frac{1}{1+e^{-\\alpha_1(L-X_t)}}\\). 这意味着股票的价格倾向于均值回归(mean-reverting),均值即为参考水平\\(L,\\)拉力系数为\\(\\alpha.\\) 我们不妨设\\(S_t=X_t,\\)且可以看到下一时刻的状态\\(S_{t+1}\\)只与\\(S_t\\)有关而与\\(S_0,S_1,...,S_{t-1}\\)无关。即可写作 \\[ P[S_{t+1}|S_t,S_{t-1},...,S_0]=P[S_{t+1}|S_t]\\text{ for all }t\\geq 0. \\] 这就被成为Markov性。 书中还给出了相应的代码 Code from dataclasses import dataclass import numpy as np @dataclass class Process1: @dataclass class State: price: int level_param: int # level to which price mean-reverts alpha1: float = 0.25 # strength of mean-reversion (non-negative value) def up_prob(self, state: State) -&gt; float: return 1./(1+np.exp(-self.alpha1*(self.level_param-state.price))) def next_state(self, state:State) -&gt; State: up_move: int = np.random.binomial(1, self.up_prob(state),1)[0] #生成随机移动 up_move = 0 or 1 return Process1.State(price=state.price + up_move * 2 - 1) # 若up_move = 1, 则价格上升1，若为0价格下降1 接下来，我们使用 Python 的生成器功能（使用yield）编写一个简单的模拟器，如下所示： Code def simulation(process, start_state): state = start_state while True: yield state state = process.next_state(state) 现在我们可以使用此模拟器函数生成采样轨迹。在下面的代码中，我们从 start_price的价格\\(X_0\\)​开始，在time_steps时间步长内生成num_traces个采样轨迹。使用 Python 的生成器功能，我们可以使用itertools.islice函数“懒惰地”执行此操作。 Code import itertools def process1_price_traces( start_price: int, level_param: int, alpha1: float, time_steps: int, num_traces: int ) -&gt; np.ndarray: process = Process1(level_param=level_param, alpha1=alpha1) start_state = Process1.State(price=start.price) return np.vstack([ np.fromiter((s.price for s in itertools.islice( simulation(process, start_state), time_steps + 1 )), float) for _ in range(num_traces)]) \\[ P[X_{t+1}=X_t+1]=\\begin{cases} 0.5(1-\\alpha_2(X_t-X_{t-1}))&amp;\\text{ if }t&gt;0\\\\ 0.5&amp;\\text{ if }t=0 \\end{cases} \\] 其中\\(\\alpha_2\\)是一个“拉力强度”参数，取值在\\([0,1]\\)之间。这里的直觉时下一步的移动的方向偏向于前一次移动的反方向。我们注意到如果依然按照前文建模则无法满足Markov性质，因为\\(X_{t+1}\\)取值的概率不仅依赖于\\(X_t,\\)还依赖于\\(X_{t-1}.\\)不过我们可以在这里做一个小技巧，即创建一个扩展状态\\(S_t\\)由一对\\((X_t,X_{t-1})\\)组成。当\\(t=0\\)时状态\\(S_0\\)可以取值\\((X_0,null)\\),这里的null只是一个符号。通过将状态\\(S_t\\)视为\\((X_t,X_t-X_{t-1})\\)建模可以发现Markov性质得到了满足。 \\[ \\begin{aligned} &amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1}),...,(X_0,null)]\\\\ =&amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1})]\\\\ =&amp;0.5(1-\\alpha_2(X_{t+1}-X_t)(X_t-X_{t-1})) \\end{aligned} \\] 关于上面的式子deepseek给出了证明。 人们自然会想知道，为什么状态不单单由\\(X_t - X_{t-1}\\) 组成——换句话说，为什么 \\(X_t\\) 也需要作为状态的一部分。确实，单独知道\\(X_t - X_{t-1}\\)可以完全确定 \\(X_{t+1}-X_t\\)的概率。因此，如果我们将状态设定为在任意时间步 \\(t\\)仅为 \\(X_t - X_{t-1}\\)，那么我们确实会得到一个只有两个状态 +1 和 -1 的马尔可夫过程（它们之间的概率转移）。然而，这个简单的马尔可夫过程并不能通过查看时间\\(t\\)的状态 \\(X_t - X_{t-1}\\) 来告诉我们股票价格 \\(X_t\\) 的值。在这个应用中，我们不仅关心马尔可夫状态转移概率，还关心从时间 \\(t\\) 的状态中获取任意时间 \\(t\\) 的股票价格信息。因此，我们将状态建模为对 \\(( X_t, X_{t-1} )\\)。 请注意，如果我们将状态 \\(S_t\\) 建模为整个股票价格历史 \\(( X_0, X_1,..., X_t )，\\)那么马尔可夫性质将显然得到满足，将\\(S_t\\)建模为对\\((X_t,X_{t-1})\\)Markov性质也会得到满足。然而，我们选择 \\(S_t := (X_t, X_t - X_{t-1})\\) 是“最简单”的内部表示。实际上，在整本书中，我们对各种过程建模状态的努力是确保马尔可夫性质，同时使用“最简单/最小”的状态表示。 Process3是Process2的扩展，其中下一个移动的概率不仅依赖于上一时刻的移动还依赖于过去所有的移动。具体来说，它依赖于过去上涨次数的数量记为\\(U_t=\\sum_{i=1}^t\\max(X_i-X_{i-1},0)\\),与过去下跌次数的数量，记为\\(D_t=\\sum_{i=1}^t\\max(X_{i-1}-X_i,0)\\)之间的关系。表示为 \\[ P[X_{t+1}=X_t+1]=\\begin{cases} \\frac{1}{1+(\\frac{U_t+D_t}{D_t}-1)^{\\alpha_3}}&amp;\\text{ if }t&gt;0\\\\ 0.5&amp;\\text{ if }t=0 \\end{cases} \\] 其中\\(\\alpha_3\\in\\mathbb{R}_{\\geq0}\\)是一个拉力强度参数，将上述概率表达式视为\\(f(\\frac{D_t}{U_t+D_t};\\alpha_3)\\)其中\\(f:[0,1]\\rightarrow[0,1]\\)是一个sigmoid型函数 \\[ f(x;\\alpha)=\\frac{1}{1+(\\frac{1}{x}-1)^{\\alpha}}. \\] 下一个上涨移动的概率基本依赖\\(\\frac{U_t}{U_t+D_t}\\)即过去时间步中下跌次数的比例。因此，如果历史上的下跌次数大于上涨次数，那么下一个价格移动\\(X_{t+1}-X_t\\)将会有更多的向上拉力，反之亦然。 我们将\\(S_t\\)建模为由对\\((U_t,D_t)\\)组成，这样\\(S_t\\)的Markov性质可以得到满足 \\[ \\begin{aligned} &amp;P[(U_{t+1},D_{t+1})|(U_t,D_t),...,(U_0,D_0)]=P[(U_{t+1},D_{t+1})|(U_t,D_t)]\\\\ &amp;=\\begin{cases} f(\\frac{D_t}{U_t+D_t};\\alpha_3)&amp;\\text{ if }U_{t+1}=U_t+1,D_{t+1}=D_t\\\\ f(\\frac{U_t}{U_t+D_t};\\alpha_3)&amp;\\text{ if }U_{t+1}=U_t,D_{t+1}=D_t+1 \\end{cases} \\end{aligned} \\] 重要的是与前面两个过程不同，股票价格\\(X_t\\)实际上并不是过程3中状态\\(S_t\\)的一部分，这是因为\\(U_t,D_t\\)共同包含了捕捉\\(X_t\\)的足够信息，因为\\(X_t=X_0+U_t-D_t.\\) 1.1.3 Markov过程的正式定义 书中的定义和定理将由限制在离散时间和可数状态集合。 Def 3.3.1 Markov过程由以下组成 一个可数状态集合\\(\\mathcal{S}\\)（称为状态空间）和一个子集\\(\\mathcal{T}\\subset \\mathcal{S}\\)​（称为终止状态集合）。 一个时间索引的随即状态序列\\(S_t\\in S,\\)时间步为\\(t=0,1,2,...\\),每个状态转移都满足Markov性质:\\(P[S_{t+1}|S_t,...,S_0]=P[S_{t+1}|S_t],\\text{for all }t\\geq0.\\) 终止：如果某个时间步\\(T\\)的结果\\(S_T\\)是集合\\(\\mathcal{T}\\)中的一个状态，则该序列的结果在时间步\\(T\\)终止。 将\\(P[S_{t+1}|S_t]\\)称为时间\\(t\\)的转移概率。 Def 3.3.2 一个时间齐次Markov过程是一个Markov过程且\\(P[S_{t+1}|S_t]\\)与\\(t\\)无关。 这意味着时间齐次Markov过程的动态可以通过下面的函数完全指定： \\[ P:(\\mathcal{S}-\\mathcal{T})\\times\\mathcal{S}\\rightarrow[0,1] \\] 定义为\\(P(s&#39;,s)=P[S_{t+1}=s&#39;|S_t=s]\\)使得\\(\\sum_{s&#39;\\in S}P(s,s&#39;)=1,\\text{for all}s\\in\\mathcal{S-T}.\\)​ 注意上述规范中\\(P\\)的参数没有时间索引\\(t\\)（因此称为时间齐次）。此外注意到一个非时间齐次的Markov过程可以通过将所有状态和时间索引\\(t\\)来结合转换为齐次Markov过程。这意味着如果一个非时间齐次的Markov过程的原始状态空间是\\(\\mathcal{S}\\)，那么对应的时间齐次Markov过程的状态空间是\\(\\mathbb{Z}_{\\geq0}\\times\\mathcal{S}.\\) 1.1.4 Markov过程的稳态分布 Def 3.7.1 对于状态空间\\(\\mathcal{S}=\\mathbb{N}\\)的离散、时间齐次的Markov过程及其转移概率函数\\(P:\\mathbb{N}\\times\\mathbb{N}\\rightarrow [0,1]\\),稳态分布是一个概率分布函数\\(\\pi:\\mathbb{N}\\rightarrow [0,1]\\),满足 \\[ \\pi(s&#39;)=\\sum_{s\\in\\mathbb{N}}\\pi(s)\\cdot P(s,s&#39;),\\text{ for all }s&#39;\\in\\mathbb{N} \\] 稳态分布\\(\\pi\\)的直观理解是，在特定条件下如果我们让Markov过程无限运行，那么在长期内，状态在特定步出现频率（概率）由分布\\(\\pi\\)给出，该分布与时间步无关。 如果将稳态分布的定义专门化为有限状态、离散时间、时间齐次的Markov过程，状态空间为\\(S=\\{s_1,...,s_2\\}=\\mathbb{N},\\)那么我们可以将稳态分布\\(\\pi\\)​表示为 \\[ \\pi(s_j)=\\sum_{i=1}^n\\pi(s_i)\\cdot P(s_i,s_j),\\text{ for al }j=1,2,...,n \\] 下面使用粗体符号表示向量和矩阵。故\\(\\boldsymbol{\\pi}\\)是一个长度为\\(n\\)的列向量，\\(\\boldsymbol{\\mathcal{P}}\\)是\\(n\\times n\\)的转移概率矩阵，其中行是原状态，列为目标状态，每行的和为1。那么上述定义的表述就可以简洁地表示为： \\[ \\boldsymbol{\\pi}^T=\\boldsymbol{\\pi}^T\\cdot\\boldsymbol{\\mathcal{P}},\\text{ or }\\boldsymbol{\\mathcal{P}}^T\\cdot\\boldsymbol{\\pi}=\\boldsymbol{\\pi} \\] 后一个式子可以说明\\(\\boldsymbol{\\pi}\\)是矩阵\\(\\boldsymbol{\\mathcal{P}}\\)​的特征值为1对应的特征向量。 1.1.5 Markov奖励过程的形式主义 我们之所以讲述Markov过程是因为希望通过为Markov过程添加增量特性来逐步进入Markov决策过程，也就是强化学习的算法框架。现在开始讲述介于二者之间的中间框架即Markov奖励过程。基本上我们只是为每次从一个状态转移到下一个状态时引入一个数值奖励的概念。这些奖励是随机的，我们需要做的就是在进行状态转移时指定这些奖励的概率分布。 Markov奖励过程的主要目的是计算如果让过程无限运行（期望从每个非终止状态获得的奖励总和）我们将累积多少奖励，考虑到未来的奖励需要适当地折现。 Def 3.8.1 Markov奖励过程是一个Markov过程以及一个时间索引序列的奖励随机变量\\(R_t\\in\\mathcal{D},\\mathcal{D}\\)是\\(\\mathbb{R}\\)中一个可数子集，\\(t=1,2,...,\\)满足Markov性质： \\[ P[(R_{t+1},S_{t+1})|S_{t},S_{t-1},...,S_0]=P[(R_{t+1},S_{t+1})|S_t]\\text{ for all }t\\geq0 \\] 我们将\\(P[(R_{t+1},S_{t+1})|S_t]\\)称为Markov Reward Process在时间\\(t\\)地转移概率。由于我们通常假设Markov的时间齐次性，我们将假设MRP具有时间齐次性，即\\(P[(R_{t+1},S_{t+1})|S_t]\\)与\\(t\\)​无关。 由时间齐次性的假设，MRP的转移概率可以表示为转移概率函数 \\[ \\mathcal{P}_R:\\mathcal{N\\times D\\times S}\\rightarrow[0,1] \\] 定义为 $$ \\[\\begin{aligned} &amp;\\mathcal{P}_R(s,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|S_t=s]\\text{ for }t=0,1,2,...,\\\\ &amp;\\text{for all }s\\in\\mathcal{N},r\\in\\mathcal{D},s&#39;\\in\\mathcal{S},\\text{ s.t. }\\sum_{s&#39;\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,r,s&#39;)=1,\\text{ for all }s\\in \\mathcal{N} \\end{aligned}\\] $$ 当涉及模拟时我们需要单独指定起始状态的概率分布。 现在可以扩展更多理论。给定奖励转移函数\\(\\mathcal{P}_R\\)，我们可以得到 隐式Markov过程的概率转移函数\\(P:\\mathbb{N}\\times S\\rightarrow [0,1]\\)可以定义为 \\[ \\mathcal{P}(s,s&#39;)=\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,r,s&#39;) \\] 奖励转移函数\\(\\mathcal{R}_T:\\mathcal{N\\times S}\\rightarrow \\mathbb{R}\\)定义为 \\[ \\mathcal{R}_T(s,s&#39;)=\\mathbb{E}[R_{t+1}|S_{t+1}=s&#39;,S_t=s]=\\sum_{r\\in\\mathcal{D}}\\frac{\\mathcal{P}_R(s,r,s&#39;)}{\\mathcal{P}(s,s&#39;)}=\\sum_{r\\in\\mathcal{D}}\\frac{\\mathcal{P}_R(s,r,s&#39;)}{\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,r,s&#39;)}\\cdot r \\] 我们在实践中遇到的大多数MRP奖励规范可以直接表示为奖励转移函数\\(\\mathcal{R}_T\\).最后我们想强调的是，可以将\\(\\mathcal{P}_R\\)或\\(\\mathcal{R}_T\\)转换为一种更紧凑的奖励函数。该函数足以执行涉及MRP的关键计算，这个奖励函数\\(\\mathcal{R}:\\mathcal{N}\\rightarrow \\mathbb{R}\\)定义为 \\[ \\mathcal{R}(s)=\\mathbb{E}[R_{t+1}|S_t=s]=\\sum_{s&#39;\\in\\mathcal{S}}\\mathcal{P}(s,s&#39;)\\cdot\\mathcal{R}_T(s,s&#39;)=\\sum_{s&#39;\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,r,s&#39;)\\cdot r \\] 1.1.6 Markov奖励过程的价值函数 现在，我们准备正式定义涉及MRP的主要问题，我们希望计算从任何非终止状态出发的“期望累积奖励”。允许在奖励累积时使用贴现因子\\(\\gamma\\in[0,1]\\),我们将回报\\(G_t\\)定义为时间\\(t\\)之后的“未来奖励的贴现累积”。形式上： \\[ G_t=\\sum_{i=t+1}^\\infty \\gamma^{i-t-1}\\cdot R_i=R_{t+1}+\\gamma \\cdot R_{t+2}+\\gamma^2\\cdot R_{t+3}+.... \\] 即使对于终止序列（例如\\(t=T\\)时终止，即\\(S_T\\in\\mathcal{T}\\)）我们只需将\\(i&gt;T\\)的\\(R_i=0\\). 我们希望识别具有较大期望回报的非终止状态和具有较小期望回报的非终止状态。事实上，这是涉及MRP的主要问题——计算MRP中每个非终止状态的期望回报。形式上，我们感兴趣的是计算价值函数： \\[ V:\\mathcal{N}\\rightarrow\\mathbb{R} \\] 定义为 \\[ V(s)=\\mathbb{E}[G_t|S_t=s]\\text{ for all }s\\in\\mathcal{N},\\text{ for all }t=0,1,2,... \\] 贝尔曼指出价值函数具有递归结构，具体来说 \\[\\begin{align} V(s)=&amp;\\mathbb{E}[R_{t+1}|S_t=s]+\\gamma\\cdot\\mathbb{E}[R_{t+2}|S_t=s]+...\\\\ =&amp;\\mathcal{R}(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\\cdot\\mathbb{E}[R_{t+2}|S_{t+1}=s&#39;]\\\\ &amp;+\\gamma^2\\sum_{s&#39;\\in\\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\\sum_{s&#39;&#39;\\in\\mathcal{N}}P[S_{t+2}=s&#39;&#39;|S_{t+1}=s&#39;]\\cdot\\mathbb{E}[R_{t+3}|S_{t+2}=s&#39;&#39;]\\\\ &amp;+...\\\\ =&amp;\\mathcal{R}(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,s&#39;)\\cdot\\mathcal{R}(s&#39;)+\\gamma^2\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,s&#39;)\\sum_{s&#39;&#39;\\in\\mathcal{N}}\\mathcal{P}(s&#39;,s&#39;&#39;)\\mathcal{R}(s&#39;&#39;)+...\\\\ =&amp;\\mathcal{R}(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,s&#39;)\\cdot(R(s&#39;)+\\gamma\\cdot\\sum_{s&#39;&#39;\\in\\mathcal{N}}\\mathcal{P}(s&#39;,s&#39;&#39;)\\cdot\\mathcal{R}(s&#39;&#39;)+...)\\\\ =&amp;\\mathcal{R}(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,s&#39;)\\cdot V(s&#39;) \\text{ for all }s\\in\\mathcal{N} \\tag{1.1} \\end{align}\\] 我们将这个价值函数的递归方程@ref(eq:bellman)称为Markov奖励过程的Bellman方程。 1.2 Markov决策过程 1.2.1 不确定性下的序列决策难题 通常，MDP具有两个截然不同且相互依赖的高级特征 1. 在每个时间步\\(t,\\)观察到状态\\(S_t\\)后，从指定的动作集合中选择一个动作\\(A_t.\\) 2. 给定观察到的状态\\(S_t\\)和执行的动作\\(A_t,\\)下一个时间步的状态\\(S_{t+1}\\)和奖励\\(R_{t+1}\\)的概率通常不仅取决于状态\\(S_t\\)还取决于动作\\(A_t.\\) 我们的任务是最大化每个状态的期望回报（即最大化价值函数）。在一般情况下，这似乎是一个非常困难的问题，因为存在循环的相互作用。一方面，动作依赖于状态；另一方面，下一个状态/奖励的概率依赖于动作和状态。此外，动作可能会对奖励产生延迟影响，如何区分不同时间步的动作对未来奖励的影响也是一个挑战。如果没有动作和奖励之间的直接对应关系，我们如何控制动作以最大化期望累积奖励？为了回答这个问题，我们需要建立一些符号和理论。在我们正式定义马尔可夫决策过程框架及其相关（优雅的）理论之前，让我们先设定一些术语。 人工智能视角下的MDP 使用人工智能的语言，我们说在每个时间步\\(t,\\)智能体（Agent）（我们设计的算法）观察到状态\\(S_t,\\)然后智能体执行动作\\(A_t,\\)之后环境（Environment）（在看到\\(S_t,A_t\\)后）生成一个随机对\\((S_{t+1},R_{t+1})\\)。接着，智能体观察到下一个状态\\(S_{t+1}\\)，循环重复直到达到终止状态。这种循环相互作用如图1.1所示 Code knitr::include_graphics(&quot;https://Go9entle.github.io/picx-images-hosting/image.3yekzoaheh.webp&quot;) 图1.1: Markov Decision Process 1.2.2 Markov决策过程的正式定义 与Markov过程和Markov奖励过程的定义类似，为了便于阐述，以下Markov决策过程的定义和理论将针对离散时间、可数状态空间和可数的下一个状态与奖励转移对。 定理 Markov决策过程包括以下内容： 可数状态集合\\(\\mathcal{S}\\)（称为状态空间），终止状态集合\\(\\mathcal{T}\\subset \\mathcal{S}\\),以及可数动作集合\\(\\mathcal{A}\\)（称为动作空间）。 时间索引的环境生成随机状态序列\\(S_t\\in\\mathcal{S}\\)（时间步\\(t=0,1,2,...\\)），时间索引的环境生成奖励随机变量序列\\(R_t\\in\\mathcal{D}\\)（\\(\\mathcal{D}\\)是\\(\\mathbb{R}\\)的可数子集），以及时间索引的智能体可控动作序列\\(A_t\\in\\mathcal{A}\\) Markov性 \\[\\begin{equation*} P[(R_{t+1},S_{t+1})|(S_t,A_t,S_{t-1},A_{t-1},...,S_0,A_0)]=P[ (R_{t+1},S_{t+1})|(S_t,A_t)]\\text{ for all }t\\geq 0 \\end{equation*}\\] 终止：如果某个时间步\\(T\\)的状态\\(S_T\\in\\mathcal{T}\\),则该序列结果在时间步\\(T\\)终止。 在更一般的情况下，如果状态或奖励是不可数的，相同的概念仍然适用，只是数学形式需要更加详细和谨慎。具体来说，我们将使用积分代替求和，使用概率密度函数（用于连续概率分布）代替概率质量函数（用于离散概率分布）。为了符号的简洁性，更重要的是为了核心概念的理解（而不被繁重的数学形式分散注意力），我们选择默认使用离散时间、可数\\(\\mathcal{S}\\)、可数\\(\\mathcal{A}\\)和可数\\(\\mathcal{D}\\). 我们将\\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\\)称为Markov决策过程在时间\\(t\\)的转移概率。 与Markov过程和MRP一样，我们默认Markov决策过程是时间其次的，即\\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\\)与\\(t\\)无关。这意味着Markov决策过程的转移概率在最一般情况下可以表示为状态-奖励转移概率函数： \\[ \\mathcal{P}_R:\\mathcal{N\\times A\\times D\\times S}\\rightarrow [0,1] \\] 定义为 \\[ \\mathcal{P}_R(s,a,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|(S_t=s,A_t=a)] \\] 对于时间步\\(t=0,1,2,...\\)，对于所有的\\(s,s&#39;\\in\\mathcal{N},a\\in\\mathcal{A},r\\in\\mathcal{D}\\)满足 \\[ \\sum_{s&#39;\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;)=1\\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\] 这又可以通过状态-奖励转移概率函数\\(\\mathcal{P}_R\\)来表征，给定\\(\\mathcal{P}_R\\)的规范，我们可以构造 状态转移概率函数： \\[ \\mathcal{P}:\\mathcal{N\\times A\\times S}\\rightarrow [0,1] \\] 定义为 \\[ \\mathcal{P}(s,a,s&#39;)=\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;) \\] 奖励转移函数： \\[ \\mathcal{R}_T:\\mathcal{N\\times A\\times S}\\rightarrow \\mathbb{R} \\] 定义为 \\[\\begin{align} \\mathcal{R}_T(s,a,s&#39;)&amp;=\\mathbb{E}[R_{t+1}|(S_{t+1}=s&#39;,S_t=s,A_t=a)]\\\\ &amp;=\\sum_{r\\in\\mathcal{D}}\\frac{\\mathcal{P}_R(s,a,r,s&#39;)}{\\mathcal{P}(s,a,s&#39;)}\\cdot r\\\\ &amp;=\\sum_{r\\in\\mathcal{D}}\\frac{\\mathcal{P}_R(s,a,r,s&#39;)}{\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;)}\\cdot r \\end{align}\\] 在实践中，我们遇到的大多数Markov决策过程的奖励规范可以直接表示为奖励转移函数\\(\\mathcal{R}_T\\)而不是更一般的\\(\\mathcal{P}_R\\).最后我们想强调的是可以将\\(\\mathcal{P}_R\\)或\\(\\mathcal{R}_T\\)转换为“更紧凑”的奖励函数，该函数足以执行设计MDP的关键计算，这个奖励函数为： \\[ \\mathcal{R}:\\mathcal{N\\times A}\\rightarrow \\mathbb{R} \\] 定义为： \\[\\begin{aligned} \\mathcal{R}(s,a)&amp;=\\mathbb{E}[R_{t+1}|(S_t=s,A_t=a)]\\\\ &amp;=\\sum_{s\\in\\mathcal{S}}\\mathcal{P}(s,a,s&#39;)\\cdot\\mathcal{R}_T(s,a,s&#39;)\\\\ &amp;=\\sum_{s\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;)\\cdot r \\end{aligned}\\] 1.2.3 策略 理解了MDP的动态后，我们现在转向智能体动作的规范，即作为当前状态函数的动作选择。在一般情况下，我们假设智能体将根据当i请安状态\\(S_t\\)的概率分布执行动作\\(A_t\\),我们将此函数称为策略（Policy）。 形式上，策略是一个函数： \\[ \\pi:\\mathcal{N\\times A}\\rightarrow [0,1] \\] 定义为: \\[ \\pi(s,a)=P[A_t=a|S_t=s]\\text{ for }t=0,1,2,...\\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\] 使得 \\[ \\sum_{a\\in\\mathcal{A}}\\pi(s,a)=1\\text{ for all }s\\in\\mathcal{N} \\] 需要注意的是，上述定义假设策略是Markov的，即动作概率仅依赖于当前状态，而不依赖于历史状态。上述定义还假设策略是平稳的，即\\(P[A_t=a|S_t=s]\\)在时间\\(t\\)上是不变的。如果我们遇到策略需要依赖于时间\\(t\\)的情况，我们可以简单地将\\(t\\)包含在状态中，从而使策略变得平稳（尽管这会增加状态空间的规模，从而导致计算成本的增加）。 当策略对每个状态的动作概率集中在单个动作上（即只要到达一个状态，动作是确定的）时，我们称之为确定性策略。形式上，确定性策略\\(\\pi_D:\\mathcal{N}\\rightarrow \\mathcal{A}\\)具有以下性质：对所有的\\(s\\in\\mathcal{N},\\) \\[ \\pi(s,\\pi_D(s))=1\\text{ and }\\pi(s,a)=0, \\text{ for all }a\\ne \\pi_D(s) \\] 我们将非确定性的策略称为随机策略（随机反映了智能体将根据\\(\\pi\\)指定的概率分布执行随机动作的事实）。 1.2.4 [Markov决策过程，策略]:= Markov奖励过程 本节有一个重要的见解——如果我们用固定策略\\(\\pi\\)（通常是一个固定的随机策略，注意与确定性策略区分）评估MDP，我们会得到一个由MDP和策略\\(\\pi\\)共同隐含的MRP。我们可以用符号精确地澄清这一点，但首先MDP和MRP中存在一些符号冲突。我们使用 \\(\\mathcal{P}_R\\)表示MRP转移概率函数，同时也表示MDP的状态-奖励转移概率函数； \\(\\mathcal{P}\\)表示MRP中隐含的Markov过程的转移概率函数，同时也表示MDP的状态转移函数； \\(\\mathcal{R}_T\\)表示MRP的奖励转移函数，同时也表示MDP的奖励转移函数； \\(\\mathcal{R}\\)表示MRP的奖励函数，同时也表示MDP的奖励函数。 我们将在\\(\\pi\\)隐含的MRP的函数\\(\\mathcal{P}_R,\\mathcal{P},\\mathcal{R}_T,\\mathcal{R}\\)加上上标\\(\\pi\\)以区分这些函数MDP和\\(\\pi\\)隐含的MRP中的使用。 假设我们给定一个固定策略\\(\\pi\\)和一个由其状态-奖励转移概率函数\\(\\mathcal{P}_R\\)指定的MDP，那么由MDP与策略\\(\\pi\\)评估隐含的MRP的转移概率函数\\(\\mathcal{P}_R^\\pi\\)定义为： \\[ \\mathcal{P}_R^\\pi(s,r,s&#39;)=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{P}_R(s,a,r,s&#39;) \\] 类似地有 \\[\\begin{align} \\mathcal{P}^\\pi(s,s&#39;)&amp;=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{P}(s,a,s&#39;)\\\\ \\mathcal{R}_T^\\pi(s,s&#39;)&amp;=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{R}_T(s,a,s&#39;)\\\\ \\mathcal{R}^\\pi(s)&amp;=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{R}(s,a) \\end{align}\\] 因此，每当我们谈论用固定策略评估的 MDP时，你应该知道我们实际上是在谈论隐含的 MRP。 1.2.5 固定策略下的MDP价值函数 现在我们准备讨论用固定策略\\(\\pi\\)评估的MDP的价值函数（也称为MDP预测问题，“预测”指的是该问题涉及在智能体遵循特定策略时预测未来期望回报）。与MRP情况类似，我们定义 \\[ G_t=\\sum_{i=t+1}^\\infty \\gamma^{i-t-1}\\cdot R_i=R_{t+1}+\\gamma \\cdot R_{t+2}+\\gamma^2\\cdot R_{t+3}+.... \\] 其中\\(\\gamma\\in[0,1]\\)是指定的贴现因子。即便对于终止序列我们也使用上述回报的定义。 用固定策略\\(\\pi\\)评估的MDP的价值函数为 \\[ V^\\pi:\\mathcal{N}\\rightarrow \\mathbb{R} \\] 定义为： \\[ V^\\pi(s)=\\mathbb{E}_{\\pi,\\mathcal{P}_R}[G_t|S_t=s]\\text{ for all }s\\in\\mathcal{N},\\text{ for all }t=0,1,2,... \\] 我们假设每当我们讨论价值函数时，折扣因子\\(\\gamma\\)是适当的，以确保每个状态的期望回报是有限的——特别是对于可能发散的连续（非终止）MDP，\\(\\gamma&lt;1\\). 我们将\\(V^\\pi(s)=\\mathbb{E}_{\\pi,\\mathcal{P}_R}[G_t|S_t=s]\\)展开如下： \\[\\begin{align} &amp;\\mathbb{E}_{\\pi,\\mathcal{P}_R}[R_{t+1}|S_t=s]+\\gamma\\cdot\\mathbb{E}_{\\pi,\\mathcal{P}_R}[R_{t+2}|S_t=s]+\\gamma^2\\cdot\\mathbb{E}_{\\pi,\\mathcal{P}_R}[R_{t+3}|S_t=s]+...\\\\ =&amp;\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\sum_{a&#39;\\in\\mathcal{A}}\\pi(s&#39;,a&#39;)\\cdot\\mathcal{R}(s&#39;,a&#39;)\\\\ &amp;+\\gamma^2\\cdot\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a&#39;,s&#39;)\\sum_{a&#39;\\in\\mathcal{A}}\\pi(s&#39;,a&#39;)\\sum_{s&#39;&#39;\\in\\mathcal{N}}\\mathcal{P}(s&#39;,a&#39;&#39;,s&#39;&#39;)\\sum_{a&#39;&#39;\\in\\mathcal{A}}\\pi(s&#39;&#39;,a&#39;&#39;)\\cdot\\mathcal{R}(s&#39;&#39;,a&#39;&#39;)\\\\ &amp;+...\\\\ =&amp; \\mathcal{R}^\\pi(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}^\\pi(s,s&#39;)\\cdot\\mathcal{R}^\\pi(s&#39;)+\\gamma^2\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}^\\pi(s,s&#39;)\\sum_{s&#39;&#39;\\in\\mathcal{N}}\\mathcal{P}^\\pi(s&#39;,s&#39;&#39;)\\cdot\\mathcal{R}^\\pi(s&#39;&#39;)+... \\end{align}\\] 最后一个表达式等于\\(\\pi\\)隐含的MRP的状态\\(s\\)的价值函数。因此，用固定策略\\(\\pi\\)评估的MDP的价值函数\\(V^\\pi\\)与\\(\\pi\\)隐含的MRP的价值函数完全相同，因此我们可以将MRP的Bellman方程应用于\\(V^\\pi\\)，即 \\[\\begin{align} V^\\pi(s)=&amp;\\mathcal{R}^\\pi(s)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}^\\pi(s,s&#39;)\\cdot V^\\pi(s&#39;)\\\\ =&amp;\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot V^\\pi(s&#39;)\\\\ =&amp;\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot(\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot V^\\pi(s&#39;))\\text{ for all }s\\in\\mathcal{N} \\tag{1.2} \\end{align}\\] 对于状态空间不太大的有限MDP，方程 (1.2) 可以通过线性代数求解\\(V^\\pi\\)。更一般地，方程 (1.2) 将成为本书其余部分开发各种动态规划和强化学习算法以解决MDP预测问题的关键方程。 然而，另一个价值函数在开发MDP算法时也至关重要——它将（状态、动作）对映射到从该（状态，动作）对出发的期望回报，当用固定策略评估时。这被称为用固定策略评估的MDP地动作—价值函数： \\[ Q^\\pi:\\mathcal{N\\times A}\\rightarrow \\mathbb{R} \\] 定义为： \\[ Q^\\pi(s,a)=\\mathbb{E}_{\\pi,\\mathcal{P}_R}[G_t|(S_t=s,A_t=a)]\\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\text{ for all }t=0,1,2,... \\] 为了避免术语混淆，我们将\\(V^\\pi\\)称为策略\\(\\pi\\)的状态-价值函数（尽管通常简称为价值函数），以区别于动作-价值函数\\(Q^\\pi\\).解释\\(Q^\\pi(s,a)\\)的方式是，它是从给定非终止状态\\(s\\)出发，首先采取动作\\(a\\)，然后遵循策略\\(\\pi\\)的期望回报。通过这种解释，我们可以将\\(V^\\pi(s)\\)视作\\(Q^\\pi(s,a)\\)的“加权平均”（对于所有从非终止状态\\(s\\)出发的所有可能动作\\(a\\)）,权重等于给定状态\\(s\\)的动作\\(a\\)的概率（即\\(\\pi(s,a)\\)）。具体来说： \\[\\begin{equation} V^\\pi(s)=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot Q^\\pi(s,a)\\text{ for all }s\\in\\mathcal{N} \\tag{1.3} \\end{equation}\\] 将\\(Q^\\pi(s,a)\\)展开后得到 \\[\\begin{equation} Q^\\pi(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot V^\\pi(s&#39;) \\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\tag{1.4} \\end{equation}\\] 结合方程(1.3)和方程(1.4)我们得到 \\[\\begin{equation} Q^\\pi(s,a)=\\mathcal{R}(s,a)+\\gamma \\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\sum_{a&#39;\\in\\mathcal{A}}\\pi(s&#39;,a&#39;)\\cdot Q^\\pi(s&#39;,a&#39;)\\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\tag{1.5} \\end{equation}\\] 方程(1.2)被称为MDP状态-价值函数贝尔曼策略方程，方程(1.5)被称为MDP动作-价值函数贝尔曼策略方程。方程(1.2)、(1.3)、(1.4)和(1.5)统称为MDP贝尔曼策略方程。 1.2.6 最优价值函数和最优策略 最后，我们要达到Markov决策问题的主要目的——识别能够产生最有价值函数的策略，即从每个非终止状态出发的最佳可能期望回报。我们说，当我们识别出MDP的最优价值函数（及相关的最优策略，即产生最优价值函数的策略）时，MDP就被“解决”了。识别最优价值函数及其相关最优策略被称为MDP控制问题。“控制”指的是该问题涉及通过策略的迭代修改来引导动作，以推动价值函数向最优性发展。 形式上，最优价值函数定义为 \\[ V^*:\\mathcal{N}\\rightarrow \\mathbb{R} \\] 定义为 \\[ V^*(s)=\\max_{\\pi\\in\\Pi} V^\\pi(s) \\text{ for all }s\\in\\mathcal{N} \\] 其中\\(\\Pi\\)是\\(\\mathcal{N,A}\\)空间上的平稳随机策略集合。 上述定义的解释是，对于每个非终止状态\\(s\\)，我们考虑所有可能的随机平稳策略并在这些\\(\\pi\\)中选择最大化\\(V^\\pi(s).\\)需要注意的是，\\(\\pi\\)的选择是针对每个\\(s\\)单独进行的，因此可以想象，不同的\\(\\pi\\)的选择可能会为不同的\\(s\\in\\mathcal{N}\\)最大化\\(V^\\pi(s).\\)因此，从上述\\(V^*\\)的定义中，我们还不能谈论“最优策略”的概念。因此，现在让我们只关注上述定义的最有价值函数。 同样，最优动作-价值函数定义为 \\[ Q^*:\\mathcal{N\\times A}\\rightarrow \\mathbb{R} \\] 定义为 \\[ Q^*(s,a)=\\max_{\\pi\\in\\Pi} Q^\\pi(s,a) \\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\] \\(V^*\\)通常被称为最优状态-价值函数，以区别于最优动作-价值函数\\(Q^*\\)（尽管为了简洁，\\(V^*\\)通常也被简称为最优价值函数）。需要明确的是，最优价值函数默认情况下值得就是最优状态-价值函数\\(V^*\\). 正如固定策略的价值函数具有递归公式一样，贝尔曼指出我们可以为最优价值函数创建递归公式。让我们从展开给定非终止状态\\(s\\)的最优状态-价值函数\\(V^*(s)\\)开始——我们考虑从状态\\(s\\)出发可以采取的所有可能动作\\(a\\in\\mathcal{A},\\)并选择能够产生最佳动作-价值的动作\\(a,\\)即选择出能够产生最优\\(Q^*(s,a)\\)的动作\\(a\\)。形式上给出了以下方程 \\[ V^*(s)=\\max_{a\\in\\mathcal{A}}Q^*(s,a)\\text{ for all }s\\in\\mathcal{N} \\tag{1.6} \\] 同样让我们思考从给定非终止状态和动作对\\((s,a)\\)出发的最优性意味着什么，也就是展开\\(Q^*(s,a)\\).首先我们获得即时的期望奖励\\(\\mathcal{R}(s,a)\\).接下来，考虑所有可能的我们可以转到的状态\\(s&#39;\\in\\mathcal{S}\\)并从每个非终止状态\\(s&#39;\\)出发递归地采取最优动作。形式上，这给出了以下方程： \\[ Q^*(s,a)=\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot V^*(s&#39;)\\text{ for all }s\\in\\mathcal{N},a\\in\\mathcal{A} \\tag{1.7} \\] 将(1.7)中的\\(Q^*(s,a)\\)代入(1.6),可以得到 \\[ V^*(s)=\\max_{a\\in\\mathcal{A}}\\{ \\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot V^*(s&#39;) \\} \\text{ for all }s\\in\\mathcal{N} \\tag{1.8} \\] 方程(1.8)被称为MDP状态-价值函数贝尔曼最优性方程。 将方程(1.6)代入方程(1.7)可以得到 \\[ Q^*(s,a)=\\mathcal{R}(s,a)=\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot \\max_{a&#39;\\in\\mathcal{A}} Q^*(s&#39;,a&#39;)\\text{ for all }s\\in\\mathcal{A},a\\in\\mathcal{A} \\tag{1.9} \\] 方程(1.9)被称为MDP动作-价值函数贝尔曼最优性方程。 方程(1.8)、(1.6)、(1.7)和(1.9)统称为MDP贝尔曼最优性方程。我们应该强调，当有人说 MDP贝尔曼方程或简称为贝尔曼方程时，除非他们明确说明，否则他们指的是 MDP 贝尔曼最优性方程（通常是 MDP 状态-价值函数贝尔曼最优性方程）。这是因为 MDP 贝尔曼最优性方程解决了马尔可夫决策过程的最终目的——识别最优价值函数和实现最优价值函数的相关策略（即使我们能够解决 MDP 控制问题）。 我们需要强调的是，贝尔曼最优性方程并没有直接给出计算最优质函数或实现最优质函数的策略的具体方法——它们只是阐述了最优值函数的一个强大数学性质，这一性质帮助我们提出动态规划或者强化学习的算法来计算最优值函数及其相关的策略。 我们一直在使用“实现最优值函数的策略/策略组合”这个词，但我们还没有给出这样的策略的明确定义。事实上，正如之前提到的，从\\(V^*\\)的定义来看并不清楚是否存在这样的策略能实现\\(V^*\\)（因为可以设想不同的策略\\(\\pi\\)对于不同的状态\\(s\\in\\mathcal{N}\\)实现\\(V^\\pi(s)\\)最大化）。因此我们定义最优策略\\(\\pi^*:\\mathcal{N\\times A}\\rightarrow [0,1]\\)主导所有其他策略的策略，在价值函数上优于所有其他策略。形式化地说 \\[ \\pi^*\\in\\Pi\\text{ is an Optimal Policy if } V^{\\pi^*}(s)\\geq V^\\pi(s)\\text{ for all }\\pi\\in\\Pi \\text{ and for all states }s\\in\\mathcal{N}. \\] 最优策略\\(\\pi^*\\)的定义表明，它是一个“优于或等于”所有其他静态策略的策略，且适用于所有非终止状态（注意可能存在多个最优策略）。将这个定义与最优值函数\\(V^*\\)的定义结合，接下来的自然问题是：是否存在一个最优策略\\(\\pi^*\\),对所有的\\(s\\in\\mathcal{N}\\)最大化\\(V^\\pi(s)\\),也就是是否存在一个\\(\\pi^*\\)使得\\(V^*(s)=V^{\\pi^*}(s)\\)对所有\\(s\\in\\mathcal{N}.\\)下面的定理和证明是针对我们默认的MDP设置（离散时间、可数空间、时间齐次）的。 定理 对于任何（离散时间、可数空间、时间齐次）的MDP： 存在一个最优策略\\(\\pi^*\\in\\Pi\\). 所有最优策略都实现最优值函数。 所有最优策略实现最优动作-价值函数，即对于所有的\\(s\\in\\mathcal{N},a\\in\\mathcal{A},Q^{\\pi^*}(s,a)=Q^*(s,a)\\)对于所有的最优策略\\(\\pi^*\\). 定理的证明略去。 我们的确定性如下定义 \\[ \\pi^*_D(s)=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}} Q^*(s,a)\\text{ for all }s\\in\\mathcal{N} \\tag{1.10} \\] 方程(1.10)是一个关键构造，它与贝尔曼最优性方程紧密结合，在涉及各种动态规划和强化学习算法以解决MDP控制问题（即求解\\(V^*,Q^*,\\pi^*\\)）时起到重要作用。最后，值得注意的是，不像预测问题在小状态空间中有一个直接的线性代数求解器，控制问题是非线性的，因此没有类似的直接线性代数求解器。控制问题的最简单解法（即使是在小状态空间中）就是我们将在下一章中讨论的动态规划算法。 1.3 动态规划算法 1.3.1 Planning vs Learning 在本书中，我们将从人工智能的角度探讨预测与控制（并且我们将特别使用人工智能的术语）。我们将区分没有马尔可夫决策过程（MDP）环境模型的算法（没有访问概率转移（\\(\\mathcal{P}_R\\)）函数）与有马尔可夫决策过程环境模型的算法（意味着我们可以通过显式的概率分布表示或仅通过采样模型访问\\(\\mathcal{P}_R\\)）。前者（没有模型访问的算法）被称为学习算法，以反映人工智能代理需要与真实世界环境互动（例如，一个机器人学习在实际森林中导航）并从其通过与环境互动获得的数据（遇到的状态、采取的行动、观察到的奖励）中学习价值函数的事实。后者（有MDP环境模型的算法）被称为规划算法，以反映人工智能代理不需要与真实世界环境互动，实际上是通过模型预测各种行动选择的未来状态/奖励的概率场景，并基于预测的结果求解所需的价值函数。在学习和规划中，贝尔曼方程是驱动算法的基本概念，但算法的细节通常会使它们看起来相当不同。本章将仅关注规划算法，实际上将只关注一种称为动态规划的规划算法子类。 1.3.2 不动点理论 定义 一个函数\\(f:\\mathcal{X\\rightarrow X}\\)的不动点是指一个\\(x\\in\\mathcal{X},\\)使得满足方程\\(x=f(x).\\) 1.3.3 贝尔曼策略算子以及策略评估算法 本节介绍第一个动态规划算法即策略评估算法。策略评估算法解决了在固定策略\\(\\pi\\)下计算有限Markov决策过程（MDP）的价值函数的问题（即有限MDP的预测问题）。我们知道这等价于计算\\(\\pi\\)隐式的有限Markov奖励过程（MRP）的价值函数。为了避免符号混淆，注意对符号的\\(\\pi\\)上标表示它是指\\(\\pi\\)隐式MRP的符号。预测问题的具体定义如下 设MDP（以及\\(\\pi\\)隐式的MRP）的状态集为\\(\\mathcal{S}=\\{s_1,...,s_n\\}\\),不失一般性，设\\(\\mathcal{N}=(s_1,...,s_m)\\)为非终止状态。我们给定一个固定策略\\(\\pi:\\mathcal{N\\times A}\\rightarrow [0,1]\\).我们还给定\\(\\pi\\)隐式的MRP转移概率函数： \\[ \\mathcal{P}_R^\\pi:\\mathcal{N\\times D\\times S}\\rightarrow [0,1] \\] 该函数以数据结构的形式提供（因为状态是有限的，且每个非终止状的下一状态和奖励转移对也是有限的）。预测问题就是计算在固定策略\\(\\pi\\)下评估的MDP的价值函数（等价于\\(\\pi\\)隐式MRP的价值函数），我们用\\(V^\\pi:\\mathcal{N}\\rightarrow \\mathbb{R}\\)来表示。 根据前面的内容，通过从\\(\\mathcal{P}_R^\\pi\\)中提取隐式Markov过程的转移概率函数\\(\\mathcal{P}^\\pi:\\mathcal{N\\times S}\\rightarrow [0,1]\\)和奖励函数\\(\\mathcal{R}^\\pi:\\mathcal{N}\\rightarrow \\mathbb{R},\\)我们可以对价值函数\\(V^\\pi:\\mathcal{N}\\rightarrow \\mathbb{R}\\)(表示为列向量\\(\\boldsymbol{V}^\\pi\\in\\mathbb{R}^m\\))执行以下计算来求解这个预测问题： \\[ \\boldsymbol{V}^\\pi=(\\boldsymbol{I}_m-\\gamma \\boldsymbol{\\mathcal{P}}^\\pi)^{-1}\\cdot \\boldsymbol{\\mathcal{R}}^\\pi \\] 其中\\(\\boldsymbol{I}_m\\)是\\(m\\)阶的单位矩阵，列向量\\(\\boldsymbol{\\mathcal{R}}^\\pi\\in\\mathbb{R}^m\\)表示\\(\\mathcal{R}^\\pi,\\boldsymbol{\\mathcal{P}}^\\pi\\)是一个\\(m\\)阶的矩阵代表\\(\\mathcal{P}^\\pi\\)（其中的行和列对应非终止状态）。然而当\\(m\\)很大时这种计算方式不能很好地扩展，因此我们需要寻找一个数值算法来解这个MRP贝尔曼方程 \\[ \\boldsymbol{V}^\\pi=\\boldsymbol{\\mathcal{R}}^\\pi+\\gamma \\boldsymbol{\\mathcal{P}}^\\pi\\cdot \\boldsymbol{V}^\\pi \\] 我们定义贝尔曼策略算子\\(\\boldsymbol{B}^\\pi:\\mathbb{R}^m\\rightarrow \\mathbb{R}^m\\)为 \\[ \\boldsymbol{B}^\\pi(\\boldsymbol{V})=\\boldsymbol{\\mathcal{R}}^\\pi+\\gamma \\boldsymbol{\\mathcal{P}}^\\pi\\cdot \\boldsymbol{V}^\\pi \\text{ for any vector }\\boldsymbol{V} \\text{ in the vector space }\\mathbb{R}^m \\tag{1.11} \\] 因此，MRP贝尔曼方程就可以表示为 \\[ \\boldsymbol{V}^\\pi=\\boldsymbol{B}^\\pi(\\boldsymbol{V}^\\pi) \\] 这意味着\\(\\boldsymbol{V}^\\pi\\)是贝尔曼策略算子\\(\\boldsymbol{B}^\\pi\\)地一个不动点！注意，贝尔曼策略算子可以推广到非有限MDP地情况，并且\\(\\boldsymbol{V}^\\pi\\)仍然是各种感兴趣的推广的不动点。然而，由于本章重点是开发有限MDP算法因此仍然使用上述狭义的（方程(1.11)）定义。此外，为了证明本章基于不动点的动态规划算法的正确性，我们假设折扣因子\\(\\gamma&lt;1\\). 我们希望提出一种度量使得\\(\\boldsymbol{B}^\\pi\\)是一个压缩映射从而能够利用Banach不动点定理，通过反复应用贝尔曼策略算子\\(\\boldsymbol{B}^\\pi\\)来解决这个预测问题。对于任何值函数\\(\\boldsymbol{V}\\in\\mathbb{R}^m\\)（这表示\\(V:\\mathcal{N}\\rightarrow \\mathbb{R}\\)），我们将表达任何状态\\(s\\in\\mathcal{N}\\)的值为\\(\\boldsymbol{V}(s)\\). 我们的度量$d:mm \\(将是\\)L^$范数，定义为： \\[ d(\\boldsymbol{X},\\boldsymbol{Y})=\\Vert\\boldsymbol{X}-\\boldsymbol{Y} \\Vert_{\\infty}=\\max_{s\\in\\mathcal{N}}\\vert(\\boldsymbol{X}-\\boldsymbol{Y})(s)\\vert \\] \\(\\boldsymbol{B}^\\pi\\)是在无穷范数下的压缩映射，这是因为对于所有的\\(\\boldsymbol{X},\\boldsymbol{Y}\\in\\mathbb{R}^m,\\)我们有 \\[ \\max_{s\\in\\mathcal{N}}\\vert \\boldsymbol{B}^\\pi(\\boldsymbol{X})-\\boldsymbol{B}^\\pi(\\boldsymbol{Y})(s)\\vert=\\gamma\\cdot\\max_{s\\in\\mathcal{N}}\\vert(\\boldsymbol{P}^\\pi\\cdot(\\boldsymbol{X}-\\boldsymbol{Y})(s))\\vert\\leq \\gamma \\cdot\\max_{s\\in\\mathcal{N}}\\vert(\\boldsymbol{X}-\\boldsymbol{Y})(s)\\vert \\] 因此调用Banach不动点定理就证明了下面的定理 定理（策略评估收敛定理）： 对于一个有限的MDP，若\\(\\vert\\mathcal{N}\\vert=m,\\gamma&lt;1,\\)如果\\(\\boldsymbol{V}^\\pi\\in\\mathbb{R}^m\\)是在固定策略\\(\\pi:\\mathcal{N\\times A}\\rightarrow [0,1]\\)下评估的价值函数，则\\(\\boldsymbol{V}^\\pi\\)是贝尔曼策略算子\\(\\boldsymbol{B}^\\pi\\)的唯一不动点，并且： \\[ \\lim_{i\\rightarrow \\infty}(\\boldsymbol{B}^\\pi)^i(\\boldsymbol{V}_0)\\rightarrow \\text{ for all starting Value Functions }\\boldsymbol{V}_0\\in\\mathbb{R}^m \\] 这给我们提供了以下的迭代算法（称为固定策略\\(\\pi\\)下的策略评估算法）： 从任意\\(\\boldsymbol{V}_0\\in\\mathbb{R}^m\\)开始 对于每次迭代\\(i=0,1,...,\\)计算： \\[ \\boldsymbol{V}_{i+1}=\\boldsymbol{B}^\\pi(\\boldsymbol{V}_{i})=\\boldsymbol{\\mathcal{R}}^\\pi+\\gamma \\boldsymbol{\\mathcal{P}}^\\pi\\cdot \\boldsymbol{V}_i \\] 当\\(d(\\boldsymbol{V}_{i},\\boldsymbol{V}_{i+1})=\\max_{s\\in\\mathcal{N}}\\)(i-{i+1})(s) $足够小时停止算法。 请注意，尽管我们将贝尔曼策略算子\\(\\boldsymbol{B}^\\pi\\)定义为作用于\\(\\pi\\)隐式的MRP值函数，但我们也可将其看作作用于MDP的值函数。为了支持MDP的视角，我们将方程(1.11)重新表达为MDP转移/奖励规范如下所示 \\[ \\boldsymbol{B}^\\pi(s)=\\sum_{a\\in\\mathcal{A}}\\pi(s,a)\\cdot\\mathcal{R}(s,a)+\\gamma\\sum_{a\\in\\mathcal{A}} \\pi(s,a)\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot \\boldsymbol{V}(s&#39;)\\text{ for all }s\\in\\mathcal{N} \\tag{1.12} \\] 如果给定MRP的非终止状态数\\(m,\\)则每次迭代的运行时间为\\(O(m^2).\\)注意，要从给定的MDP和给定的策略构建MRP需要\\(O(m^2\\cdot k)\\)次运算，其中\\(k=|\\mathcal{A}|.\\) 1.3.4 贪心策略 我们之前提到过要展示三种动态规划算法。第一种策略评估如上一节所见。解决了MDP预测问题。接下来两节中介绍的两种将会解决MDP控制问题，本节是从预测到控制的一个过渡，在这一节中，我们定义了一个函数，该函数通过“贪心”技术来改进值函数或者策略。形式上，贪心策略函数 \\[ G:\\mathbb{R}^m\\rightarrow (\\mathcal{N\\rightarrow A}) \\] 将一个值函数\\(\\boldsymbol{V}\\)(表示为向量)映射到一个确定性策略\\(\\pi&#39;:\\) \\[ G(\\boldsymbol{V})(s):\\pi_D&#39;(s)=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}\\{\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot\\boldsymbol{V}(s&#39;) \\} \\text{ for all }s\\in\\mathcal{N} \\tag{1.13} \\] 请注意，对于任何特定的\\(s,\\)如果两个或多个动作\\(a\\)实现了\\(\\mathcal{R}(s,a)+\\gamma\\cdot\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot\\boldsymbol{V}(s&#39;)\\)的最大化,那我们将使用任意一个动作打破平局并分配一个单一的动作\\(a\\)来作为上述\\(\\arg\\max\\)操作的输出。下面使用一个等效的表达式(来指导代码) \\[ G(\\boldsymbol{V})(s)=\\mathop{\\arg\\max}_{a\\in\\mathcal{A}}\\left\\{\\sum_{s&#39;\\in\\mathcal{S}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;)\\cdot(r+\\gamma\\cdot \\boldsymbol{W}(s&#39;))\\right\\}\\text{ for all }s\\in\\mathcal{N} \\tag{1.14} \\] 其中\\(\\boldsymbol{W}\\in\\mathbb{R}^n\\)定义为 \\[\\begin{equation} \\boldsymbol{W}(s)=\\begin{cases} \\boldsymbol{V}(s&#39;)&amp;\\text{ if }s&#39;\\in\\mathcal{N}\\\\ 0&amp;\\text{ if }s&#39;\\in\\mathcal{T=S-N} \\end{cases} \\end{equation}\\] 注意在方程(1.14)中必须使用\\(\\mathcal{P}_R\\)，我们需要考虑到所有状态\\(s&#39;\\in\\mathcal{S}\\)的转移而不是方程(1.13)中\\(s&#39;\\in\\mathcal{N}\\)的状态。因此，我们需要小心处理\\(s&#39;\\in\\mathcal{T}\\)的转移。 “贪心”一次来源于“贪心算法”，意味着一种算法通过在局部做出最优选择期望能接近全局最优解。在这里，贪心策略意味着如果我们有一个策略\\(\\pi\\)及其对应的值函数\\(\\boldsymbol{V}^\\pi\\)(假设通过策略评估算法获得)，那么应用贪心策略函数\\(G\\)到\\(\\boldsymbol{V}^\\pi\\)将得到一个确定性策略\\(\\pi_D&#39;,\\)它预期在某种意义上比\\(\\pi\\)更好，具体而言即\\(\\boldsymbol{V}^{\\pi&#39;_D}\\)要优于\\(\\boldsymbol{V}^\\pi\\). 1.3.5 策略提升 像“更好”、“提升”这样的术语指的是值函数或者策略（后者指的是评估给定策略的MDP的值函数）。那么，什么叫做值函数\\(X:\\mathcal{N}\\rightarrow \\mathbb{R}\\)比\\(Y:\\mathcal{N}\\rightarrow \\mathbb{R}\\)更好呢？下面的定义给出了答案 定义（值函数的比较） 我们说对于一个有限的MDP，值函数\\(X\\)比值函数\\(Y\\)更好，记作\\(X\\geq Y,\\)当且仅当 \\[ X(s)\\geq Y(s)\\quad \\forall s\\in\\mathcal{N} \\] 如果我们处理的是有限的MDP（具有\\(m\\)个非终止状态），我们会将值函数表示为向量的形式\\(\\boldsymbol{X},\\boldsymbol{Y}\\in\\mathbb{R}^m.\\) 因此，每当你听到更好的值函数或者改进的值函数这样的术语时，应该理解为值函数在每个状态下都不比它所比较的值函数更差。 那么，什么是\\(\\pi_D&#39;=G(\\boldsymbol{V}^\\pi)\\)比\\(\\pi\\)更好呢？下面是Richard Bellman的一个重要定理，给出了明确的解释： 定理（策略改进定理）：对于一个有限的MDP，对于任意策略\\(\\pi\\),都有： \\[ \\boldsymbol{V}^{\\pi_D&#39;}=\\boldsymbol{V}^{G(\\boldsymbol{V}^{\\pi})}\\geq \\boldsymbol{V}^{\\pi} \\] 这个证明基于应用贝尔曼策略算子在给定MDP的值函数上的作用（注意这种MDP视角下的贝尔曼策略算子在(1.12)中表示）。我们首先注意到，反复应用贝尔曼策略算子\\(B^{\\pi_D&#39;}\\)从值函数\\(\\boldsymbol{V}^\\pi\\)开始，最终会收敛到\\(\\boldsymbol{V}^{\\pi_D&#39;}.\\)形式上， \\[ \\lim_{i\\rightarrow \\infty}(B^{\\pi_D&#39;})^i(\\boldsymbol{V}^\\pi)=\\boldsymbol{V}^{\\pi_D&#39;} \\] 因此证明的关键是证明 \\[ (B^{\\pi_D&#39;})^{i+1}(\\boldsymbol{V}^\\pi)\\geq (B^{\\pi_D&#39;})^i(\\boldsymbol{V}^\\pi)\\quad \\forall i=0,1,2,... \\] 这意味着通过反复应用贝尔曼策略算子得到一个不下降的值函数序列，随着反复应用，会不断改善直到收敛到值函数\\(\\boldsymbol{V}^{\\pi_D&#39;}\\). 策略改进定理为我们提供了用来解决MDP控制问题的第一个动态规划算法（称为策略迭代）。策略迭代算法是Ronald Howard（1960）提出的。 1.3.6 策略迭代算法 策略提升定理的证明向我们展示了怎么从一个关于策略\\(\\pi\\)的值函数\\(\\boldsymbol{V}^\\pi\\)出发，通过贪婪策略改进生成策略\\(\\pi_D&#39;=G(\\boldsymbol{V})\\),然后以\\(\\boldsymbol{V}^\\pi\\)为起始价值函数进行策略评估（使用策略\\(\\pi_D&#39;\\)），得到改进后的价值函数\\(\\boldsymbol{V}^{\\pi_D&#39;}\\),该价值函数优于我们最初的价值函数\\(\\boldsymbol{V}^\\pi.\\)现在需要注意的是，我们可以重复这一过程，从\\(\\pi_D&#39;,\\boldsymbol{V}^{\\pi_D&#39;}\\)出发，进一步改进策略\\(\\pi_D&#39;&#39;\\)及其相关的改进价值函数\\(\\boldsymbol{V}^{\\pi_D&#39;&#39;}\\),我们可以继续这种方式，生成进一步改进的策略及其相关的价值函数，直到无法再改进为止。这种将策略改进与使用改进策略进行策略评估相结合的方法被称为策略迭代算法。 从任意价值函数\\(\\boldsymbol{V}_0\\in\\mathbb{R}^m\\)开始； 迭代\\(j=0,1,2,...,\\)在每次迭代中计算： 确定性策略:\\(\\pi_{j+1}=G(\\boldsymbol{V}_j)\\) 价值函数：\\(\\boldsymbol{V}_{j+1}=\\lim_{i\\rightarrow \\infty}(B^{\\pi_{j+1}})(\\boldsymbol{V}_j)\\) 当\\(d(\\boldsymbol{V}_i,\\boldsymbol{V}_{i+1})=\\max_{s\\in\\mathcal{N}} |(\\boldsymbol{V}_i-\\boldsymbol{V}_{i+1})(s)|\\)足够小，停止算法。 因此，当价值函数无法进一步改进时，算法终止，当这种情况发生时，以下等式应成立： \\[ \\boldsymbol{V}_j=(B^{G(\\boldsymbol{V_j})})^i(\\boldsymbol{V}_j)=\\boldsymbol{V}_{j+1}\\text{ for all }i=0,1,2,... \\] 特别地，当\\(i=1\\)时有： \\[ \\boldsymbol{V}_j(s)=B^{G(\\boldsymbol{V_j})}(\\boldsymbol{V}_j)(s)=\\mathcal{R}(s,G(\\boldsymbol{V}_j)(s))+\\gamma\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,G(\\boldsymbol{V_j})(s),s&#39;)\\cdot \\boldsymbol{V_j}(s&#39;)\\text{ for all }s\\in\\mathcal{N} \\] 由方程(1.13)可知，我们对于每个\\(s\\in\\mathcal{N},\\pi_{j+1}(s)=G(\\boldsymbol{V_j})(s)\\)是最大化\\(\\{ \\mathcal{R}(s,a)+\\gamma\\sum_{s&#39;}\\mathcal{P}(s,a,s&#39;)\\cdot \\boldsymbol{V_j}(s&#39;) \\}\\)的动作，因此 \\[ \\boldsymbol{V_j}(s)=\\max_{a\\in\\mathcal{A}}\\left\\{ \\mathcal{R}(s,a)+\\gamma\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot\\boldsymbol{V_j}(s&#39;) \\right\\}\\text{ for all }s\\in\\mathcal{N} \\] 但这实际上是MDP状态-价值函数贝尔曼最优性方程，这意味着\\(\\boldsymbol{V}_j=\\boldsymbol{V}^*,\\)即当\\(\\boldsymbol{V}_{j+1}=\\boldsymbol{V}_j\\)时，策略迭代算法已收敛到最优价值函数。策略迭代算法收敛时的确定性策略\\(\\pi_j:\\mathcal{N\\rightarrow A}\\)是一个最优策略，因为\\(\\boldsymbol{Y}_{\\pi_j}=\\boldsymbol{V}_j\\approx \\boldsymbol{V}^*\\),这意味着用确定性策略\\(\\pi_j\\)评估MDP可以实现最优价值函数。这表明策略迭代算法解决了MDP控制问题。这证明了以下定理： 定理（策略迭代收敛定理）： 对于具有\\(|\\mathcal{N}|=m,\\gamma&lt;1\\)的有限MDP，策略迭代算法收敛到最优价值函数\\(\\boldsymbol{V}^*\\in\\mathbb{R}^m\\)以及一个确定性最优策略\\(\\pi_D^*:\\mathcal{N\\rightarrow A}\\)，无论我们从哪个价值函数\\(\\boldsymbol{V}_0\\)开始算法。 1.3.7 贝尔曼最优性算子与值迭代算法 通过对方程(1.13)进行微调（将\\(\\arg\\max\\)改为\\(\\max\\)）,我们定义贝尔曼最优性算子： \\[ B^*:\\mathbb{R}^m\\rightarrow \\mathbb{R}^m \\] 作为向量空间\\(\\mathbb{R}^m\\)中向量（表示价值函数）的以下非线性变换： \\[ B^*(\\boldsymbol{V})(s)=\\max_{a\\in\\mathcal{A}}\\left\\{ \\mathcal{R}(s,a)+\\gamma\\sum_{s\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot \\boldsymbol{V}(s&#39;)\\right\\}\\text{ for all }s\\in\\mathcal{N} \\tag{1.15} \\] 我们将在数学推导中使用方程(1.15),但我们需要一个不同但等价的表达式来指导代码，使用接口操作的是\\(\\mathcal{P}_R\\)并非\\(\\mathcal{P,R}\\),等价表达式如下： \\[ B^*(\\boldsymbol{V})(s)=\\max_{a\\in\\mathcal{A}}\\left\\{ \\sum_{s\\in\\mathcal{N}}\\sum_{r\\in\\mathcal{D}}\\mathcal{P}_R(s,a,r,s&#39;)\\cdot (r+\\gamma\\boldsymbol{W}(s&#39;)) \\right\\}\\text{ for all }s\\in\\mathcal{N} \\tag{1.16} \\] 其中\\(\\boldsymbol{W}\\in\\mathbb{R}^n\\)定义为 \\[\\begin{equation} \\boldsymbol{W}(s)=\\begin{cases} \\boldsymbol{V}(s&#39;)&amp;\\text{ if }s&#39;\\in\\mathcal{N}\\\\ 0&amp;\\text{ if }s&#39;\\in\\mathcal{T=S-N} \\end{cases} \\end{equation}\\] 注意在方程(1.16)中，由于我们需要考虑到所有状态\\(s&#39;\\in\\mathcal{S}\\)的转移而不是方程(1.13)中\\(s&#39;\\in\\mathcal{N}\\)的状态。因此，我们需要小心处理\\(s&#39;\\in\\mathcal{T}\\)的转移。 对于每一个\\(s\\in\\mathcal{N},\\)在(1.15)中产生的最大化动作\\(a\\in\\mathcal{A}\\)是由确定性策略\\(\\pi_D\\)在(1.13)中规定的动作。因此，如果我们使用贪婪策略\\(G(\\boldsymbol{V})\\)在任何价值函数上应用贝尔曼策略算子，它应该与应用贝尔曼最优性算子相同，因此： \\[ B^{G(\\boldsymbol{V})}(\\boldsymbol{V})=B^*(\\boldsymbol{V})\\text{ for all }\\boldsymbol{V}\\in\\mathbb{R}^m \\tag{1.17} \\] 特别地，通过将\\(\\boldsymbol{V}\\)转化为策略\\(\\pi\\)的价值函数\\(\\boldsymbol{V}^\\pi\\),我们得到： \\[ B^{G(\\boldsymbol{V}^\\pi)}(\\boldsymbol{V}^\\pi)=B^*(\\boldsymbol{V}^\\pi) \\] 这是策略评估第一阶段的一个简洁表示，其中使用了改进的策略\\(G(\\boldsymbol{V}^\\pi)\\)（注意贝尔曼策略算子、贝尔曼最优性算子和贪婪策略函数如何在这个方程中结合在一起）。 正如贝尔曼策略算子\\(B^\\pi\\)是由MDP贝尔曼策略方程（等价于MRP贝尔曼方程）所驱动的，贝尔曼最优性算子\\(B^*\\)是由MDP状态-价值函数贝尔曼最优性方程（重新陈述如下）所驱动的 \\[ \\boldsymbol{V}^*(s)=\\max_{a\\in\\mathcal{A}}\\left\\{\\mathcal{R}(s,a)+\\gamma\\sum_{s&#39;\\in\\mathcal{N}}\\mathcal{P}(s,a,s&#39;)\\cdot\\boldsymbol{V}^*(s&#39;)\\right\\}\\text{ for all }s\\in\\mathcal{N} \\] 因此，我们可以简洁地将MDP状态-价值函数贝尔曼最优性方程表示为： \\[ \\boldsymbol{V^*}=B^*(\\boldsymbol{V^*}) \\] 这意味着\\(\\boldsymbol{V^*}\\)是贝尔曼最优性算子\\(B^*\\)地一个不动点。 需要注意的是，我们提供的贪婪策略函数和贝尔曼最优性算子的定义可以推广到非有限MDP，因此我们可以推广方程(1.17)，并且\\(\\boldsymbol{V^*}\\)是贝尔曼最优性算子的不动点的陈述仍然成立。然而，在本章中由于我们专注于开发有限 MDP 的算法，因此我们将坚持为有限 MDP 提供的定义。 正如我们证明\\(B^\\pi\\)是一个压缩函数一样，我们希望证明\\(B^*\\)也是一个压缩函数（在\\(L^\\infty\\)范数下），以便我们可以利用Banach不动点定理，并通过迭代应用贝尔曼最优性算子\\(B^*\\)来解决控制问题。因此，我们需要证明对于所有\\(\\boldsymbol{X},\\boldsymbol{Y}\\in\\mathbb{R}^m:\\) \\[ \\max_{s\\in\\mathcal{N}}|(B^*(\\boldsymbol{X})-B*(\\boldsymbol{Y}))(s)|\\leq \\gamma\\cdot \\max_{s\\in\\mathcal{N}}|(\\boldsymbol{X}-\\boldsymbol{Y})(s)| \\] 这个证明比之前为\\(B^\\pi\\)所做的证明要难一些，这里需要利用\\(B^*\\)的两个关键性质，单调性和常数平移性：对于所有的\\(\\boldsymbol{X}\\in\\mathbb{R}^m,c\\in\\mathbb{R},B^*(\\boldsymbol{X}+c)(s)=B^*(\\boldsymbol{X})(s)+\\gamma c\\)对于所有的\\(s\\in\\mathcal{N}\\).证明暂时略去，调用Banach不动点定理证明下面的定理： 定理（值迭代收敛定理）：对于具有\\(|\\mathcal{N}|=m,\\gamma&lt;1\\)的有限MDP，如果\\(\\boldsymbol{V}^*\\in\\mathbb{R}^m\\)是最优值函数，则\\(\\boldsymbol{V}^*\\)是贝尔曼最优性算子\\(B^*\\)的唯一不动点，并且： \\[ \\lim_{i\\rightarrow \\infty}(B^*)^i(\\boldsymbol{V}_0)\\rightarrow \\boldsymbol{V}^*\\text{ for all staring Value Function }\\boldsymbol{V}_0\\in\\mathbb{R}^m \\] 这为我们提供了以下迭代算法，称为值迭代算法，有Richard Bellman提出： 从任意价值函数\\(\\boldsymbol{V}_0\\)开始; 迭代\\(i=0,1,2,...,\\)在每次迭代中计算： \\[ \\boldsymbol{V}_{i+1}(s)=B^*(\\boldsymbol{V}_i)(s)\\text{ for all }s\\in\\mathcal{N} \\] 当\\(d(\\boldsymbol{V}_i,\\boldsymbol{V}_{i+1})=\\max_{s\\in\\mathcal{N}} |(\\boldsymbol{V}_i-\\boldsymbol{V}_{i+1})(s)|\\)足够小，停止算法。 1.3.8 从最优值函数到最优策略 需要注意的是，策略迭代算法在每次迭代中都会生成一个策略及其对应的价值函数。因此，最终当我们收敛到最优值函数\\(\\boldsymbol{V}_j=\\boldsymbol{V}^*\\)时，策略迭代算法总会有一个确定性策略\\(\\pi_j\\)与\\(\\boldsymbol{V}_j\\)相关联，使得： \\[ \\boldsymbol{V}_j=\\boldsymbol{V}^{\\pi_j}=\\boldsymbol{V}^* \\] 我们将\\(\\pi_j\\)称为最优策略\\(\\pi^*,\\)即产生最优值函数\\(\\boldsymbol{V}^*\\)的策略，即 \\[ \\boldsymbol{V}^{\\pi^*}=\\boldsymbol{V}^* \\] 然而，值迭代算法没有与之相关联的策略，因为整个算法缺乏策略表示，仅操作价值函数。因此，现在的问题是：当值迭代手来难道最优价值函数\\(\\boldsymbol{V}_i=\\boldsymbol{V}^*\\),我们如何获得一个最优策略使得： \\[ \\boldsymbol{V}^{\\pi^*}=\\boldsymbol{V}_i=\\boldsymbol{V}^* \\] 答案在于贪婪策略函数\\(G.\\)方程(1.17)告诉我们： \\[ B^{G(\\boldsymbol{V})}(\\boldsymbol{V})=B^*(\\boldsymbol{V})\\text{ for all }\\boldsymbol{V}\\in\\mathbb{R}^m \\] 将\\(\\boldsymbol{V}\\)特化为\\(\\boldsymbol{V}^*\\),我们得到 \\[ B^{G(\\boldsymbol{V}^*)}(\\boldsymbol{V}^*)=B^*(\\boldsymbol{V}^*) \\] 但我们又指导\\(\\boldsymbol{V}^*\\)是贝尔曼算子\\(B^*\\)的不动点，因此 \\[ B^{G(\\boldsymbol{V}^*)}(\\boldsymbol{V}^*)=\\boldsymbol{V}^* \\] 这表明，用确定性贪婪策略\\(G(\\boldsymbol{V}^*)\\)(使用贪婪策略函数从最优价值函数创建的策略)评估MDP，实际上实现了最优价值函数\\(\\boldsymbol{V}^*.\\)换句话说，\\(G(\\boldsymbol{V}^*)\\)是我们一直在寻找的确定性最优策略\\(\\pi^*.\\) 1.3.9 广义策略迭代 本节中将深入探讨策略迭代算法的结构，并展示如何将其结构推广到更一般的情况。让我们首先从二维布局的角度来看策略迭代中价值函数从初始值函数\\(\\boldsymbol{V}_0\\)到最终价值函数\\(\\boldsymbol{V}^*\\)的演进过程。 策略迭代的二维布局 策略迭代的演进过程可以用以下二维布局表示： $$ \\[\\begin{align} \\pi_1=G(\\boldsymbol{V}_0),\\boldsymbol{V}_0\\rightarrow B^{\\pi_1}(\\boldsymbol{V}_0)\\rightarrow &amp;... (B^{\\pi_1})^i(\\boldsymbol{V}_0)\\rightarrow... \\boldsymbol{V}^{\\pi_1}=\\boldsymbol{V}_1\\\\ \\pi_2=G(\\boldsymbol{V}_1),\\boldsymbol{V}_1\\rightarrow B^{\\pi_2}(\\boldsymbol{V}_1)\\rightarrow &amp;... (B^{\\pi_2})^i(\\boldsymbol{V}_1)\\rightarrow... \\boldsymbol{V}^{\\pi_2}=\\boldsymbol{V}_2\\\\ &amp;...\\\\ &amp;...\\\\ \\pi_{j+1}=G(\\boldsymbol{V}_j),\\boldsymbol{V}_j\\rightarrow B^{\\pi_{j+1}}(\\boldsymbol{V}_j)\\rightarrow &amp;... (B^{\\pi_{j+1}})^i(\\boldsymbol{V}_j)\\rightarrow... \\boldsymbol{V}^{\\pi_{j+1}}=\\boldsymbol{V}_j\\\\ \\end{align}\\] $$ 每一行代表在特定策略下价值函数的演进过程。每一行从使用贪婪策略函数\\(G\\)创建策略开始，其余部分是通过对该策略应用贝尔曼策略算子\\(B^\\pi\\)指导收敛到该策略的价值函数。因此每一行以策略改进开始，其余部分是策略评估。注意，每一行的结束通过贪婪策略函数\\(G\\)与下一行的开始无缝衔接！ 策略迭代的三重循环 策略迭代算法实际上包含三重循环： 最外层循环：遍历二维布局中的每一行（每次迭代生成一个改进的策略）。 中间层循环：遍历每一行中的列（每次迭代应用贝尔曼策略算子，即策略评估的迭代）。 最内层循环：遍历所有状态\\(s\\in\\mathcal{N},\\)因为在应用贝尔曼策略算子更新价值函数时需要遍历所有状态（在应用贪婪策略函数改进策略时也需要遍历所有状态）。 策略迭代的高层次视角 从更高层次来看，策略迭代时策略评估和策略改进交替进行的过程： 策略评估：根据当前策略生成价值函数。 策略改进：根据当前价值函数生成贪婪策略（相对于前一个策略有所改进）。 这种交替过程使得价值函数和策略逐渐趋于一致，直到最终收敛。 策略迭代的可视化 下图展示了策略迭代中价值函数和策略的演进过程。图中： 下部的线（策略线）：表示策略的演进。 上部的线（价值函数线）：表示价值函数的演进。 指向价值函数线的箭头：表示对给定策略\\(\\pi\\)的策略评估，生成价值函数\\(\\boldsymbol{V}^\\pi\\). 指向策略线的箭头：表示从价值函数\\(\\boldsymbol{V}^\\pi\\)生成贪婪策略\\(\\pi&#39;=G(\\boldsymbol{V}^\\pi)\\). 策略评估和策略改进是“竞争”的——它们“朝不同方向推动”，但最终目标是使价值函数和策略趋于一致。 广义策略迭代 广义策略迭代（Generalized Policy Iteration, GPI）是 Sutton 和 Barto 在其强化学习书中强调的一个重要概念，它统一了所有动态规划（DP）和强化学习（RL）算法的变体。GPI 的核心思想是： 策略评估：可以使用任何策略评估方法 策略改进：可以使用任何策略改进方法（不一定是经典策略迭代算法中的方法）。 GPI 的关键在于，策略评估和策略改进不需要完全达到它们各自追求的一致性目标。例如： 策略评估：可以只进行几次贝尔曼策略评估，而不是完全收敛到\\(V^{\\pi}\\). 策略改进：可以只更新部分状态的策略，而不是所有状态。 值迭代作为GPI的实例 值迭代是 GPI 的一个具体实例。在值迭代中，每次迭代只应用一次贝尔曼策略算子，然后进行策略改进。其二维布局如下： $$ $$ 在值迭代中，策略改进步骤保持不变，但策略评估简化为仅应用一次贝尔曼策略算子。 广义策略迭代是强化学习中最核心的概念之一。几乎所有强化学习控制算法都可以视为 GPI 的特例。例如，在某些简单的强化学习控制算法中： 策略评估：只对单个状态进行。 策略改进：也只对单个状态进行。 这些算法本质上是单状态策略评估和单状态策略改进的交替序列。 总结 贝尔曼方程和广义策略迭代是强化学习中最重要的两个概念。 GPI 的核心思想是交替进行某种形式的策略评估和策略改进。 GPI 统一了动态规划和强化学习的各种算法，是理解强化学习控制问题的基础。 1.4 动态资产配置和消费 1.4.1 个人财务的优化 个人财务对某些人来说可能非常简单（每月领取工资，花光所有工资）而对另一些人来说可能非常复杂（例如，那些在多个国家拥有多家企业并拥有复杂资产和负债的人）。在这里，我们将考虑一个相对简单但包含足够细节的情况，以便提供动态资产配置和消费问题的基本要素。假设你的个人财务包括以下几个方面： 收入：这可能包括你的定期工资，通常在一段时间内保持不变，但如果你获得晋升或找到新工作，工资可能会发生变化。这也包括你从投资组合中变现的资金，例如如果你卖出一些股票并决定不再投资于其他资产。此外，还包括你从储蓄账户或某些债券中获得的利息。还有许多其他收入来源，有些是固定的定期支付，有些则在支付金额和时间上具有不确定性，我们不会一一列举所有不同的收入方式。我们只想强调，在生活中的各个时间点获得收入是个人财务的关键方面之一。 消费：这里的“消费”指的是“支出”。需要注意的是，人们需要定期消费以满足基本需求，如住房、食物和衣物。你支付的房租或按揭贷款就是一个例子——它可能是每月固定金额，但如果你的按揭利率是浮动的，它可能会有所变化。此外，如果你搬到新房子，房租或按揭可能会有所不同。你在食物和衣物上的支出也构成了消费。这通常每月相对稳定，但如果你有了新生儿，可能需要额外的支出用于婴儿的食物、衣物甚至玩具。此外，还有超出“必需品”的消费——比如周末在高级餐厅用餐、夏季度假、购买豪华汽车或昂贵的手表。人们从这种消费中获得“满足感”或“幸福感”（即效用）。这里的关键点是，我们需要定期决定每周或每月花多少钱（消费）。在动态决策中，人们面临着消费（带来消费效用）和储蓄（将钱投入投资组合，希望钱能增值，以便未来能够消费更多）之间的张力。 投资：假设你可以投资于多种资产——简单的储蓄账户提供少量利息、交易所交易的股票（从价值股到成长股，各自有不同的风险-收益权衡）、房地产（你购买并居住的房子确实被视为一种投资资产）、黄金等大宗商品、艺术品等。我们将投资于这些资产的资金组合称为投资组合（关于投资组合理论的简要介绍见附录B）。我们需要定期决定是否应该将大部分资金投入储蓄账户以保安全，还是应该将大部分投资资金分配于股票，或者是否应该更具投机性，投资于早期初创公司或稀有艺术品。审查投资组合的构成并可能重新分配资金（称为重新平衡投资组合）是动态资产配置的问题。还需要注意的是，我们可以将部分收入投入投资组合（意味着我们选择不立即消费这笔钱）。同样，我们可以从投资组合中提取部分资金用于消费。将资金投入或提取出投资组合的决策本质上是我们所做的动态消费决策，它与动态资产配置决策密切相关。 以上描述希望为你提供了资产配置和消费的双重动态决策的基本概念。最终，我们的个人目标是在一生中最大化消费的期望总效用（可能还包括在你去世后为配偶和子女提供的消费效用）。由于投资组合本质上是随机的，并且我们需要定期做出资产配置和消费决策，你可以看到这具备了随机控制问题的所有要素，因此可以建模为马尔可夫决策过程（尽管通常相当复杂，因为现实生活中的财务有许多细节）。以下是该MDP的粗略和非正式草图（请记住，我们将在本章后面为简化的情况形式化MDP）： 状态：状态通常可能非常复杂，但主要包括年龄（用于跟踪达到MDP时间范围的时间）、投资于每种资产的资金数量、所投资资产的估值，以及可能还包括其他方面，如工作/职业状况（用于预测未来工资的可能性）。 动作：动作是双重的。首先，它是每个时间步骤中选择的投资金额向量（时间步骤是我们审查投资组合以重新分配资金的时间周期）。其次，它是选择消费的灵活/可选的资金数量（即超出我们承诺支付的固定支出，如房租）。 奖励：奖励是我们视为灵活/可选的消费效用——它对应于动作的第二部分。 模型：模型（给定当前状态和动作的下一个状态和奖励的概率）在大多数现实生活情况中可能相当复杂。最困难的方面是预测我们生活和职业中明天可能发生的事情（我们需要这种预测，因为它决定了我们未来获得收入、消费和投资的可能性）。此外，投资资产的不确定性运动也需要由我们的模型捕捉。 现在，我们准备采用这个MDP的一个简单特例，它去除了许多现实世界的摩擦和复杂性，但仍保留了关键特征（特别是双重动态决策方面）。这个简单的特例是默顿投资组合问题（Merton 1969）的主题，他在1969年的一篇里程碑论文中提出并解决了这个问题。他公式的一个关键特征是时间是连续的，因此状态（基于资产价格）演化为连续时间随机过程，而动作（资产配置和消费）是连续进行的。我们在下一节中介绍他论文的重要部分。 1.4.2 Merton投资组合问题及其解决 现在，我们描述默顿投资组合问题并推导其解析解，这是数学经济学中最优雅的解决方案之一。该解决方案的结构将为我们提供关于资产配置和消费决策如何不仅依赖于状态变量，还依赖于问题输入的深刻直觉。 我们将时间记为\\(t\\)，并假设当前时间为\\(t=0\\).假设你刚刚退休并且你将再活\\(T\\)年。因此，用上一节的语言来说，你余生将不会获得任何收入，除了从投资组合中提取资金的选项。再假设你还没有固定支出，如按揭贷款、订阅费等等，这意味着你所有的消费都是灵活/可选的，即你可以在任何时间点选择消费任何非负实数。以上所有假设都是不合理的（但如果在养老金的资产配置中，这是合理的！），但有助于保持问题的简单性，以便于解析处理。 我们将任何时间\\(t\\)的财富（记为\\(W_t\\)）定义为你的投资资产的总市场价值。请注意，由于没有外部的收入，并且所有消费都是可选的，\\(W_t\\)就是你的净资产。假设有固定数量的\\(n\\)个风险资产和1个无风险资产。如前所述，目标是通过在任何时间点的双重动作——资产配置和消费（消费等于在任何时间点从投资组合中提取的资金）——最大化你一生中消费的期望总效用。请注意，由于没有外部资金来源，并且所有从投资组合中提取的资金都会立即被消费，因此你永远不会向投资组合中添加资金。投资组合的增长只能来自投资组合中资产市场价值的增长。最后，我们假设消费效用函数是恒定相对风险厌恶（CRRA）的。 为了便于阐述，我们将问题形式化，并针对\\(n=1\\)（即只有1个风险资产）的情况推导出Merton的优美的解析解。该解可以直观地推广到\\(n&gt;1\\)个风险资产的情况。 由于我们在连续时间中进行操作，风险资产遵循一个随机过程\\(S\\),具体来说是一个几何布朗运动 \\[ dS_t=\\mu S_t+\\sigma S_t dZ_t, \\] 其中\\(\\mu\\in\\mathbb{R},\\sigma\\in\\mathbb{R}_+\\)是固定常数（注意，对于\\(n\\)个资产则分别为向量和矩阵）。 无风险资产没有不确定性，并且在连续时间内有固定的增长率，因此在时间\\(t\\)时无风险资产\\(R_t\\)的估值由下式给出 \\[ dR_t=rR_tdt, \\] 其中\\(r\\in\\mathbb{R}\\)是一个固定常数。我们将单位时间内财富的消费记为\\(c(t,W_t)\\geq0\\)，以明确消费决策通常取决于\\(t,W_t.\\)将时间\\(t\\)时分配给风险资产的财富比例记为\\(\\pi(t,W_t).\\)注意，\\(c(t,W_t),\\pi(t,W_t)\\)共同构成了时间\\(t\\)时的决策（MDP动作）。为了保持简洁，将\\(c(t,W_t),\\pi(w,W_t)\\)分别写为\\(c_t,\\pi_t\\),但请在整个推导过程中认识到两者都是时间\\(t\\)和财富\\(W_t\\)的函数。 最后，我们假设消费的效用函数为 \\[ U(x)=\\frac{x^{1-\\gamma}}{1-\\gamma}, \\] 其中风险厌恶参数\\(\\gamma\\neq1\\).\\(\\gamma\\)是CRRA系数，等于\\(\\frac{-xU&#39;&#39;(x)}{U&#39;(x)}.\\)我们不会讨论\\(\\gamma=1\\)时的CRRA效用函数即\\(U(x)=\\log x\\). 由于我们假设没有向投资组合中添加资金，且没有买卖任何分数量的风险和无风险资产的交易成本，财富的时间演化应被概念化为分配比例\\(\\pi_t\\)的连续调整和从投资组合中的连续提取（等于连续消费\\(c_t\\)）.因此从时间\\(t\\)到\\(t+dt\\)的财富变化\\(dW_t\\)由下式给出： \\[\\begin{equation} dW_t=((r+\\pi_t\\cdot (\\mu-r))\\cdot W_t-c_t)dt+\\pi_t\\sigma W_t dZ_t. \\tag{1.18} \\end{equation}\\] 这是一个确定财富随机演化的伊藤过程。 我们的目标是确定在任何时间\\(t\\)时的最优\\((\\pi(t,W_t),c(t,W_t))\\),以最大化 \\[ \\mathbb{E}\\left[\\int_t^T\\frac{e^{-\\rho(s-t)}\\cdot c_s^{1-\\gamma}}{1-\\gamma} ds+\\frac{e^{-\\rho (T-t)}\\cdot B(T)\\cdot W_T^{1-\\gamma}}{1-\\gamma}|W_t \\right], \\] 其中\\(\\rho\\geq0\\)是效用贴现率，用于考虑未来消费效用可能低于当前效用的事实，\\(B(\\cdot)\\)被称为遗赠函数，可以视为你在时间\\(T\\)去世时留给家人的钱。我们可以为任意遗赠函数\\(B(T)\\)解决这个问题，但为了简单起见，我们考虑\\(B(T)=\\epsilon^\\gamma,0&lt;\\epsilon \\ll 1\\),意味着无遗赠。出于技术原因我们不将其设为0，这将在后面变得明显。 我们应该将这个问题视为一个连续时间的随机控制问题，其中MDP定义如下： 时间\\(t\\)的状态为\\((t,W_t)\\) 时间\\(t\\)的动作为\\((\\pi_t,c_t)\\) 时间\\(t&lt;T\\)时的单位时间奖励为： \\[ U(c_t)=\\frac{c_t^{1-\\gamma}}{1-\\gamma} \\] 在终端时刻\\(T\\)的奖励为 \\[ B(T)\\cdot U(W_T)=\\epsilon^\\gamma\\cdot \\frac{W_T^{1-\\gamma}}{1-\\gamma} \\] 时间\\(t\\)时的回报是累积贴现奖励 \\[ \\int_t^T\\frac{e^{-\\rho(s-t)\\cdot c_s^{1-\\gamma}}}{1-\\gamma} ds+\\frac{e^{-\\rho (T-t)}\\cdot \\epsilon^\\gamma\\cdot W_T^{1-\\gamma}}{1-\\gamma} \\] 我们的目标是找到策略：\\((t,W_t)\\rightarrow (\\pi_t,c_t)\\),以最大化期望汇报。 我们第一步是写出Hamilton-Jacobi-Bellman(HJB)方程，这是连续时间中的Bellman最优性方程的类比。我们将最优的价值函数记为\\(V^*\\),使得时间\\(t\\)时财富\\(W_t\\)的最优价值为\\(V^*(t,W_t).\\)这里的HJB方程可以特化为下面的式子 \\[\\begin{equation} \\max_{\\pi_t,c_t}\\left\\{\\mathbb{E}_t[dV^*(t,W_t)+\\frac{c_t^{1-\\gamma}}{1-\\gamma}] dt\\right\\}=\\rho V^*(t,W_t)dt \\tag{1.19} \\end{equation}\\] 现在对于\\(dV^*\\)使用伊藤引理，移走\\(dZ_t\\)的部分因为这是一个鞅，并且将等式两边都除以\\(dt\\),以产生任意\\(0\\leq t&lt;T\\)的HJB方程的偏微分形式 \\[ \\max_{\\pi_t,c_t}\\left\\{ \\frac{\\partial V^*}{\\partial t}+\\frac{\\partial V^*}{\\partial W_t} \\cdot ((\\pi_t(\\mu-r)+r)W_t-c_t)+\\frac{\\partial^2 V^*}{\\partial W_t^2}\\cdot \\frac{\\pi_t^2\\sigma^2W_t^2}{2}+\\frac{c_t^{1-\\gamma}}{1-\\gamma}\\right\\}=\\rho\\cdot V^*(t,W_t) \\tag{1.20} \\] 这个HJB方程由下面的终端条件 \\[ V^*(T,W_T)=\\epsilon^\\gamma \\cdot\\frac{W_T^{1-\\gamma}}{1-\\gamma} \\] 可以将(1.20)写得更简洁一些： \\[ \\max_{\\pi_t,c_t} \\Phi(t,W_t;\\pi_t,c_t)=\\rho\\cdot V^*(t,W_t) \\tag{1.21} \\] 需要强调的是，我们处理的约束条件是\\(W_t&gt;0,c_t\\geq0,0\\leq t&lt;T.\\) 为了找到最优的\\(\\pi_t^*,c_t^*\\)，我们求得\\(\\Phi\\)的一阶条件得 \\[\\begin{align} \\pi_t^*&amp;=\\frac{-\\frac{\\partial V^*}{\\partial W_t}\\cdot (\\mu-r)}{\\frac{\\partial^2 V^*}{\\partial W_t^*}\\cdot\\sigma^2\\cdot W_t} \\tag{1.22}\\\\ c_t^*&amp;=\\left( \\frac{\\partial V^*}{\\partial W_t} \\right)^{-\\frac{1}{\\gamma}} \\tag{1.23} \\end{align}\\] 下面将(1.22)和(1.23)代入到(1.20)中得到下面的式子，这就给了我们最优价值函数的PDE(显然下面的\\(w\\)应为\\(W_t\\))： \\[ V^*_t-\\frac{(\\mu-r)^2}{2\\sigma^2}\\cdot\\frac{(V^*_{w})^2}{V^*_{ww}}+V_w^*\\cdot r\\cdot w+\\frac{\\gamma}{1-\\gamma}\\cdot(V_w^*)^{\\frac{\\gamma-1}{\\gamma}}=\\rho\\cdot V^* \\tag{1.24} \\] 终值条件仍是 \\[ V^*(T,W_T)=\\epsilon^\\gamma \\cdot\\frac{W_T^{1-\\gamma}}{1-\\gamma} \\] \\(\\Phi\\)的二阶条件在以下假设下得以满足\\(c^*_t&gt;0,W_t&gt;0,\\frac{\\partial^2V^*}{\\partial W_t^2}&lt;0\\)对于所有的\\(0\\leq t&lt;T\\),稍后将会证明这些条件在我们推导的解中得到满足，并且\\(U(\\cdot)\\)是凹函数，即\\(\\gamma&gt;0.\\) 接下来我们希望将PDE(1.24)简化为ODE，这样可以进行简单的求解。为此我们假设解的形式是一个关于时间的确定性函数\\(f(t):\\) \\[ V^*(t,W_t)=f(t)^\\gamma\\cdot\\frac{W_t^{1-\\gamma}}{1-\\gamma} \\] 将猜解的形式代入PDE当中，可以得到 \\[\\begin{align} &amp;f&#39;(t)=\\nu f(t)-1,\\\\ \\text{where }&amp;\\nu=\\rho-(1-\\gamma)\\left(\\frac{(\\mu-r)^2}{2\\sigma^2\\gamma}+r \\right) \\end{align}\\] 此时我们注意到遗赠函数\\(B(T)=\\epsilon^\\gamma\\)对拟合猜测解非常有用，因此该ODE的边界条件为\\(f(T)=\\epsilon\\).由此可以解得 \\[\\begin{equation} f(t)= \\begin{cases} \\frac{1+(\\nu\\epsilon-1)e^{-\\nu(T-t)}}{\\nu}&amp;\\text{ for }\\nu\\neq 0\\\\ T-t+\\epsilon&amp;\\text{ for }\\nu=0 \\end{cases} \\end{equation}\\] 于是最优分配策略和消费策略如下 \\[\\begin{align} \\pi^*(t,W_t)&amp;=\\frac{\\mu-r}{\\sigma^2\\gamma} \\tag{1.25}\\\\ c^*(t,W_t)&amp;= \\begin{cases} \\frac{\\nu W_t}{1+(\\nu\\epsilon-1)e^{-\\nu(T-t)}} &amp;\\text{ for }\\nu\\neq0\\\\ \\frac{W_t}{T-t+\\epsilon}&amp;\\text{ for }\\nu=0 \\end{cases} \\tag{1.26} \\end{align}\\] 同时有了\\(f(t)\\)的表达式。\\(V^*(t,W_t)\\)的表达式也就迎刃而解了。注意\\(f(t)&gt;0\\)对于所有\\(0\\leq t&lt;T,\\forall \\nu,\\)确保了\\(W_t&gt;0,c_t^*&gt;0,\\frac{\\partial^2V^*}{\\partial W_t^2}&lt;0\\).这确保了约束条件\\(W_t&gt;0,c_t\\geq0\\)得以满足，同时二阶条件得以满足。解决Merton的投资组合问题的一个非常重要的经验教训是HJB公式是关键，这种解法为类似的连续时间随机控制问题提供了模板。 1.4.3 Merton投资组合问题解的直觉 \\(\\pi^*\\)是个常数，代表无论财富如何年龄如何，都该将相同的财富比例投资于风险资产。 而\\(c^*\\)中风险资产的超额回报\\((\\mu-r)\\)出现在分子上，\\(\\sigma,\\gamma\\)出现在坟墓上，波动率越大或者风险厌恶程度更高，自然会减少投资于风险资产，而当我们还年轻时我们希望消费的少一些，但是快死了的时候会增加消费（因为最优策略是死得一贫如洗，假设没有遗产）。 将最优策略(1.25)和(1.26)代入财富过程(1.18)，可以得到进行最优资产配置和最优消费的时候，财富过程为 \\[ dW_t^*=(r+\\frac{(\\mu-r)^2}{\\sigma^2\\gamma}-\\frac{1}{f(t)})\\cdot W_t^* \\cdot dt+\\frac{\\mu-r}{\\sigma\\gamma}\\cdot W_t^* \\cdot dZ_t \\tag{1.27} \\] 是一个对数正态过程，对数正态波动率是常数，对数正态飘逸与财富无关但依赖于时间，我们可以解得 \\[ \\mathbb{E}[W_t^*]=W_0\\cdot e^{(r+\\frac{(\\mu-r)^2}{\\sigma^2\\gamma})t}\\cdot e^{-\\int_0^t\\frac{du}{f(u)}} \\] 1.4.4 离散时间资产配置 在这一节中，将会讨论问题的离散时间版本，这使得问题具有解析可解性。类似于Merton的连续时间投资组合问题，我们在时间\\(t=0\\)是拥有财富\\(W_0,\\)在每个离散时间步长\\(t=0,1,...,T-1\\)时我们可以在没有约束没有交易成本的情况下，将财富\\(W_t\\)分配到风险资产和无风险资产的投资组合中。风险资产的回报在每个时间步长内为常数\\(r.\\) 我们假设在任何\\(t&lt;T\\)时没有消费财富并且在时间\\(T\\)时会清算并消费财富\\(W_T.\\)因此我们的目标是通过在每个\\(t=0,1,...,T-1\\)时动态地分配风险资产\\(x_t\\in\\mathbb{R}\\)和剩余的\\(W_t-x_t\\)无风险资产，最大化最终时间步\\(t=T\\)时的期望财富效用。假设单步折现因子为\\(\\gamma\\),最终时间步\\(T\\)的财富效用由以下CARA函数给出 \\[ U(W_T)=\\frac{1-e^{-aW_T}}{a}\\text{ for some fixed } a\\neq 0 \\] 因此问题变成了在每个\\(t=0,1,...,T_1\\)时通过选择\\(x_t\\)来最大化 \\[ \\mathbb{E} \\left[\\gamma^{T-t}\\cdot \\frac{1-e^{-aW_T}}{a}\\mid (t,W_t)\\right] \\] 等价于最大化 \\[ \\mathbb{E} \\left[-\\frac{e^{-aW_T}}{a}\\mid (t,W_t)\\right] \\tag{1.28} \\] 我们将这个问题表述为一个连续状态和连续动作的离散时间有限时域MDP，并准确指定其转移状态、奖励和折现因子，然后我们的目标是求解MDP控制问题找到最优策略。 有限时域MDP的终止时间为\\(T\\),因此所有时间\\(t=T\\)的状态都是终止状态。时间步\\(t=0,1,...,T\\)的状态\\(s_t\\in\\mathcal{S}_t\\)包含财富\\(W_t.\\)决策\\(a_t\\in\\mathcal{A}_t\\)是对风险投资的投资量\\(x_t,\\)因此每个时间步对无风险投资的投资量为\\(W_t-x_t.\\)时间步\\(t\\)的确定性策略\\(\\pi_t\\)记为\\(\\pi_t(W_t)=x_t,\\)同样，时间步\\(t\\)时最优确定性策略\\(\\pi_t^*\\)记为\\(\\pi_t^*(W_t)=x^*_t.\\) 我们将\\(t\\)到\\(t+1\\)的风险资产单步回报的随机变量记为\\(Y_t\\sim N(\\mu,\\sigma^2)\\)对于所有的\\(t=0,1,...,T-1.\\)因此 \\[ W_{t+1}=x_t\\cdot(1+Y_t)+(W_t-x_t)\\cdot (1+r)=x_t\\cdot(Y_t-r)+W_t\\cdot(1+r) \\tag{1.29} \\] MDP的奖励在每个\\(t=0,1,...,T-1\\)时为0，因此基于上述简化的目标(1.28),MDP在\\(t=T\\)时的奖励设置为随机量\\(\\frac{-e^{aW_T}}{a}.\\)我们将MDP的折现因子设置为\\(\\gamma=1,\\)在时间\\(t\\)时的价值函数（给定策略\\(\\pi=(\\pi_0,\\pi_1,...,\\pi_{T-1})\\)）记为 \\[ V_t^\\pi(W_t)=\\mathbb{E}_\\pi\\left[ -\\frac{e^{-aW_T}}{a}\\mid (t,W_t) \\right] \\] 在时间\\(t\\)时的最优价值函数记为 \\[ V_t^*(W_t)=\\max_{\\pi} V_t^\\pi(W_t)=\\max_\\pi\\left\\{\\mathbb{E}_\\pi\\left[ -\\frac{e^{-aW_T}}{a}\\mid (t,W_t) \\right] \\right\\} \\] 贝尔曼最优方程为(当\\(t=0,1,...,T-2\\)时) \\[ V_t^*(W_t)=\\max_{x_t}Q_t^*(W_t,x_t)=\\max_{x_t}\\left\\{\\mathbb{E}_{Y_t\\sim N(\\mu,\\sigma^2)}\\left[ V_{t+1}^*(W_{t+1}) \\right] \\right\\} \\] 且 \\[ V_{T-1}^*(W_{T-1})=\\max_{x_{T-1}}Q_{T-1}^*(W_{T-1},x_{T-1})=\\max_{x_{T-1}}\\left\\{\\mathbb{E}_{Y_{T-1}\\sim N(\\mu,\\sigma^2)}\\left[ \\frac{-e^{-aW_T}}{a} \\right] \\right\\} \\] 其中\\(Q_t^*\\)是时间\\(t\\)时的最优动作价值函数。 我们通过对最优价值函数的形式进行合理猜测，得到 \\[ V_t^*(W_t)=-b_t e^{-c_tW_t} \\tag{1.30} \\] 其中\\(b_t,c_t\\)与财富\\(W_t\\)无关，接下来我们使用这个最优价值函数的形式来表达贝尔曼最优性方程 \\[ V_t^*(W_t)=\\max_{x_t}\\left\\{\\mathbb{E}_{Y_t\\sim N(\\mu,\\sigma^2)}\\left[ -b_{t+1}e^{-c_{t+1}W_{t+1}} \\right] \\right\\} \\] 利用公式(1.29)可以将其改写为 \\[ V_t^*(W_t)=\\max_{x_t}\\left\\{\\mathbb{E}_{Y_t\\sim N(\\mu,\\sigma^2)}\\left[ -b_{t+1}e^{-c_{t+1}(x_t(Y_t-r)+W_t(1+r))} \\right] \\right\\} \\] 这个指数形式的期望值在正态分布下计算结果为 \\[ V_t^*(W_t)=\\max_{x_t}\\left\\{-b_{t+1}\\exp\\left\\{-c_{t+1}(1+r)W_t-c_{t+1}(\\mu-r)x_t+c_{t+1}^2 \\frac{\\sigma^2}{2} x_{t}^2\\right\\} \\right\\} \\tag{1.31} \\] 又由于\\(V_{t}^*(W_t)=\\max_{x_t}Q^*_t(W_t,x_t),\\)从上面的公式中可以推断出最优动作价值函数\\(Q^*_t(W_t,x_t)\\)的函数形式为 \\[ Q^*_t(W_t,x_t)=-b_{t+1}\\exp\\left\\{-c_{t+1}(1+r)W_t-c_{t+1}(\\mu-r)x_t+c_{t+1}^2 \\frac{\\sigma^2}{2} x_{t}^2\\right\\} \\tag{1.32} \\] 由于贝尔曼最优性方程(1.31)右侧涉及对\\(x_t\\)的最大化操作，我们可以认为最大化操作内的项对于\\(x_t\\)的偏导数是0，这使得我们能够将最优分配\\(x_t^*\\)表示为\\(c_{t+1}\\)的函数，如下所示 \\[ -c_{t+1}(\\mu-r)+\\sigma^2c_{t+1}^2x_t^*=0 \\] 即 \\[ x_t^*=\\frac{\\mu-r}{\\sigma^2 c_{t+1}} \\tag{1.33} \\] 代入贝尔曼最优性方程(1.31)可得 \\[ V_{t}^*(W_t)=-b_{t+1}\\exp\\left\\{ -c_{t+1}(1+r)W_t-\\frac{(\\mu-r)^2}{2\\sigma^2}\\right\\} \\] 但由于 \\[ V_{t}^*(W_t)=-b_t\\exp\\{-c_tW_t\\} \\] 就可以得到如下递推方程 \\[ b_t=b_{t+1}\\exp\\left\\{-\\frac{(\\mu-r)^2}{2\\sigma^2}\\right\\},\\quad c_t=c_{t+1}(1+r) \\] 我们还可以通过知道在\\(t+T\\)终端时刻的MDP的奖励\\(\\frac{-e^{-aW_T}}{a}\\)即终端时刻的财富效用来计算\\(b_{T-1},c_{T-1},\\)然后递推得到\\(b_t,c_t\\)的值。 在\\(t=T-1\\)时： \\[ V_{T-1}^*(W_{T-1})=\\max_{x_{T-1}}\\left\\{\\mathbb{E}_{Y_{T-1}\\sim N(\\mu,\\sigma^2)}\\left[ \\frac{-e^{-aW_T}}{a} \\right] \\right\\} \\] 通过公式(1.29)可以写为 \\[ V_{T-1}^*(W_{T-1})=\\max_{x_{T-1}}\\left\\{\\mathbb{E}_{Y_{T-1}\\sim N(\\mu,\\sigma^2)}\\left[ \\frac{-\\exp\\{-a(x_{T-1}(Y_{T-1}-r)+W_{T-1}(1+r))\\}}{a} \\right] \\right\\} \\] 利用矩母函数的相关知识可以将其改写为 \\[ V_{T-1}^*(W_{T-1})=-e^{-\\frac{(\\mu-r)^2}{2\\sigma^2}}-\\frac{a(1+r)W_{T-1}}{a} \\] 因此 \\[ b_{T-1}=\\frac{e^{-\\frac{(\\mu-r)^2}{2\\sigma^2}}}{a},\\quad c_{T-1}=a(1+r) \\] 递推得到当\\(t=0,1,...,T-2\\)时（当然\\(t=T-1\\)也满足下面的递推式） \\[ b_t=\\frac{e^{-\\frac{(\\mu-r)^2(T-t)}{2\\sigma^2}}}{a},\\quad c_t=a(1+r)^{T-t} \\] 将\\(c_{t+1}\\)带入到(1.33)即可得到最优策略的解 \\[ \\pi_t^*(W_t)=x_t^*=\\frac{\\mu-r}{\\sigma^2a(1+r)^{T-t-1}},\\text{ for all }t=0,1,...,T-1 \\tag{1.34} \\] 请注意最优策略在时间步\\(t\\)时不依赖于状态\\(W_t.\\)因此对于固定时间的最优策略\\(\\pi_t^*\\)是一个常数确定性的策略函数。 将\\(b_t,c_t\\)的解代入公式(1.30)即可得到最优价值函数的解： \\[ V_t^*(W_t)=\\frac{-e^{-\\frac{(\\mu-r)^2(T-t)}{2\\sigma^2}}}{a}\\cdot e^{-a(1+r)^{T-t}W_t} \\text{ for all }t=0,1,...,T-1 \\tag{1.35} \\] 将\\(b_{t+1},c_{t+1}\\)代入到公式(1.32)可以得到最优动作价值函数的解（对于所有的\\(t=0,1,...,T-1\\)） \\[ Q^*_t(W_t,x_t)=\\\\ \\frac{-e^{\\frac{-(\\mu-r)^2(T-t-1)}{2\\sigma^2}}}{a}\\exp\\left\\{-a(1+r)^{T-t}W_t-a(\\mu-r)(1+r)^{T-t-1}x_t+\\frac{(a\\sigma(1+r)^{T-t-1})^2}{2}x_t^2 \\right\\} \\] 1.4.5 现实世界的应用 上面的讨论和设置通过简化假设提供了分析的可解性。使得问题具有解析可解性的具体简化假设包括： 资产回报的正态分布 CRRA/CARA假设 无摩擦市场交易（没有交易成本没有约束，价格/分配数量/消费都是连续的） 但是，现实世界中的动态资产配置和消费问题并不像我们讨论的那么简单和清晰。实际的资产价格波动更加复杂，效用函数不一定符合简单的 CRRA/CARA 形式。实际上，交易通常发生在离散空间中——资产价格、分配数量和消费量往往是离散的。此外，当我们改变资产配置或清算部分投资组合以进行消费时，会产生交易成本。更重要的是，交易并不总是在连续时间内进行——通常会有特定的交易窗口或交易限制。最后，许多投资是流动性差的（如房地产），或者在特定的时间之前无法清算（如退休基金），这对从投资组合中提取资金进行消费构成了重大约束。因此，尽管价格、分配数量和消费可能接近连续变量，但上述摩擦使得我们无法在简化示例中那样利用微积分的便利。 考虑到上述现实世界的因素，我们需要利用动态规划——更具体地说，利用近似动态规划，因为现实世界的问题通常涉及大规模的状态空间和行动空间（即使这些空间不是连续的，它们通常接近连续）。对价值函数的适当函数逼近是解决这些问题的关键。实现一个完整的现实世界投资和消费管理系统超出了本书的范围，但我们可以实现一个具有足够理解的示例，展示如何实现一个完整的现实世界应用。我们要实现的设置包括： 一个风险资产和一个无风险资产； 有限时间步数（类似于上一节的离散时间设置）； 在有限时间结束之前不进行消费（即不从投资组合中提取资金），因此折现因子设置为1； 风险资产回报的任意分布； 随时间变化的无风险资产回报； 任意效用函数； 在每个时间步，有限数量的风险资产投资选择； 初始财富的任意分布。 "],["glm.html", "2 广义线性模型 2.1 广义线性模型的指数族分布", " 2 广义线性模型 2.1 广义线性模型的指数族分布 假设我们观察到数据\\(\\{y_i\\}\\)是独立随机变量\\(\\{Y_i\\}\\)的一个值，广义线性模型由以下成分组成： 随机成分 \\[ Y_i\\sim f(y_i|\\theta_i,\\phi), \\] 其中\\(f(\\cdot)\\)是指数分布族的概率密度函数或概率质量函数。 链接函数\\(g(\\cdot)\\)满足 \\[ \\eta_i=g(\\mu_i), \\] 其中\\(\\mu_i=E(Y_i)\\). 线性预测器成分 \\[ \\eta_i=x_i^T\\beta. \\] 指数族 一族概率密度函数或概率质量函数被称为指数族，如果满足形式 \\[ f(y|\\theta)=a(\\theta)b(y)\\exp\\left( \\sum_{i=1}^kw_i(\\theta)t_i(y) \\right),y\\in A, \\] 其中我们假设： \\(A\\)不依赖于某个\\(k\\)维向量\\(\\boldsymbol{\\theta}\\)​. \\(a(\\theta)&gt;0\\)是不依赖于\\(y\\)的实值函数。 \\(w_i(\\theta),i=1,...,k\\)是不依赖于\\(y\\)的实值函数。 \\(b(y)\\geq0\\)是不依赖于\\(\\theta\\)的实值函数。 \\(t_i(y),i=1,2,...,k\\)是不依赖于\\(\\theta\\)的实值函数 典型形式 与其说\\(\\theta\\)是参数不如令\\(\\eta_i=w_i(\\theta)\\)称为典型或自然参数，概率密度或质量函数就会变为 \\[ f(y|\\boldsymbol{\\eta})=a^*(\\boldsymbol{\\eta})b(y)\\exp\\left( \\sum_{i=1}^k\\eta_it_i(y) \\right),y\\in A. \\] 这种方程的形式不是唯一的。 指数族的示例 所有这些分布都是指数族： 正态分布 Gamma分布 Beta分布 逆高斯分布 二项分布 Poisson分布 负二项分布 示例 正态分布： \\[ f(y|\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left( -\\frac{(y-\\mu)^2}{2\\sigma^2} \\right). \\] 二项分布 \\[ f(y|p)=\\binom{m}{y}p^y(1-p)^{m-y}. \\] 反例 \\[ f(y|\\theta)=\\theta^{-1}\\exp(1-(y/\\theta)),0&lt;\\theta&lt;y&lt;\\infty \\] 不是一个指数族。 \\(\\boldsymbol{\\eta}\\)的充分统计量 假设我们有一组随机变量\\(\\{Y_i\\}\\)且满足\\(Y_i\\sim f(y_i|\\eta)\\)（在规范形势下）。 于是 $$ \\[\\begin{aligned} &amp;\\prod_{i=1}^nf(y_i|\\boldsymbol{\\eta})\\\\ =&amp;\\prod_{i=1}^n\\left[b(y_i)a^*(\\boldsymbol{\\eta})\\exp\\left( \\sum_{j=1}^k\\eta_jt_j(y_i) \\right) \\right]\\\\ =&amp;\\left(\\prod_{i=1}^nb(y_i) \\right)(a^*(\\boldsymbol{\\eta}))^n\\exp\\left(\\sum_{j=1}^k\\eta_j\\sum_{i=1}^nt_j(y_i)\\right). \\end{aligned}\\] $$ 因此\\(\\{ t_j(\\boldsymbol{y})=\\sum_{i=1}^nt_j(y_i):j=1,...,n \\}\\)是\\(\\{\\eta_j\\}\\)的充分统计量。 指数分布族的似然函数 \\[ \\begin{aligned} L(\\boldsymbol{\\eta}|\\boldsymbol{y})&amp;=\\prod_{i=1}^nf(y_i|\\boldsymbol\\eta)\\\\ &amp;=\\left(\\prod_{i=1}^nb(y_i) \\right)(a^*(\\boldsymbol{\\eta}))^n\\exp\\left(\\sum_{j=1}^k\\eta_j\\sum_{i=1}^nt_j(y_i)\\right). \\end{aligned} \\] 于是对数似然函数为 \\[ l(\\boldsymbol{\\eta}|\\boldsymbol{y})=\\sum_{i=1}^n\\log b(y_i)+n\\log a^*(\\boldsymbol{\\eta})+\\sum_{j=1}^k\\eta_jt_j(\\boldsymbol{y}). \\] GLM setup 利用Aitkin et al. (1989) 提出的指数分布族的特定形式来定义 假设\\(\\{Y_i:i=1,...,n\\}\\)是一组随机变量且它们的质量分布函数或质量密度函数为 \\[ f(y_i|\\theta_i,\\phi)=\\exp\\left( \\frac{y_i\\theta_i-b(\\theta_i)}{a(\\phi)}+c(y_i,\\phi) \\right).\\tag{2.1}\\label{aitkin} \\] 其中\\(\\theta_i\\)是规范参数，\\(\\phi\\)是尺度参数。 - 假设有一固定的函数\\(g(\\cdot)\\)使得\\(\\eta_i=g(\\mu_i)=\\boldsymbol{x}_i^T\\beta\\)其中\\(\\mu_i=E(Y_i|X_i).\\)下面将会证明\\(\\theta_i\\)与\\(\\mu_i\\)是相关的。 如何说明\\(f\\)是指数分布族？ 如果\\(\\phi\\)是已知的，则\\(f(\\cdot)\\)是指数分布族。 如果\\(\\phi\\)未知，则\\(f\\)可能不是指数分布族。 广义“线性模型” 令\\(a(\\phi)=\\phi,b(\\theta_i)=\\frac{\\theta_i^2}{2},c(y,\\phi)=-\\frac{1}{2}(y^2/\\phi+\\log (2\\pi\\phi))\\). 于是 \\[\\begin{aligned} f(y_i|\\theta_i,\\phi)&amp;=\\exp\\left( \\frac{y_i\\theta_i-\\theta_i^2/2}{\\phi}-\\frac{1}{2}\\left[ \\frac{y^2}{\\phi}+\\log(2\\pi\\phi) \\right] \\right)\\\\ &amp;=(2\\pi\\phi)^{-1/2}\\exp\\left( -\\frac{(y_i-\\theta_i)^2}{2\\phi} \\right), \\end{aligned}\\] 是正态分布的密度函数。如果我们令 \\[ \\eta_i=\\theta_i=\\mu_i=E(Y_i)=\\boldsymbol{x}_i^T\\beta,\\text{ and }\\phi=\\sigma^2 \\] 我们就得到了线性模型。 \\(Y_i\\)的均值与方差 设\\(Y_i\\sim f(y_i|\\theta_i,\\phi)\\),接下来可以证明 \\[\\begin{aligned} E(Y_i)&amp;=b&#39;(\\theta_i)\\\\ Var(Y_i)&amp;=b&#39;&#39;(\\theta_i)a(\\phi) \\end{aligned}\\] 由\\((\\ref{aitkin})\\)提出的指数分布族的特定形式可以证明 \\[\\begin{align} l(\\theta_i|y_i)=\\frac{y_i\\theta_i-b(\\theta_i)}{a(\\phi)}+c(y_i,\\phi)，\\\\ \\frac{\\partial}{\\partial \\theta_i}l(\\theta_i|y_i)=\\frac{1}{a(\\phi)}[y_i-b&#39;(\\theta_i)]. \\end{align}\\] 由恒等式\\(E\\left[ \\frac{\\partial}{\\partial \\theta_i}l(\\theta_i|y_i) \\right]=0,\\)得 \\[\\begin{align} 0&amp;=\\frac{1}{a(\\phi)}[E(Y_i)-b&#39;(\\theta_i)],a(\\phi)&gt;0\\\\ \\Rightarrow &amp; E(Y_i)=b&#39;(\\theta). \\end{align}\\] 又因为\\(\\frac{\\partial^2}{\\partial \\theta_i^2}l(\\theta_i|y_i)=-\\frac{b&#39;&#39;(\\theta_i)}{a(\\phi)}\\),利用恒等式 \\[ -E\\left[ \\frac{\\partial^2}{\\partial \\theta_i^2}l(\\theta_i|y_i) \\right]=E\\left[ \\frac{\\partial}{\\partial \\theta_i}l(\\theta_i|y_i)\\right]^2 \\] 可以得到 \\[\\begin{align} \\frac{b&#39;&#39;(\\theta_i)}{a(\\phi)}=\\frac{E[Y_i-b&#39;(\\theta_i)]^2}{a(\\phi)^2}&amp;\\\\ i.e.\\; b&#39;&#39;(\\theta_i)=\\frac{E[Y_i-b&#39;(\\theta_i)]^2}{a(\\phi)}&amp;\\\\ i.e.\\;Var(Y_i)=b&#39;&#39;(\\theta_i)a(\\phi)&amp;. \\end{align}\\] 例2.1 二项分布作为指数分布族 设\\(Y_i,i=1,...,n\\)是一列独立且分别服从于二项分布\\(b(n,p_i)\\)的随机变量序列，则 \\[\\begin{align} f(y_i|p_i)&amp;=\\binom{n}{y_i}p_i^{y_i}(1-p_i)^{n-y_i}\\\\ &amp;=\\exp\\left(y_i\\log\\left( \\frac{p_i}{1-p_i} \\right)+n_i\\log(1-p_i)+\\log\\binom{n}{y_i} \\right) \\end{align}\\] 令\\(\\theta_i=\\log\\left( \\frac{p_i}{1-p_i} \\right)\\),即 \\[ p_i=\\frac{e^{\\theta_i}}{1+e^{\\theta_i}} \\] 又因为 \\[ \\log(1-p_i)=\\log\\left(\\frac{1}{1+e^{\\theta_i}} \\right), \\] 于是我们有 \\[ f(y_i|p_i,\\phi)=\\exp\\left(\\frac{y_i\\theta_i-b(\\theta_i)}{a(\\phi)}+c(y_i,\\phi) \\right), \\] 其中 \\[\\begin{align} &amp;a(\\phi)=\\phi=1,\\\\ &amp;b(\\theta_i)=n\\log(1+e^{\\theta_i}),\\\\ &amp;c(y_i,\\phi)=\\log\\binom{n}{y_i}. \\end{align}\\] 二项分布的期望和方差也可以通过上面提到的规范形式导出，正是我们很熟悉的结果。 Poisson分布作为指数分布族 设\\(Y_i,i=1,...,n\\)是一列独立且分别服从于Poisson分布\\(P(\\lambda_i)\\)的随机变量序列，则 \\[\\begin{align} f(y_i|\\lambda_i,\\phi)&amp;=\\frac{\\lambda_i^{y_i}e^{-\\lambda_i}}{y_i!}\\\\ &amp;=\\exp(y_i\\log(\\lambda_i)-\\lambda_i-\\log(y_i!)), \\end{align}\\] 令\\(\\theta_i=\\log\\lambda_i,\\lambda_i=e^{\\theta_i}\\),于是有 \\[\\begin{align} f(y_i|\\lambda_i,\\phi)&amp;=\\exp(y_i\\theta_i-e^{\\theta_i}-\\log(y_i!)),\\\\ &amp;=\\exp\\left(\\frac{y_i\\theta_i-b(\\theta_i)}{a(\\phi)}+c(y_i,\\phi) \\right), \\end{align}\\] 其中 \\[\\begin{align} &amp;a(\\phi)=\\phi=1,\\\\ &amp;b(\\theta_i)=e^{\\theta_i},\\\\ &amp;c(y_i,\\phi)=\\log(y_i!). \\end{align}\\] 也可以通过\\(b(\\theta_i)\\)的\\(1,2\\)阶导数导出其期望与方差。 链接函数 链接函数是一个函数\\(g(\\cdot)\\)使得\\(\\eta_i=g(\\mu_i),\\)其中\\(\\mu_i=E(Y_i).\\)我们在上面已经证明了\\(\\mu_i=E(Y_i)=b&#39;(\\theta_i).\\)因此\\(g(\\cdot)\\)将均值\\(\\mu_i\\)或典型参数\\(\\theta_i\\)与\\(\\eta_i\\)连接起来。我们通常假设\\(g(\\cdot)\\)是一个双射且连续可微的函数。 典型链接函数 典型链接函数是指\\(g(\\cdot)\\)使得\\(\\eta_i=\\theta_i\\). 这表明了 \\[ \\eta_i=g(b&#39;(\\theta_i))=\\theta_i, \\] 因此\\(g(b&#39;(\\cdot))\\)必是恒等函数。 下面是典型链接函数的例子： 正态分布中\\(\\eta_i=\\theta_i=\\mu_i\\),于是典型链接函数就是恒等函数。 二项分布中\\(\\eta_i=\\theta_i=\\log(\\frac{p_i}{1-p_i})=\\log(\\frac{\\mu_i}{1-\\mu_i})\\),于是典型连接函数是对数几率比函数(logit link function: log odds ratio). Poisson分布中\\(\\eta_i=\\theta_i=\\log(\\lambda_i)=\\log(\\mu_i)\\),于是典型链接函数是对数函数。 "],["RFWLFR.html", "3 Random forest weighted local Frechet regression with random objects 3.1 提出的方法", " 3 Random forest weighted local Frechet regression with random objects 《带有随机对象的随机森林加权局部 Frechet 回归》这篇文章是中国人民大学统计学院2025年春季研究生课程广义线性模型的小组汇报论文。小组成员除了我还包括Huang Jinming &amp; Yang Zhe.这是论文原文。 3.1 提出的方法 在@ref(Local constant method)节正式展示方法之前，需要进行一些初步的准备工作，包括提供Frechet回归的背景介绍，并解释Frechet树的构建过程，这是我们方法的基本组成部分。 3.1.1 预备知识 (#rfwlfr21带数字是否可以？？) 3.1.1.1 Frechet回归 设\\((\\Omega,d)\\)为一个配备特定度量\\(d\\)的度量空间，\\(\\mathcal{R}^p\\)是一个\\(p\\)维的欧氏空间。我们考虑一个随机对\\((X,Y)\\sim F,\\)其中\\(X\\sim\\mathcal{R}^p,Y\\sim \\Omega\\)并且\\(F\\)是\\((X,Y)\\)的联合分布。记\\(X,Y\\)的边际分布分别为\\(F_X,F_Y\\),条件分布\\(F_{X|Y},F_{Y|X}\\)也假设存在。当\\(\\Omega\\subseteq \\mathcal{R}\\)时，经典回归的目标是估计条件均值 \\[ m(x)=E(Y\\mid X=x)=\\mathop{\\arg\\min}_{y\\in\\mathcal{R}} E\\left\\{(Y-y)^2\\mid X=x\\right\\}. \\] 通过将欧几里得距离替换成\\(\\Omega\\)的内在度量\\(d,\\)条件Fr'echet均值可以定义为 \\[ m_{\\oplus}(x)=\\mathop{\\arg\\min}_{y \\in \\Omega} M_{\\oplus}(x,y)=\\mathop{\\arg\\min}_{y \\in \\Omega} E\\big\\{d^{2}(Y, y) \\mid X=x\\big\\}. \\] 给定一个独立同分布的训练样本\\(\\mathcal{D}_n=\\{(X_i,Y_i)\\}_{i=1}^n\\)其中\\((X_i,Y_i)\\sim F\\),Fr'echet回归的目标是在样本层面估计\\(m_{\\oplus}(x)\\),为此Hein (2009) 将Nadaraya-Watson回归推广到Frechet版本： \\[ \\hat{m}_{\\oplus}^{\\text{NW}}(x)=\\mathop{\\arg\\min}_{y \\in \\Omega} \\frac{1}{n} \\sum_{i=1}^{n} K_{h}\\left(X_{i}-x\\right) d^{2}\\left(Y_{i}, y\\right), \\] 其中\\(K\\)是一个平滑核（如Epanechnikov核或高斯核），\\(h\\)为带宽，\\(K_h(\\cdot)=h^{-1}K(\\cdot/h)\\).Petersen和Müller（2019）将标准的多元线性回归和局部线性回归重新表述为加权Fréchet均值的函数，并提出了全局Fréchet回归和局部Fréchet回归： \\[ \\hat{m}_{\\oplus}(x)=\\mathop{\\arg\\min}_{y \\in \\Omega} \\frac{1}{n}\\sum_{i=1}^{n} s_{i n}(x) d^{2}\\left(Y_{i}, y\\right), \\] 其中\\(s_{in}(x)\\)在全局和局部Fr'echet回归中具有不同的表示形式。 Nadaraya-Watson Fr'echet回归和局部Fréchet回归都涉及核加权函数\\(K\\)，这在\\(p ≥ 3\\)时限制了它们的应用。为了解决这个问题，我们的目标是借助随机森林的力量，生成一个更强大的加权函数，以应对适度较大的\\(p\\). 图3.1展示了纽约曼哈顿黄色出租车交通流统计和预测结果。这个问题可以表述为一个Fréchet回归问题，其中响应变量是一个网络（矩阵），并且考虑了14个预测变量。特别地，当\\(p = 14\\)时，Nadaraya-Watson Fréchet回归和局部Fréchet回归方法显示出显著的局限性。在这里，我们使用Fréchet充分维数约简（Ying和Yu，2022）来实现局部Fréchet回归。尽管全局Fréchet回归不受维度限制，但它依赖于线性假设才能达到令人满意的效果。相反，从图3.1可以明显看出，我们将在本文中提出的两种方法的预测准确性高于全局Fréchet回归。 图3.1: The first plot illustrates the flow statistics of yellow taxis in ten distinct zones of Manhattan, New York, during a certain time period. The thickness of the edges connecting vertices corresponds to the level of inter-zone traffic, while the size of vertices represents the total traffic volume within each zone. The remaining five plots from left to right are the predictions given by the global Fr’echet regression, local Fr’echet regression after dimension reduction, single index Fr’echet regression, RFWLCFR and RFWLLFR. 3.1.1.2 Frechet树 回归树\\(T\\)从根节点（整个输入空间）递归地划分输入空间。在每次划分时，父节点根据某个特征方向和某个切割点被分成两个子节点，这些切割点由特定的划分标准决定。经过多次划分后，子节点变得足够小，形成叶节点，叶节点中的样本数据用于估计条件（Fréchet）均值。 在本文中，我们使用Fréchet树来指代处理度量空间值响应的回归树，不管其划分标准是什么。这里我们引入了一种自适应标准——方差减少划分标准，它利用预测变量\\(X=(X^{(1)},...,X^{(p)})\\)和响应变量\\(Y\\)的信息来决定节点的划分。\\(Y\\)来自一般度量空间的杂质(impurity)不再由欧几里得距离下的方差来度量而是使用Fr'echet方差。一个内部节点\\(A\\)的分裂可以用一对\\((j,c),j\\in\\{1,...,p\\}\\)来表示，表示在特征\\(X^{(j)}\\)方向上，节点\\(A\\)在位置\\(c\\)被分裂，我们选取最优的\\((j_n^*,c_n^*)\\)使得样本Frechet方差尽可能减小，以至于在同一子节点的样本具有高度的相似性。具体地，分裂准则如下 \\[ \\mathcal{L}_n(j,c)=\\frac{1}{N_n(A)}\\left\\{\\sum_{i:X_i\\in A}d^2(Y_i,\\bar{Y}_A) -\\sum_{i:X_i\\in A_{j,l}}d^2(Y_i,\\bar{Y}_{A_{j,l}})-\\sum_{i:X\\in A_{j,r}}d^2(Y_i,\\bar{Y}_{A_{j,r}}) \\right\\}, \\] 其中\\(A_{j,l}=\\{ x\\in A:x^{(j)}&lt;c \\},A_{j,r}=\\{ x\\in A:x^{(j)}\\geq c \\},N_n(A)\\)是落在节点\\(A\\)中的样本数量，并且\\(\\bar{Y}_A=\\mathop{\\arg\\min}_{y\\in\\Omega} \\sum_{i:X_i\\in A}d^2(Y_i,y)\\)也就是节点\\(A\\)中样本\\(Y_i\\)的Frechet均值，\\(\\bar{Y}_{A_{j,l}},\\bar{Y}_{A_{j,r}}\\)也是类似定义的。于是最优分裂对由下面的式子决定 \\[ (j_n^*,c_n^*)=\\mathop{\\arg\\max}_{j,c} \\mathcal{L}_n(j,c). \\] 3.1.2 Local constant method 单棵树模型可能会因为调参的不同而面临较大的偏差或方差。为了提高预测精度，我们可以聚合多棵树来构建随机森林。随机森林的预测误差与不同树之间的相关性密切相关。除了通过重采样训练数据集来生长个别树之外，通常还会引入额外的随机性，以进一步降低树之间的相关性，从而提高随机森林的性能。例如，每次分裂前会随机选择一个特征子集，分裂方向仅基于该子集来设计。在这里，我们用\\(\\xi\\sim\\Xi\\)表示这种辅助随机性。 我们首先考虑经典的欧几里得响应的随机森林。每棵树是在训练数据集\\(\\mathcal{D}_n\\)的一个子样本\\(\\mathcal{D}_n^b=\\{(X_{i_{b,1}},Y_{i_{b,1}}),(X_{i_{b,2}},Y_{i_{b,2}}),...,(X_{i_{b,s_n}},Y_{i_{b,s_n}})\\}\\)上训练的，其中\\(1\\leq i_{b,1}&lt; i_{b,2}&lt;...&lt;i_{b,s_n}\\leq n.\\)本文中假设子样本大小\\(s_n\\rightarrow \\infty,s_n/n\\rightarrow 0\\)当\\(n\\rightarrow \\infty\\)时。数据重采样是无放回的。由\\(\\mathcal{D}_n^b\\)和随机抽取\\(\\xi_b\\sim\\Xi\\)构造的第\\(b\\)棵树\\(T_b\\)给出了\\(m(x)\\)的估计： \\[ T_b(x;\\mathcal{D}_n^b,\\xi_b)=\\frac{1}{N(L_b(x;\\mathcal{D}_n^b,\\xi_b))}\\sum_{i:X_i\\in L_b(x;\\mathcal{D}_n^b,\\xi_b)} Y_i, \\] "],["qrm.html", "4 量化风险管理 4.1 风险管理的基本概念", " 4 量化风险管理 4.1 风险管理的基本概念 4.1.1 建模价值和价值变动 风险映射 在描述金融风险的一般数学模型中，通常把未来世界状态的不确定性表示为概率空间\\((\\Omega,\\mathcal{F},P)\\),这是接下来要介绍的所有随机变量的定义域。考虑风险（或者损失\\(L\\)）的随机变量\\(X:\\Omega\\rightarrow \\mathbb{R}\\). 考虑一个给定的由资产和可能负债组成的投资组合。这个投资组合在时间\\(t\\)（当前时刻）的价值记为\\(V_t\\)(于是\\(V_{t+1}\\)是一随机变量，在假设\\(t\\)时刻\\(V_t\\)已知)。我们考虑一个给定的时间展望期\\(\\Delta t\\)并假设 在\\(\\Delta t\\)中投资组合成分不变; 在\\(\\Delta t\\)中没有流动和流出. 显然这些假设只能针对比较短的\\(\\Delta t,\\)在长期中不太可能实现这些假设。 投资组合价值的变化由下面的式子给出 \\[ \\Delta V_{t+1}=V_{t+1}-V_t \\] 同时也能给出损失的定义\\(L_{t+1}:=-\\Delta V_{t+1}.\\)在量化风险管理中我们主要考虑的是损失。 Remark 2.1 随机变量\\(L_{t+1}\\)的分布称为损失分布。 风险管理从业者通常比较关注所谓的损益（P&amp;L）分布即为\\(-L_{t+1}=\\Delta V_t\\)的分布。 对于较长的时间间隔，可以选择把损失定义为\\(L_{t+1}:=V_t-V_{t+1}/(1+r_{t,1})\\),其中\\(r_{t,1}\\)是指从\\(t\\)时刻到\\(t+1\\)时刻简单的无风险利率，它度量了时间\\(t\\)的单位货币损失，但在书中基本忽略这个问题。 通常来说，投资组合价值\\(V_t\\)是由时间函数和风险因子的\\(d\\)维随机向量\\(\\boldsymbol{Z}_t=(Z_{t,1},...,Z_{t,d})&#39;\\)构建的模型表示的，对于一些可测函数\\(f:\\mathbb{R}_+\\times \\mathbb{R}^d\\)来说可以写出表达式 \\[ V_t=f(t,\\boldsymbol{Z}_t) \\] 风险因子通常被假设是可以被观察到的，因此在\\(t\\)时刻随机向量\\(\\boldsymbol{Z}_t\\)表示为实现价值\\(\\boldsymbol{z}_t\\),投资组合价值\\(V_t\\)由实现价值\\(f(t,\\boldsymbol{z}_t)\\)表示。下面提供一些关于映射过程的例子。 我们将一定时间范围内风险因子变化定义为随机向量\\(\\boldsymbol{X}_{t+1}:=\\boldsymbol{Z}_{t+1}-\\boldsymbol{Z}_t\\),假定当前时间为\\(t\\),投资组合的损失可以由下式得出： \\[ L_{t+1}=-(f(t+1,\\boldsymbol{z}_t+\\boldsymbol{X}_{t+1})-f(t,\\boldsymbol{z}_t)) \\] 由该式可以看出损失分布由风险因子变化\\(\\boldsymbol{X}_{t+1}\\)的分布所决定。 因此我们可以把\\(L_{t+1}\\)写作\\(L(\\boldsymbol{X}_{t+1})\\),其中损失算子\\(L(\\boldsymbol{x}):=-(f(t+1,\\boldsymbol{z}_t+\\boldsymbol{x})-f(t,\\boldsymbol{z}_t))\\). 如果\\(f\\)是可微的，其一阶泰勒近似（由于\\(f(\\boldsymbol{y})\\approx f(\\boldsymbol{y}_0)+\\nabla f(\\boldsymbol{y}_0)^T(\\boldsymbol{y}-\\boldsymbol{y}_0)\\)其中 \\(\\boldsymbol{y}=(t+1,Z_{t,1}+X_{t+1,1},...,Z_{t,d}+X_{t+1,d})\\), \\(\\boldsymbol{y}_0=(t,Z_{t,1},...,Z_{t,d})\\)） 就是 \\[ f(t+1,\\boldsymbol{Z}_t+\\boldsymbol{X}_{t+1})\\approx f(t,\\boldsymbol{Z}_t)+f_t(t,\\boldsymbol{Z}_t)\\cdot 1+\\sum_{j=1}^df_{z_j}(t,\\boldsymbol{Z}_t)\\cdot X_{t+1,j} \\] 因此我们可以定义损失的一阶近似\\(L_{t+1}^\\Delta\\)形式， \\[\\begin{align} L_{t+1}^\\Delta:&amp;=-\\left( f_t(t,\\boldsymbol{z}_t)+\\sum_{j=1}^df_{z_j}(t,\\boldsymbol{z}_t)X_{t+1,j} \\right)\\\\ &amp;=-(c_t+\\boldsymbol{b}_t&#39;\\boldsymbol{X}_{t+1}), \\tag{4.1} \\end{align}\\] 这个式子将损失表示为风险因子变化的线性函数。如果风险因子变化很小（即如果我们在很短的时间范围内测量风险）并且投资组合的价值与风险因子线性相关（即如果函数\\(f\\)的二阶导数很小）则上式中的近似质量非常好。 现在通过有关市场风险、信用风险和保险领域的一些例子说明典型的风险管理问题如何适用于这个框架之中。 例 2.1 股票投资组合 考虑一个包含\\(d\\)种股票的固定投资组合，\\(t\\)时刻投资组合中股票\\(i\\)的份额用\\(\\lambda_i\\)表示，股票\\(i\\)的价格过程用\\((S_{t,i})_{t\\in\\mathbb{N}}\\)表示。根据金融和风险管理的标准做法，使用自然对数价格作为风险因子，即\\(Z_{t,i}:=\\ln S_{t,i},i=1,...,d.\\)于是我们有 \\[ V_t=f(t,\\boldsymbol{Z_t})=\\sum_{i=1}^d\\lambda_i\\exp(Z_{t,i}) \\] 风险因子变化表示为\\(X_{t+1,i}=\\ln S_{t+1,i}-\\ln S_{t,i}\\),相当于投资组合中股票的对数收益。从\\(t\\)时刻到\\(t+1\\)时刻投资组合的损失可以由下式得出： \\[ L_{t+1}=-(V_{t+1}-V_t)=-\\sum_{i=1}^d\\lambda_iS_{t,i}(e^{X_{t+1,i}}-1) \\] 线性化的损失可以写为 \\[ L_{t+1}^\\Delta=-\\sum_{i=1}^d\\lambda_iS_{t,i}X_{t+1,i}=-V_t\\sum_{i=1}^dw_{t,i}X_{t+1,i} \\] 其中权重\\(w_{t,i}:=(\\lambda_iS_{t,i})/V_t\\)是指\\(t\\)时刻投资于股票\\(i\\)的投资组合的价值比例。当然也可以通过\\(f_{z_j}(t,\\boldsymbol{Z}_t)=\\lambda_i\\exp(Z_{t,i})=\\lambda_iS_{t,i}\\overset{d}{=}\\tilde{w}_{t,i}=V_t w_{t,i}\\)如下改写线性化损失 \\[ L^\\Delta_{t+1}=\\sum_{i=1}^d\\tilde{w}_{t,i}X_{t+1,i}=-\\tilde{\\boldsymbol{w}}^T_tX_{t+1} \\] 这是前面一阶近似的线性损失(\\(\\ref{lt}\\))的特例，只需\\(c_t=0,\\boldsymbol{b}_t=\\tilde{\\boldsymbol{w}_t}.\\) 如果\\(\\mathbb{E}\\boldsymbol{X}_{t+1}=\\boldsymbol{\\mu},\\text{cov}\\boldsymbol{X}_{t+1}=\\Sigma\\)已知，可以得到 \\[\\begin{align} \\mathbb{E}L_{t+1}^\\Delta&amp;=-\\sum_{i=1}^d\\tilde{w}_{t,i}\\mathbb{E}(X_{t+1,i})=-\\tilde{\\boldsymbol{w}}_t^T\\boldsymbol{\\mu},\\\\ \\text{var}L_{t+1}^\\Delta&amp;=\\text{var}(\\tilde{\\boldsymbol{w}}^T_tX_{t+1})=\\tilde{\\boldsymbol{w}}^T_t\\text{cov}(\\boldsymbol{X}_{t+1})\\tilde{\\boldsymbol{w}}_t =\\tilde{\\boldsymbol{w}}^T_t\\Sigma\\tilde{\\boldsymbol{w}}_t. \\end{align}\\] 例2.2 欧式看涨期权（European call option） 考虑一个有关衍生证券投资组合的简单例子，也就是一个建立在零股息股票上的到期日为\\(T,\\)行权价为\\(K\\)的标准欧式看涨期权。我们用BS期权定价公式来确定投资组合的价值。在时间\\(t\\)时，价格为\\(S_t\\)的股票的看涨期权价值如下 \\[ V_t=C^{\\text{BS}}(t,S_t;r,\\sigma,K,T):=S_t\\Phi(d_1)-Ke^{-r(T-t)}\\Phi(d_2), \\] 其中\\(\\Phi\\)是标准正态分布的分布函数，\\(r\\)是连续复利无风险利率，\\(\\sigma\\)是标的股票对数收益率的波动率，并且 \\[ d_1=\\frac{\\ln (S/K)+(r+\\frac{1}{2}\\sigma^2)(T-t)}{\\sigma\\sqrt{T-t}},\\quad d_2=d_1-\\sigma\\sqrt{T-t} \\] 为了简便起见，假设距离期权到期的时间\\(T-t\\)以投资期为一单位来度量（即一年一年），同时参数\\(r,\\sigma\\)​也采取了同样的度量单位（即年化收益率和波动率）。 然而实际情况下除了\\(\\ln S_t,r_t,\\sigma_t\\)也都是风险因子，因此我们定义风险因子向量如下 \\[ \\boldsymbol{Z}_t=(\\ln S_t,r_t,\\sigma_t)&#39;\\Rightarrow \\boldsymbol{X}_{t+1}=(\\ln (S_{t+1}/S_t),r_{t+1}-r_t,\\sigma_{t+1}-\\sigma_t)&#39;. \\] 后一个式子是风险因子的变动，可以发现有三种风险因子。 这意味着风险因素下的映射\\(f\\)由以下公式给出： \\[ V_t=C^{\\text{BS}}(t,e^{Z_{t,1}};Z_{t,2},Z_{t,3},K,T)=:f(t,\\boldsymbol{Z}_t) \\] 而线性化的损失（忽略\\(C^{\\text{BS}}\\)的参数）即为 \\[\\begin{align} L_{t+1}^\\Delta =&amp; -(f_t(t,\\boldsymbol{Z}_t)+\\sum_{i=1}^3f_{z_i}(t,\\boldsymbol{Z}_t)X_{t+1,i})\\\\ =&amp;-(C_t^{\\text{BS}}\\Delta t+C_{S_t}^{\\text{BS}}S_tX_{t+1,1}+C_{r_t}^{\\text{BS}}X_{t+1,2}+C_{\\sigma_t}^{\\text{BS}}X_{t+1,3}). \\end{align}\\] 如果我们风险管理时间的范围是1天而不是1年，我们需要在这里引入\\(\\Delta t:=1/250.\\)因为上述都是年化利率和波动率。于是我们就引入风险管理常见的希腊字母了！\\(C_t^{\\text{BS}}\\)就是期权的\\(theta;C_{S_t}^{\\text{BS}}\\)是\\(delta;C_{r_t}^{\\text{BS}}\\)是\\(rho;C_{\\sigma_t}^{\\text{BS}}\\)是\\(vega.\\) 估值方法 风险中性估值 风险中性估值广泛用于金融产品的定价，例如衍生品。在风险中性定价中，金融工具的现值是由未来现金流的预期贴现值计算得到的。其中预期值的确定与某种概率测度\\(Q\\)相关，这种测度被称为风险中性定价测度。\\(Q\\)是一种人工测度，将可交易证券的贴现价格变成鞅，因此是公平下注，也叫等价鞅测度（EMM）。与现实世界/物理测度\\(\\mathbb{P}\\)相对。风险中性定价测度是一个概率测度\\(Q,\\)使得这点牛\\(Q\\)的贴现收益的期望等于\\(V_0.\\) 在\\(T\\)时，时间\\(t\\)时的债权\\(H\\)的风险中性估值通过风险中性定价规则进行： \\[ V_t^H=\\mathbb{E}^Q_{t}(e^{-r(T-t)}H),\\;t&lt;T, \\] 其中\\(\\mathbb{E}_{Q,t}\\)表示在时间\\(t\\)及之前的信息下针对\\(Q\\)的期望。\\(\\mathbb{P}\\)是基于历史数据估计的；\\(Q\\)是根据市场价格进行校准的。交易证券的价格用于在风险中性测度\\(P\\)下校准模型参数，然后用这一测度用于给非交易产品定价。 例2.4 欧式看涨期权（续） 假设行权价为\\(K\\)或到期时间为\\(T\\)的期权没有被交易，但以该股票为标的的其他期权产品是有交易的。给定\\(\\mu\\in\\mathbb{R}\\)是漂移项、\\(\\sigma&gt;0\\)是波动率，\\((W_t)\\)是一个标准布朗运动，我们假设在真实测度\\(\\mathbb{P}\\)下，股票价格\\((S_t)\\)遵循几何布朗运动模型，即所谓的B-S模型，其动态过程由下式表示 \\[ dS_t=\\mu S_tdt+\\sigma S_tdW_t \\] 众所周知，存在等价鞅测度\\(Q,\\)其中股票价格折现值\\((e^{-rt}S_t)\\)是鞅，在\\(Q\\)下股价遵循具有漂移项\\(r\\)和波动率\\(\\sigma\\)的几何布朗运动模型。欧式看涨期权到期收益\\(H=(S_T-K)^+=\\max\\{S_T-K,0\\}\\)于是风险中性估值公式就可以写作 \\[ V_t=\\mathbb{E}_t^Q(e^{-r(T-t)}(S_T-K)^+)=C^{\\text{BS}}(t,s_t;r,\\sigma,K,T),\\quad t&lt;T; \\tag{4.2} \\] 为了在\\(t\\)时刻给定看涨期权一个风险中性定价（知道股票的当前价格\\(S_t\\)、利率\\(r\\)以及期权特征\\(K,T\\)），我们需要校准模型参数\\(\\sigma\\).如上所述，我们通常使用具有不同特征的股票期权的报价\\(C^{\\text{BS}}(t,S_t;r,\\sigma,K^*,T^*)\\)来推断\\(\\sigma.\\)然后将所谓的隐含波动率代入公式 (4.2). 风险中性定价有两个理论支持。首先，数理金融的标准结果（所谓的资产定价第一基本定理）指出，当且仅当证券定价模型承认至少有一个等价鞅测度\\(Q\\)时，该模型是无套利的。因此，如果一个金融产品按照无套利原则定价，则其价格必须由一些风险中性测度\\(Q\\)的风险中性定价公式给出。第二个理由是对冲：在金融模型中，通常可以通过资产交易来复制金融产品的收益率，其中一种就是动态对冲，在一个无摩擦市场中实施对冲是由风险中性定价规则给定的。 损失分布 在确定映射\\(f\\)后，我们可以确定量化风险管理（QRM）的以下关键统计任务： 1. 找到\\(\\boldsymbol{X}_{t+1}\\)的统计模型（通常是基于历史数据估计的\\(\\boldsymbol{X}_{t+1}\\)预测模型）； 2. 计算/推导\\(L_{t+1}\\)的分布函数\\(F_{L_{t+1}}\\)（需要\\(f(t+1,\\boldsymbol{Z}_t+\\boldsymbol{X}_{t+1})\\)）; 3. 从\\(F_{L_{t+1}}\\)计算风险度量 有三种方法解决这些问题，分别是解析法、历史模拟法和蒙特卡洛方法。下面介绍历史模拟法和蒙特卡洛方法。 历史模拟法 基于 \\[ L_k=L(X_k)=-(f(t+1,\\boldsymbol{Z}_t+\\boldsymbol{X}_k)-f(t,\\boldsymbol{Z}_t)), \\tag{4.3} \\] 其中\\(k\\in\\{ t-n+1,...,t \\}.L_{t-n+1},...,L_t\\)显示了如果风险因子在过去\\(n\\)个时间步的变化再次发生，当前的投资组合将会发生什么变化。 通过经验分布函数估计\\(F_{L_{t+1}}\\): \\[ \\hat{F}_{L_{t+1},n}(x)=\\frac{1}{n}\\sum_{i=1}^nI_{\\{L_{t-i+1}\\leq x\\}},\\; x\\in \\mathbb{R}, \\tag{4.4} \\] 蒙特卡洛方法 为\\(\\boldsymbol{X}_{t+1}\\)选择合适的模型，模拟\\(\\boldsymbol{X}_{t+1}\\)，计算相应的损失，如公式(4.3),并如公式(4.4)估计\\(F_{L_{t+1}}\\). "],["stochastic.html", "5 随机分析 5.1 Introduction 5.2 预备知识", " 5 随机分析 5.1 Introduction 一般地，我们称系数可以是随机的微分方程为随机微分方程（Stochastic Differential Equation），显然随机微分方程的解一定具有随机性，因此我们只期望得到关于解的概率分布。 5.1.1 1.4 最优停时（Optimal Stopping） 假定某人计划卖掉一个资产，在开放的市场中，他的资产在\\(t\\)时刻的价格\\(X_t\\)满足随机微分方程 \\[ \\frac{dX_t}{dt}=rX_t+\\alpha X_t\\cdot [\\text{noise}], \\] 这里的\\(r,\\alpha\\)为已知的常数，折现率为已知常数\\(\\rho\\)，那么在什么时候卖掉该资产为最好？ 假设知道现在时刻\\(t\\)以前的资产表现\\(X_s(s&lt;t)\\)但是由于系统中的噪声，当然无法确信选择卖的时间是否为最优，因此要找一个停时策略，在长期运行中它应该是最好的结果，即把通胀考虑进去以后的最大化期望利润。这是一个最优停时问题。 5.1.2 1.5 随机控制（Stochastic Control） 假设某人有两个投资可能性： 无风险投资（如债券）.在\\(t\\)时刻每单位的价格\\(X_0(t)\\)按指数增长： \\[ \\frac{dX_0}{dt}=\\rho X_0 \\] 这里的\\(\\rho\\)为大于0的常数 有风险投资（如股票）.在\\(t\\)时刻每单位的价格\\(X_1(t)\\)满足随机微分方程： \\[ \\frac{dX_1}{dt}=(\\mu+\\sigma\\cdot[\\text{noise}])X_1 \\] 在每个时刻\\(t\\)，该投资者选择他的财富\\(V_t\\)中多大比例\\(u_t\\)用于风险投资，从而另一部分用于无风险投资，给定效用函数\\(U\\)和终端时刻\\(T\\),该投资问题是找到一个最优证券组合，使得终端时刻财富\\(V_T\\)的期望效用最大 \\[ \\mathop{max}\\limits_{0\\leq u_t\\leq1}\\{E[U(V_T^{(u)})]\\} \\] 5.2 预备知识 5.2.1 概率空间 随机变量 随机过程 定义2.1.1 给定全集合\\(\\Omega\\)，那么\\(\\Omega\\)上的\\(\\sigma\\)代数\\(\\mathcal{F}\\)是由\\(\\Omega\\)的某些子集构成的集合族且具有下列性质 \\(\\varnothing\\in\\mathcal{F}\\); \\(F\\in\\mathcal{F}\\Rightarrow F^C\\in\\mathcal{F}\\); \\(A_1,A_2,\\dots\\in\\mathcal{F}\\Rightarrow A:=\\mathop{\\cup}\\limits_{i=1}^{\\infty}A_i\\in\\mathcal{F}\\). 称\\((\\Omega,\\mathcal{F})\\)为一个可测空间，这个可测空间的概率测度\\(P\\)是一个实值函数，将\\(\\mathcal{F}\\)映射到\\([0,1]\\)上，满足概率测度的两个条件 \\(P(\\varnothing)=0,P(\\Omega)=1\\); 若\\(A_1,A_2,\\dots\\in\\mathcal{F}\\)且\\(\\{A_i\\}_{i=1}^{\\infty}\\)是互不相交，那么 \\[ P(\\mathop{\\cup}_{i=1}^{\\infty}A_i)=\\mathop{\\sum}_{i=1}^{\\infty}P(A_i). \\] 称\\((\\Omega,\\mathcal{F},P)\\)为一个概率空间。我们假定所有概率空间都是完备的，即\\(\\mathcal{F}\\)包括了\\(\\Omega\\)中\\(P\\)外测度为零的所有子集。 对\\(\\Omega\\)中的某一子集\\(F\\)，如果\\(F\\in\\mathcal{F}\\),则称\\(F\\)为\\(\\mathcal{F}\\)可测集，在概率上称为事件，\\(P(F)\\)就称为事件\\(F\\)发生的概率。特别地，\\(P(F)=1\\)则说事件\\(F\\)为依概率1发生或者几乎必然(a.s.)发生。 对给定的\\(\\Omega\\)的一个集合族\\(\\mathcal{U}\\),存在一个包含\\(\\mathcal{U}\\)的最小\\(\\sigma\\)代数\\(\\mathcal{H}_\\mathcal{U}\\),即 \\[ \\mathcal{H}_\\mathcal{U}=\\cap\\{\\mathcal{H}:\\mathcal{H}为\\Omega上的\\sigma代数，\\mathcal{U}\\subset\\mathcal{H}\\} \\] ，称\\(\\mathcal{H}_{\\mathcal{U}}\\)是由\\(\\mathcal{U}\\)生成的\\(\\sigma\\)代数（包含\\(\\mathcal{U}\\)的最小集合，所以做了交集）。 例如，\\(\\mathcal{U}\\)是拓扑空间\\(\\Omega\\)的所有开子集构成的集合(如\\(\\Omega=\\mathbf{R}^n\\))，那么\\(\\mathcal{B}=\\mathcal{H}_\\mathcal{U}\\)称为\\(\\Omega\\)上的Borel \\(\\sigma\\)代数。对任意元素\\(B\\in\\mathcal{B}\\)称为Borel可测集。\\(\\mathcal{B}\\)包含所有的开子集、所有的闭子集、所有的可数个闭子集的并集以及所有的可数个这种并集的交集等等。 设\\((\\Omega,\\mathcal{F},P)\\)是给定的概率空间，如果 \\[ Y^{-1}(U):=\\{\\omega\\in\\Omega;Y(\\omega)\\in U\\}\\in\\mathcal{F} \\] 对所有开集\\(U\\in\\mathbf{R}^n\\)（或等价地，对所有Borel集\\(U\\in\\mathbf{R}^n\\)）均成立，那么函数\\(Y:\\Omega\\rightarrow\\mathbf{R}^n\\)称为\\(\\mathcal{F}\\)可测的。（这实际上有点像 Y是一个随机变量？） 若\\(X:\\Omega\\rightarrow \\mathbf{R}^n\\)是任意一个函数，那么由\\(X\\)生成的\\(\\sigma\\)代数\\(\\mathcal{H}_X\\)是\\(\\Omega\\)上的包含所有形如\\(X^{-1}(U)\\)（\\(U\\in\\mathbf{R}^n\\)为开集）的最小\\(\\sigma\\)代数。不难证明 \\[ \\mathcal{H}_X=\\{X^{-1}(B);B\\in\\mathcal{B}\\}, \\] 这里\\(\\mathcal{B}\\)是\\(\\mathbf{R}^n\\)上的Borel \\(\\sigma\\)代数。显然\\(X\\)是\\(\\mathcal{H}_X\\)可测的，而\\(\\mathcal{H}_X\\)是具有上述性质的最小\\(\\sigma\\)代数。（可以当作事件域？） 引理2.1.2 如果\\(X,Y:\\Omega\\rightarrow\\mathbf{R}^n\\)是两个给定的函数，\\(Y\\)为\\(\\mathcal{H}_X\\)可测的充要条件是存在一个Borel可测函数\\(g:\\mathbf{R}^n\\rightarrow\\mathbf{R}^n\\)使得\\(Y=g(X)\\). 下面，设\\((\\Omega,\\mathcal{F},P)\\)是一个给定的完备概率空间，一个随机变量\\(X\\)是一个\\(\\mathcal{F}\\)可测函数\\(X:\\Omega\\rightarrow\\mathbf{R}^n\\).每个随机变量诱导了\\(\\mathbf{R}^n\\)上的概率测度\\(\\mu_X\\),定义为 \\[ \\mu_X(B)=P(X^{-1}(B)), \\] \\(\\mu_X\\)称为\\(X\\)的分布（！！！）。（\\(B\\)是随机变量映射的像，\\(X:\\Omega\\rightarrow\\mathcal{B}\\)） 如果\\(\\int_{\\Omega}|X(\\omega)|dP(\\omega)&lt;\\infty\\),那么 \\[ E[X]:=\\int_{\\Omega}X(\\omega)dP(\\omega)=\\int_{\\mathbf{R}^n}xd\\mu_X(x) \\] 称为\\(X\\)的期望。更一般地，如果\\(f:\\mathbf{R^n}\\rightarrow\\mathbf{R}\\)是Borel可测的，且\\(\\int_{\\Omega}|f(X(\\omega))|dP(\\omega)&lt;\\infty\\),那么 \\[ E[f(X)]:=\\int_{\\Omega}f(X(\\omega))dP(\\omega)=\\int_{\\mathbf{R}^n}f(x)d\\mu_X(x). \\] 5.2.1.1 \\(L^p\\)空间 如果\\(X:\\Omega \\rightarrow\\mathbf{R}^n\\)是一个随机变量，\\(p\\in[1,\\infty)\\)是一个常数，定义\\(X\\)上的\\(L^p\\)范数\\(||X||_p:\\) \\[ ||X||_p=||X||_{L^p(P)}=(\\int_\\Omega|X(\\omega)|^pdP(\\omega))^{\\frac{1}{p}}. \\] 如果\\(p=\\infty\\),定义 \\[ ||X||_{\\infty}=||X||_{L^{\\infty}(P)}=\\text{inf}\\{N\\in\\mathbf{R};|X(\\omega)|\\leq N\\, a.s.\\}, \\] 相应的\\(L^p\\)空间定义为 \\[ L^p(P)=L^p(\\Omega)=\\{X:\\Omega\\rightarrow\\mathbf{R}^n;||X||_p&lt;\\infty\\}, \\] 在该范数定义下，\\(L^p\\)空间是Banach空间即完备的赋范空间。如果\\(p=2\\),空间\\(L^2(P)\\)是一个Hilbert空间，即完备的内积空间，其中内积 \\[ (X,Y)_{L^2(P)}:=E[X\\cdot Y],\\quad X,Y\\in L^2(P). \\] 定义2.1.3 两个子集\\(A,B\\in\\mathcal{F}\\)称为独立的，如果 \\[ P(A\\cap B)=P(A)\\cdot P(B). \\] 集族\\(\\mathcal{A}=\\{\\mathcal{H_i};i\\in I\\}\\),如果 \\[ P(H_{i_1}\\cap \\dots \\cap H_{i_k})=P(H_{i_1}) \\cdots P(H_{i_k}), \\] 对\\(\\forall H_{i_1}\\in \\mathcal{H_{i_1}},...,H_{i_k}\\in\\mathcal{H_{i_k}}\\)成立，\\(i_1,i_2,...i_k\\)互不相同。 如果由随机变量族\\(X_i,i\\in I\\)生成的\\(\\sigma\\)代数\\(\\mathcal{H}_{X_i}\\)构成的集族是独立的，那么随机变量族也是独立的。 度过两个随机变量\\(X,Y:\\omega\\rightarrow\\mathbf{R}\\)是独立的，假设\\(E[|X|]&lt;\\infty,E[|Y|]&lt;\\infty\\)，则\\(E[XY]=E[X]E[Y]\\). 定义2.1.4 随机过程是带参数的一族随机变量：\\(\\{X_t\\}_{t\\in T}\\)定义于概率空间\\((\\Omega,\\mathcal{H},P)\\)上，取值于\\(\\mathbf{R}^n\\)中。 参数空间\\(T\\)通常是射线\\([0,\\infty).\\)注意对每个固定的\\(t\\in T\\),有随机变量 \\[ \\omega\\rightarrow X_t(\\omega);\\quad \\omega\\in \\Omega. \\] 另一方面，固定\\(\\omega\\in \\Omega\\),可以考虑函数 \\[ t\\rightarrow X_t(\\omega);\\quad t\\in T, \\] 称之为\\(X_t\\)的路径。 一般地，可以直观地把\\(t\\)当作时间，而每个\\(\\omega\\)可认为单个的“质子”或者“实验”。\\(X_t(\\omega)\\)表示在时刻\\(t\\)时质子（实验）\\(\\omega\\)的位置（或结果）。有时可以用\\(X(t,\\omega)\\)代替\\(X_t(\\omega)\\)，因此可以把随机过程看作一个从\\(T\\times\\Omega\\)到\\(\\mathbf{R}^n\\)的函数，随机过程关于\\((t,\\omega)\\)二元可测的。 最后注意到，可以认为，对每个\\(\\omega\\)，函数\\(t\\rightarrow X_t(\\omega)\\)是从\\(T\\)到\\(\\mathbf{R}^n\\)的函数，因此可认为\\(\\Omega\\)是空间\\(\\widetilde{\\Omega}=(\\mathbf{R}^n)^T\\)(即从\\(T\\)到\\(\\mathbf{R}^n\\)的所有的函数全体集合)的子集。此时，\\(\\sigma\\)代数\\(\\mathcal{F}\\)将包含下述形式集合生成的\\(\\sigma\\)代数\\(\\mathcal{B}\\): \\[ \\{\\omega;\\omega(t_1)\\in F_1,...,\\omega(t_k)\\in F_k\\},\\quad F_i\\subset \\mathbf{R}^n为Borel集 \\] 因此，我们可以将随机过程视为可测空间\\(((\\mathbf{R}^n)^T,\\mathcal{B})\\)上的一个概率测度\\(P\\). 过程\\(X=\\{X_t\\}_{t\\in T}\\)的有限维分布是定义在\\(\\mathbf{R}^{nk},k=1,2,...\\)上的测度\\(\\mu_{t_1,...,t_k}\\),其中 \\[ \\mu_{t_1,...,t_k}(F_1\\times F_2\\times \\dots F_k)=P[X_{t_1}\\in F_1,...,X_{t_k}\\in F_k];t_i\\in T, \\] \\(F_1,...,F_k\\)定义为\\(\\mathbf{R}^n\\)中的Borel集。 反之，给定\\(\\mathbf{R}^{nk},k=1,2,...\\)上的概率测度\\(\\nu_{t_1,...,t_k}\\)以后，能否构造一个随机过程\\(\\{Y_t\\}_{t\\in T}\\)使得\\(\\nu_{t_1,...,t_k}\\)作为它的有限维分布？ 定理2.1.5（Kolmogorov存在定理） 对任意的\\(t_1,...,t_k\\in T,k\\in\\mathbf{N},\\)设\\(\\nu_{t_1,...,t_k}\\)为\\(\\mathbf{R}^{nk}\\)上的概率测度，满足 (K1) \\(\\nu_{t_{\\sigma(1)},...,t_{\\sigma(k)}}(F_1\\times...\\times F_k)=\\nu_{t_1,...,t_k}(F_{\\sigma^{-1}(1)}\\times...\\times F_{\\sigma^{-1}(k)})\\)其中\\(\\sigma\\)为\\(\\{1,2,...,k\\}\\)的任意一个排列。 (K2) \\(\\nu_{t_1,...,t_k}(F_1\\times ...\\times F_k)=\\nu_{t_{\\sigma(1)},...,t_{\\sigma(k)},t_{\\sigma_{k+1}},...,t_{\\sigma_{k+m}}}(F_1\\times...\\times F_k\\times \\mathbf{R}^n\\times...\\mathbf{R}^n)\\),对任意\\(m\\in\\mathbf{N},\\)此处右边总共有\\(k+m\\)个因素。 则存在一个概率空间\\((\\Omega,\\mathcal{F},P)\\)和\\(\\Omega\\)上的随机过程\\(\\{X_t\\},X_t:\\Omega\\rightarrow \\mathbf{R}^n\\).对任意的\\(t_i\\in T,k\\in\\mathbf{N}\\)及任意的Borel集\\(F_i\\)满足 \\[ \\mu_{t_1,...,t_k}(F_1\\times F_2\\times \\dots F_k)=P[X_{t_1}\\in F_1,...,X_{t_k}\\in F_k]. \\] 5.2.2 布朗运动（Brownian Motion） 为了构造\\(\\{B_t\\}_{t\\geq0}\\),由Kolmogorov存在定理，只需要指定一族概率测度\\(\\{\\nu_{t_1,...,t_k}\\}\\)满足条件(K1),(K2)且这些测度与观察到的花粉表现一致：固定\\(x\\in\\mathbf{R}^n\\),定义 \\[ p(t,x,y)=(2\\pi t)^{-\\frac{n}{2}}\\cdot \\exp \\Big ( -\\frac{|x-y|^2}{2t}\\Big),\\quad y\\in\\mathbf{R}^n,t&gt;0. \\] 如果\\(0\\leq t_1\\leq t_2\\leq ... \\leq t_k\\)，在\\(\\mathbf{R}^{nk}\\)上定义一个测度\\(\\nu_{t_1,...,t_k}\\)使得 \\[ \\begin{aligned} &amp;\\nu_{t_1,...,t_k}(F_1\\times ...\\times F_k)\\\\ =&amp;\\int_{F_1\\times...\\times F_k}p(t_1,x,x_1)p(t_2-t_1,x_2,x_2)\\cdots p(t_k-t_{k-1},x_{k-1},x_k)dx_1\\cdots dx_k, \\end{aligned} \\] 此处\\(dy=dy_1\\cdots dy_k\\)为Lebesgue测度，\\(p(0,x,y)dy=\\delta_x(y)\\)是在\\(x\\)处的单位质点。 ​ 利用(K1)，把它延拓到所有\\(t_i\\)的有限序列。由于对\\(\\forall t\\geq0,\\int_{\\mathbf{R}^n}p(t,x,y)dy=1,\\)故(K2)满足，由Kolmogorow定理，存在一个概率空间\\((\\Omega,\\mathcal{F},P^x)\\)和一个\\(\\Omega\\)上的随机过程\\(\\{B_t\\}_{t\\geq0},\\)使得\\(B_t\\)的有限维分布为上式，即 \\[ \\begin{aligned} &amp;P^x(B_{t_1}\\in F_1,...,B_{t_k}\\in F_k)\\\\ =&amp;\\int_{F_1\\times...\\times F_k}p(t_1,x,x_1)p(t_2-t_1,x_2,x_2)\\cdots p(t_k-t_{k-1},x_{k-1},x_k)dx_1\\cdots dx_k. \\end{aligned} \\] ​ 定义2.2.1 ​ 上述过程称为初值为\\(x\\)的布朗运动的修正，注意\\(P^x(B_0=x)=1\\). ​ 下面叙述布朗运动的基本性质： \\(B_t\\)是一个Gauss过程，即对所有的\\(0\\leq t_1\\leq...t_k,\\)随机变量\\(Z=(B_{t_1},...,B_{t_k})\\in \\mathbf{R}^{nk}\\)是服从多重正态分布的，即存在一个向量\\(M\\in \\mathbf{R}^{nk}\\)和一个半正定矩阵\\(C=[c_{jm}]\\in\\mathbf{B}^{nk\\times nk}\\)使得 \\[ E^x\\Big[\\exp\\Big(i\\sum_{j=1}^{nk}u_jZ_j\\Big)\\Big]=\\exp\\Big(-\\frac{1}{2}\\sum_{j,m}u_jc_{jm}u_m+i\\sum_{j}u_jM_j\\Big) \\] 对所有的\\(u=(u_1,...,u_{nk})\\in \\mathbf{R}^{nk}\\)成立，这里的\\(i\\)是虚数单位，\\(E^x\\)表示关于概率\\(P^x\\)​所取的数学期望。 \\[ M=E^x[Z] \\] 是\\(Z\\)的均值， \\[ c_{jm}=E^x[(Z_j-M_j)(Z_m-M_m)] \\] 是\\(Z\\)的协方差矩阵。 ​ 经过计算可知， \\[ M=E^x[Z]=(x,x,...,x)\\in\\mathbf{R}^{nk}, \\] \\[ \\begin{aligned} C= \\begin{pmatrix} t_1I_n &amp; t_1I_n &amp;...&amp;t_1I_n\\\\ t_1I_n &amp; t_2I_n &amp;...&amp;t_2I_n\\\\ \\vdots &amp; \\vdots&amp; &amp;\\vdots\\\\ t_1I_n&amp;t_2I_n&amp;...&amp;t_kI_n \\end{pmatrix}， \\end{aligned} \\] 因此 \\[ E^x[B_t]=x, \\quad \\text{for all}\\,\\,t\\geq0 \\] 及 \\[ E^x[(B_t-x)^2]=nt,\\quad E^x[(B_t-x)(B_s-x)]=n\\,\\min(s,t), \\] 而且，如果\\(t\\geq s,\\)则有 \\[ E^x[(B_t-B_s)^2]=n(t-s). \\] \\(B_t\\)具有独立增量，即对任意的\\(0\\leq t_1&lt;t_2&lt;...&lt;t_k,\\) \\[ B_{t_1},B_{t_2}-B_{t_1},...,B_{t_k}-B_{t_{k-1}}是独立的. \\] "],["english.html", "6 英语 6.1 week2 6.2 week3", " 6 英语 6.1 week2 A. How to start a conversation Opening lines(开场白) = icebreakers positive (not complaints) ex: compliments news events weather Be sincere, respectful, interested B. How to keep a conversation going Asking questions = Elaboration technique don’t ask questions requiring just a yes or no answer ask questions showing your genuine interest ask questions based on the last thing a person says Seven tips be aware of body &amp; facial language don’t gossip cultivate a wide range of topics have a sense of humor don’t interrupt be enthusiastic and upbeat (ethnic vs ethic vs enthusiastic) be flexible in your point of view Topics to avoid Politics – Political opinions can be polarizing, especially in casual settings. It’s better to steer clear of deep political discussions unless you’re in a setting where it’s appropriate or both parties are open to it. Income or Wealth – Asking about someone’s salary, financial situation, or how much they earn can be seen as intrusive or inappropriate, especially if you don’t have a close relationship. Gender and Sexuality – Discussions about gender identity or sexual orientation can be sensitive. It’s important to be respectful and avoid assumptions or comments that might be seen as intrusive or inappropriate. Academic Performance – Talking about grades or student performance can be awkward and potentially make people feel uncomfortable, especially if someone is struggling in their studies or feels insecure about their achievements. Family Matters – Questions about someone’s marital status, children, or family dynamics can be too personal. Not everyone wants to discuss their family situation, especially in casual settings. Private Life or Relationships – Questions about someone’s romantic life, ex-partners, or relationship issues are usually best avoided, as they can make someone feel awkward or exposed. C. How to end a conversation break eye contact use transition words recap (sum up) what was said give handshake D. Why People make small talk There are a few different reasons why people use small talk. The first and most obvious, is to break an uncomfortable silence. Another reason, however, is simply to fill time. Some people make small talk in order to be polite. E. What I learn today I have nothing to add. 我没什么要补充的了 I want to think about it again. 我还需要再思考一下 privacy disclosure 隐私泄露 6.2 week3 A man is clapping his hands, another man is holding the railing, and they are both smiling. :question:Are there any body movements that have different meanings in different languages and cultures? The Gesture of Nodding Western Cultures: A nod typically means “yes” or agreement. Bulgaria &amp; Greece: In contrast, in some parts of Bulgaria and Greece, a nod can actually mean “no.” It’s a reverse of what most people are used to in Western cultures. The “V” Sign (Victory Sign) Western Cultures: The “V” sign made with the palm facing out is a symbol for victory or peace, often used in casual contexts. United Kingdom, Australia, New Zealand: If the palm is facing inward (with the back of the hand towards the person), it’s considered an offensive gesture, similar to giving someone the finger. The “OK” Hand Gesture Western Cultures: Making a circle with the thumb and forefinger and extending the other fingers generally means “okay” or “all good.” Brazil, Turkey, Greece: In some countries, this gesture can be highly offensive, symbolizing an insult or a rude suggestion. fist bump, White supremacists, Pat the child’s head, Micro-expressions, kinesiology, pitch, police siren, Vanity, intensity, fraternity :o:The gesture of crossing your fingers typically has different meanings depending on the context and culture. Here are the main interpretations: Good Luck General Western Meaning: The most common interpretation is that crossing your fingers is a gesture used to wish for good luck or hope for a positive outcome. For example, someone might cross their fingers before an exam or a big event, as a way of expressing hope that things will go well. Fingers Crossed for a Lie (Contradictory Meaning) Deceptive Meaning: In some cultures, particularly in the U.S. and some parts of Europe, people might cross their fingers behind their back when telling a lie or making a promise they don’t intend to keep. The crossed fingers are meant to symbolize that the person is not actually being truthful or is making a “fake” promise. It’s like a silent “get-out-of-jail-free card.” "],["cdc.html", "7 Optimal strategies for collective defined contribution plans when the stock and labor markets are co-integrated（股票和劳动力市场协同整合时集体确定缴款计划的最优策略） 7.1 Introduction 7.2 模型的公式化 7.3 养老金基金计划的最优策略 7.4 数值分析 7.5 总结", " 7 Optimal strategies for collective defined contribution plans when the stock and labor markets are co-integrated（股票和劳动力市场协同整合时集体确定缴款计划的最优策略） 7.1 Introduction 7.1.1 相关工作 在集体确定缴费（Collective Defined Contribution）养老金计划中，基金的资产集中由金融机构管理，福利取决于基金的财务状况。Gollier 2008 认为后代有义务参与养老金计划，集体养老金计划通过强制参与能够实现代际之间的风险转移，相应的风险由当前和未来的几代人共同承担从而改善福利。由于CDC养老金计划在精算研究中的成功，这种类型的养老金设计引起研究人员的极大关注。Bovenberg et al. 2007 讨论了集体养老金计划与传统计划相比的成本和收益。Cui et al. 2011 展示了集体计划和没有风险分担的最佳个人计划之间的福利比较。Wang et al. 2018 研究目标福利计划的最佳投资策略和调整后的福利支付策略，以最小化福利风险和代际转移组合。He et al. 2020 展示了DB-PAYGO养老金制度中不稳定缴费和不连续风险的成本，Wang et al. 2021 考虑具有违约风险和模型不确定性的目标福利计划（TBP）的最优投资和福利支付问题。文献假设劳动收入过程遵循几何布朗运动，然而劳动收入可能与金融市场有关，应该考虑收入与股票市场之间的关系。Mayer et al. 1974 首先实证研究了总劳动收入与资产价格之间的联系，Baxter et al. 1997 指出劳动收入与股息是协整的，他们还提出在没有协整的情况下，劳动收入和GDP的资本在长期内（的比例）可能会变为0或1。此外，Benzoni et al. 2007 在投资组合选择问题中引入了劳动力和股票市场之间的协整，他们通过分析数据表明劳动收入和股息流之间的高度相关性是令人信服的。Geanakoplos et al. 2011 假设平均劳动收入和股票市场存在长期正相关性，并提出了研究这种相关性的重要性。 本文研究了一个CDC养老金计划中的随机最优问题，在此模型中假定缴费率是固定的，福利支付取决于最终的薪资水平。本文提出的主要假设是 养老金基金可以投资于一种风险资产和一种无风险资产组成的金融市场。 将劳动收入过程描述为几何随机游走，其中的漂移项依赖于当前股息与当前劳动收入的比率，其中股息过程遵循几何布朗运动。劳动收入被假定为总劳动收入与成员特质性冲击（shocks）的乘积，其恒定增长率是未知的，我们通过一个连续时间的二状态隐马尔科夫链对特质性冲击进行建模。 养老金成员分为在职和退休。在职成员依然在职并向养老金基金缴费而退休成员从养老金基金中获得福利。每个参与者在年龄\\(a_0\\)时加入养老金计划并在年龄\\(a_1\\)​时退休。 在处理具有常数相对风险厌恶(CRRA)效用函数的最优投资问题时，传统的猜解不易用于求解相关的HJB方程，因此我们在模型不确定性的框架下进行研究选用的效用函数是绝对风险厌恶（CARA）的指数效用，假设基金经理的目标是寻找最优的投资策略（投资到风险资产的比例\\((\\pi^*(t))\\)）和福利政策\\((b^*(t))\\)，以最大化社会福利和终端盈余财富。通过求解HJB方程得到了最优资产配置和福利支付政策的显式解。 7.1.2 主要区别 与现有的CDC基金方案最优设计相比： 本文考虑了劳动和股市之间的协整关系。在现实中，劳动收入与金融市场密切相关。劳动收入的随机性和金融市场的回报是基金经理面临的不确定性来源。因此，本文通过假设劳动收入和股息的对数差值遵循均值回归过程，建立了两者之间的真实关系。劳动收入和风险资产的价格趋势具有协整性，意味着它们在长期内具有相同的趋势。当协整关系较强时，退休收入的波动性大于基金组合回报的波动性。我们的模型通过考虑劳动市场和股市之间的关系，更加贴近现实的金融市场。 此外，本文还考虑了个体性劳动收入冲击的不确定性。因此，除了让劳动收入冲击的对数遵循算术布朗运动，我们的模型进一步提出劳动冲击的增长率可能随时间随机变化。我们通过一个连续时间二态隐马尔可夫链来建模这种不确定性，这增加了解HJB方程的数学难度。 本文其余部分的组织结构如下：第2节介绍了资产和劳动收入的公式化以及相关假设；第3节推导了最优投资策略和福利替代率的显式表达式，并展示了一个应用实例；第4节通过数值实例来说明我们的结果；第5节提供了结论性意见。 7.2 模型的公式化 设\\(T&gt;0\\)为有限时间区间，定义三个标准布朗运动\\(Z_D=\\{Z_D(t),t\\geq0\\},Z_1=\\{ Z_1(t),t\\geq0 \\},Z_L=\\{Z_L(t),t\\geq0 \\}\\).它们定义在概率空间\\((\\Omega,\\mathcal{F},\\mathbb{P})\\)上，令\\(\\mathbb{F}=\\{\\mathcal{F}_t,t\\geq0\\}\\)是由布朗运动生成的扩展过滤(Augmented Filration)。假设三个标准布朗运动都是独立的。本文假设在一个过滤完备的概率空间中，对于\\(p\\geq1,\\)定义 \\[ \\begin{align} &amp;L^p_{\\mathcal{F}_t}(\\Omega;\\mathbb{R})=\\left\\{ X:\\Omega\\rightarrow\\mathbb{R}\\big\\vert X(t)是\\mathcal{F}_t可测的，\\mathbb{E}[|X(t)|^p]&lt;\\infty \\right\\},\\\\ &amp;L^2_\\mathbb{F}(s,t;\\mathbb{R})=\\left\\{ X:[s,t]\\times \\Omega\\rightarrow\\mathbb{R}\\big|X(t)是\\mathbb{F}适应的，\\mathbb{E}\\left[ \\int_s^t|X(\\nu)|^2d\\nu \\right]&lt;\\infty \\right\\},\\\\ &amp;L^p_\\mathbb{F}(\\Omega;L^2(s,t;\\mathbb{R}))=\\left\\{ X:[s,t]\\times\\Omega\\rightarrow \\mathbb{R}\\big|X(t)是\\mathbb{F}适应的，\\mathbb{E}\\left[\\left( \\int_s^t|X(\\nu)|^2d\\nu\\right)^p \\right]&lt;\\infty \\right\\},\\\\ &amp;L^p_{\\mathbb{F}}(\\Omega;C([s,t];\\mathbb{R}))=\\left\\{ X:[s,t]\\times\\Omega\\rightarrow\\mathbb{R}\\big|X(t)是有界的\\mathbb{F}适应且有连续路径，\\mathbb{E}\\left[ \\sup_{\\nu\\in[s,t]}|X(\\nu)|^p \\right]&lt;\\infty \\right\\}. \\end{align} \\] 7.2.1 金融市场 在本文的模型中我们假设基金经理可以在时间区间\\([0,T]\\)内对有一种无风险资产和一种风险资产组成的金融市场进行投资。风险资产向投资者支付持续的股息流，令\\(D(t)\\)表示风险资产在时间\\(t\\)的股息过程。股息的动态过程表示为： \\[ \\begin{cases} \\frac{dD(t)}{D(t)}=g_Ddt+\\sigma dZ_D(t),\\quad t\\in[0,T]\\\\ D(0)=d_0&gt;0 \\end{cases} \\] 其中\\(g_D,\\sigma\\)分别是股息的增长率和波动率。令定价核\\(M(t)\\)的动态表示为： \\[ \\frac{dM(t)}{M(t)}=-rdt-\\lambda_m dZ_D(t) \\] 其中\\(r&gt;0\\)表示常数的无风险利率，\\(\\lambda_m\\)表示常数的风险价格。 [!Note] 定价核(pricing kernel, AKA Stochastic Discount Factor, SDF)是一个用来将未来现金流折现到当前时刻的工具，反映了时间和不确定性对未来现金流的影响，通常用于描述金融资产价格的动态和资产的风险溢价。 令\\(X(t)\\)为风险资产的价格过程。资产价格可以通过以下方式描述，即股息的贴现总和： \\[ X(t)=\\int^\\infty_t\\mathbb{E}_t\\left[ \\frac{M(s)}{M(t)}D(s) \\right]ds. \\] 因此，我们可以推导出\\(D(t)\\)和\\(X(t)\\)成正比，即 \\[ X(t)=\\frac{D(t)}{r+\\lambda_m\\sigma-g_D}, \\] 推导如下图 于是风险资产的动态过程服从下面的几何布朗运动： \\[ \\begin{cases} \\frac{dX(t)}{X(t)}=g_Ddt+\\sigma dZ_D(t),\\quad t\\in[0,T],\\\\ X(0)=x_0&gt;0. \\end{cases} \\] 令\\(S(t)\\)为超额盈余过程，随着时间\\(t\\)变化可以描述为 \\[ \\begin{cases} \\frac{dS(t)}{S(t)}=\\frac{dX(t)+D(t)dt}{X(t)}=\\mu dt+\\sigma dZ_D(t),\\quad t\\in[0,T],\\\\ S(0)=s_0&gt;0. \\end{cases}\\tag{2.1}\\label{2.1} \\] 其中期望回报由定价核的定义得到\\(\\mu=r+\\lambda_m\\sigma.\\) [!Tip] 超额盈余过程\\(S(t)\\)描述了单位风险资产的回报率，是由风险资产的变化量和单位时间内股息流的值在资产价格\\(X(t)\\)的基础上计算超额盈余。 无风险资产\\(S_0(t)\\)的动态过程如下 \\[ \\begin{cases} \\frac{dS_0(t)}{S_0(t)}=rdt,\\\\ S_0(0)=s_{00}&gt;0. \\end{cases}\\label{2.2}\\tag{2.2} \\] 7.2.2 劳动收入 参考Benzoni et al. 2007 的工作，假设劳动收入\\(L(t)\\)是总劳动收入\\(L_1(t)\\)和成员的个体性冲击\\(L_2(t)\\)的乘积，在对数化模型中有 \\[ l(t)=l_1(t)+l_2(t),\\label{2.3}\\tag{2.3} \\] 文章通过让总劳动收入与股市之间的对数差值服从均值回归过程，来模拟总劳动收入和股市的协整性。令差值\\(y(t)\\)满足 \\[ y(t)=\\log L_1(t)-\\log D(t)-\\lambda,\\label{2.4}\\tag{2.4} \\] 其中正的常数\\(\\lambda\\)是总劳动收入与股息的长期对数比率。差值\\(y(t)\\)的动态过程由下面的方程描述 \\[ \\begin{cases} dy(t)=-ky(t)dt+v_LdZ_L(t)-v_DdZ_D(t),\\quad t\\in[0,T],\\\\ y(0)=y_0. \\end{cases} \\] 其中\\(k\\)决定了变量\\(y(t)\\)向长期均值回归的速度并捕捉了总劳动收入与股息之间的协整性（即当\\(k=0\\)时二者不存在协整性）。\\(v_L,v_D\\)分别是条件波动率，\\(Z_L(t)\\)是与总劳动收入不确定性相关的SBM. 特质性冲击的对数过程由下面的式子描述 \\[ \\begin{cases} dl_2(t)=\\left( \\alpha(t)-\\frac{v_1^2}{2} \\right)dt+v_1dZ_1(t),\\quad t\\in[0,T],\\\\ l_2(0)=l_{20}. \\end{cases} \\] 其中\\(\\alpha(t)\\)是增长率,\\(v_1\\)是相应的波动率，\\(Z_1(t)\\)是与\\(Z_L(t),Z_D(t)\\)相互独立的SBM. 7.2.3 养老金系统 本文考虑的养老金保险计划中，成员分为两组，在职成员是指向养老基金缴纳费用的工作成员，而退休成员从养老基金中获取福利。所有成员假设从\\(a_0\\)开始加入计划，直到退休年龄\\(a_1\\)为止，且死亡年龄为\\(a_2.\\)我们还假设生存函数\\(s(x),\\)且\\(s(a_0)=1\\)​. 用\\(n(t)\\)表示在时间\\(t\\)时年龄为\\(a_0\\)的新成员加入养老金计划的密度。\\(n(t)\\)是一个非负函数表示新成员加入养老金计划的密度不可能为负。然后在时间\\(t\\)时年龄为\\(x\\)的人数为 \\[ n(t-(x-a_0))s(x),\\quad t&gt;0. \\] 其中\\(t-(x-a_0)\\)可能为负，这意味着年龄为\\(x\\)的个体是在\\(x-a_0\\)年前加入计划的。当年龄为\\(x\\)的个体尚未加入养老金计划时，\\(n(t-(x-a_0))=0.\\) 劳动力市场中在职成员和退休成员的总人数分别用\\(M_1(t),M_2(t)\\)表示： \\[ M_1(t)=\\int_{a_0}^{a_1}n(t-(x-a_0))s(x)dx,\\\\ M_2(t)=\\int_{a_1}^{a_2}n(t-(x-a_0))s(x)dx. \\] 在职成员的数量决定了总的缴费率，假设\\(C_0\\)时时间\\(0\\)时的即时缴费率，\\(\\eta_1\\)是缴费的指数增长率，因此养老基金在时间\\(t\\)时的总缴费率为: \\[ C(t)=\\int_{a_0}^{a_1}n(t-x+a_0)s(x)C_0e^{\\eta_1t}dx. \\] 退休成员的数量决定了养老基金的总福利、薪资结构以及初始年度养老金的支付率。初始养老金支付率被假定为退休时最终薪资的一定比例。对于在时间\\(t\\)退休的成员，初始养老金的支付额为\\(b(t)L(t),\\)其中\\(b(t)\\)是替代率可以视为控制变量。对于在时间\\(t\\),年龄为\\(x\\)的成员(即已经退休\\(x-a_1\\)年)，最终薪资是\\(x-a_1\\)年前的薪水。与Wang et al. 2018 中的工作类似，为了确定\\(x\\)岁的退休成员在时间\\(t\\)的年度养老金支付率，引入一个新的量\\(FL(x,t)\\)表示该成员在退休\\(x-a_1\\)年后的假定薪资，这个量定义为 \\[ FL(x,t)=L(t)e^{-\\eta_0(x-a_1)},\\; t\\geq0,\\; x\\geq a_1. \\] 其中\\(L(t)\\)表示在时间\\(t\\)退休成员的薪资，假定薪资通过指数增长率\\(\\eta_0\\)进行确定性地向后推算。这个方法与成员退休时的实际薪资不同，尤其是在\\(\\eta_0&gt;0,x&gt;a_1\\)时，实际薪资与假定薪资之间的差异随着年龄增大而增大。设对假定的薪资应用一个调整因子，因此时间\\(t\\)时年龄为\\(x\\)的成员的养老金支付为： \\[ B(x,t)=b(t)L(t)e^{-\\eta_0(x-a_1)}. \\] 所有退休成员在时间\\(t\\)时的实际总退休福利（涵盖年龄\\(x\\)从\\(a_1\\sim a_2\\)的退休成员）可以通过下式得到 \\[ B(t)=\\int_{a_1}^{a_2}n(t-x+a_0)s(x)B(x,t)dx\\overset{d}{=}F(t)b(t)L(t), \\] 其中\\(F(t)\\)是一个正的函数，定义为 \\[ F(t)=\\int_{a_1}^{a_2}n(t-x+a_0)s(x)e^{-\\eta_0(x-a_1)}dx. \\] 为了增加养老金的福利，基金经理将在缴费和福利之间的盈余部分进行动态投资。在本文的模型中，金融市场由一个风险资产和一个无风险资产组成，设\\(\\pi(t)\\)是时间\\(t\\)投资于风险资产的比例。则养老金基金的财富过程为 \\[ \\begin{cases} dW(t)=\\pi(t)W(t)\\frac{dX(t)+D(t)dt}{X(t)}+(1-\\pi(t))W(t)\\frac{dS_0(t)}{S_0(t)}+C(t)dt-B(t)dt,\\\\ w(0)=w_0&gt;0.\\tag{2.5}\\label{2.5} \\end{cases} \\] [!Note] \\(W(t)\\)是养老金基金的财富过程 \\(\\pi(t)\\)是投资于风险资产的比例 \\(X(t)\\)是风险资产的价格过程 \\(D(t)\\)是风险资产的股息支付 \\(S_0(t)\\)是无风险资产的价格过程 \\(C(t)\\)是缴费项 \\(B(t)\\)是福利支付项 接下来我们定义可接受的策略和本文研究的主要问题。 Definition 2.1 对于任何固定\\(t\\in[0,T],\\)策略对\\((\\pi(t),b(t))\\)被称为可接受的，如果它满足以下条件： 投资策略和初始福利支付政策\\((\\pi(t),b(t))\\)是\\(\\mathcal{F}_t\\)适应的，以使得SED\\((\\ref{3.4})\\)存在唯一解\\(W_{\\pi,b}(t).\\) \\(\\pi(t)\\in L_{\\mathbb{F}}^2(0,T;\\mathbb{R}^+)\\)且\\(b(t)\\in L_{\\mathbb{F}}^2(0,T;\\mathbb{R}^+)\\)对所有\\(t&gt;0\\)成立。 Problem 2.1 对于初始状态\\((t,W_t),\\)养老金基金经理的目标函数是最大化： \\[ J(t,w,l,y)=\\mathbb{E}_{\\pi,b}\\left[ \\int_t^Te^{-rs}U(b(s)F(s)L(s))ds+\\lambda_1e^{-rT}U(W(T)) \\right],\\tag{2.6}\\label{2.6} \\] 其中\\(\\lambda_1\\)是一个非负常数表示对终期财富带来的效用的权重。\\(\\mathbb{E}_{\\pi,b}\\)是在概率测度\\(\\mathbb{P}\\)下给定\\(W(t)=w,L(t)=l,y(t)=y\\)时的条件期望。于是这个问题的价值函数就是 \\[ V(t,w,l,y)=\\sup_{(\\pi,b)\\in\\mathcal{A}}J(t,w,l,y), \\] 其中\\(\\mathcal{A}\\)是一组控制对的集合。 7.3 养老金基金计划的最优策略 本节通过标准的动态规划方法研究这个随机最优控制问题。当劳动市场和股票市场是协整关系时得出了最优控制的显式表达式。本文考虑两种劳动收入过程的情况，具有未知增长率的特质性冲击以及不考虑特质性冲击。 7.3.1 具有特质性冲击的最优策略 这一小节中假设员工面临关于劳动收入的不确定性，通过考虑具有未知增长率的特质性冲击来捕捉这种不确定性。本文使用一个连续时间的二状态的隐马尔科夫链来描述劳动收入冲击的内在不确定性和动态特性。具体来说，\\(\\alpha(t)\\)是未知的增长率，使用连续时间的二状态隐马尔可夫链在\\((\\Omega,\\mathcal{F},\\mathbb{P})\\)上建模，并在\\(\\alpha_1,\\alpha_2\\)之间变化，其中\\(\\alpha_1&gt;\\alpha_2.\\)增长率\\(\\alpha(t)\\)可能取高值\\(\\alpha_1\\)或低值\\(\\alpha_2.\\)在一个小时间间隔\\(\\Delta t\\)内，增长率\\(\\alpha(t)\\)在时刻\\(t\\)取\\(\\alpha_1\\)的概率是\\(1-p_1\\Delta t\\),保持\\(\\alpha_2\\)的概率为\\(1-p_2\\Delta t\\)，其中\\(p_1,p_2\\)是二状态隐马尔科夫链的转移强度，从\\(\\alpha_1\\)转移到\\(\\alpha_2\\)的概率为\\(p_1\\Delta t,\\)从\\(\\alpha_2\\)转移到\\(\\alpha_1\\)的概率为\\(p_2\\Delta t.\\)对于任何的\\(t\\)设\\(P(t)\\)为条件概率，表示给定观测信息\\(\\mathcal{F}_t,\\)增长率\\(\\alpha(t)\\)取\\(\\alpha_1\\)的概率， \\[ P(t)=\\mathbb{P}(\\alpha(t)=\\alpha_1|\\mathcal{F}_t). \\] 对数化特质性冲击的预期增长率\\(\\mu_0(t)\\)是两种可能增长率的加权平均值，给定为 \\[ \\mu_0(t)=P(t)\\alpha_1+(1-P(t))\\alpha_2=\\alpha_2+\\beta P(t), \\label{31}\\tag{3.1} \\] 其中\\(\\beta=\\alpha_1-\\alpha_2.\\)对于一个小时间间隔\\(\\Delta t,\\)对数特质性劳动冲击的总变化是\\(l_2(t+\\Delta t)-l_2(t),\\)其预期变化为\\(\\mu_0(t)\\Delta t-\\frac{v_1^2}{2}\\).参考Wang et al. 2009 中的处理方法，对相应波动率进行归一化得到 \\[ dZ_1(t)=\\frac{1}{v_1\\Delta t}\\left(l_2(t+\\Delta t)-l_2(t)-\\mu_0(t)\\Delta t-\\frac{v_1^2}{2}\\right). \\tag{3.2}\\label{3.2} \\] 将式子\\((\\ref{3.2})\\)带入式子\\((\\ref{31})\\)就得到 \\[ dl_2(t)=(\\alpha_2+\\beta P(t)-\\frac{v_1^2}{2})dt+v_1dZ_1(t). \\] &gt; [!Warning] &gt; &gt; 感觉上式有点问题，预期变化应该为\\((\\mu_0(t)-\\frac{v_1^2}{2})\\Delta t\\) &gt; &gt; 归一化之后得到 &gt; \\[ &gt; \\frac{dZ_1(t)}{dt}=\\frac{1}{v_1\\Delta t}\\left(l_2(t+\\Delta t)-l_2(t)-(\\mu_0(t)-\\frac{v_1^2}{2})\\Delta t\\right). &gt; \\] &gt; 才能得到\\(l_2(t)\\)的动态过程。 应用 Statistics of Random Process 中的结果，我们还可以得到\\(P(t)\\)的动态过程 :question: \\[ dP(t)=(p_2-(p_1+p_2)P(t))dt+v_1^{-1}\\beta P(t)(1-P(t))dZ_1(t). \\] 由式\\((\\ref{2.3}),(\\ref{2.4})\\)可知对数劳动收入可以写作 \\[ l(t)=y(t)+d(t)+\\lambda+l_2(t), \\] 其中\\(d(t)=\\log D(t)\\)且其动态过程可以写作 \\[ \\begin{cases} d\\,d(t)=(g_D-\\frac{\\sigma^2}{2})dt+\\sigma dZ_D(t),\\quad t\\in[0,T],\\\\ d(0)=d_0. \\end{cases} \\] 应用Ito公式，对数化劳动收入就导出为 :question: \\[ d\\,l(t)=[-ky(t)+g_D-\\frac{\\sigma^2}{2}+\\lambda+\\beta P(t)+\\alpha_2-\\frac{v_1^2}{2}]dt\\\\ +(\\sigma-v_D)dZ_D(t)+v_LdZ_L(t)+v_1dZ_1(t). \\] 使用Ito公式就可得到 \\[ \\begin{cases} &amp;\\frac{dL(t)}{L(t)}&amp;=[-ky(t)+g_D-\\frac{\\sigma^2}{2}+\\lambda+\\beta P(t)+\\alpha_2+\\frac{v_L^2}{2}+\\frac{1}{2}(\\sigma-v_D)^2]dt\\\\ &amp; &amp;+(\\sigma-v_D)dZ_D(t)+v_LdZ_L(t)+v_1dZ_1(t),\\quad t\\in[0,T].\\\\ &amp;L(0)&amp;=l_0.\\label{3.3}\\tag{3.3} \\end{cases} \\] 结合式子\\((\\ref{2.1}),(\\ref{2.2})\\)就可以重写 \\[ \\begin{cases} \\frac{dW(t)}{W(t)}=[(\\mu-r)\\pi+r+\\frac{C(t)}{W(t)}-\\frac{F(t)L(t)b(t)}{W(t)}]dt+\\pi\\sigma dZ_D(t),\\\\ W(0)=w_0&gt;0,\\tag{3.4}\\label{3.4} \\end{cases} \\] 其中\\(L(t),P(t)\\)的动态过程也已经给出。 随后我们用\\(P\\)表示\\(P(t)\\),\\(L(t)\\)记作\\(l,W(t)\\)记作\\(w,\\)稍作符号上的简化，对于初始状态\\((t,W_t)\\),基金经理的目标是最大化以下的目标函数 \\[ J(t,w,l,y,P)=\\mathbb{E}_{\\pi,b}\\left[ \\int_t^Te^{-rs}U(b(s)F(s)L(s))ds+\\lambda_1e^{-rT}U(W(T))\\right],\\label{3.6}\\tag{3.6} \\] 该问题的价值函数由下面的公式给出 \\[ V(t,w,l,y,P)=\\sup_{(\\pi,b)\\in\\mathcal{A}}J(t,w,l,y,P),\\tag{3.7}\\label{3.7} \\] 其中\\(\\mathcal{A}\\)是控制对的集合。假设效用函数表达式如下 \\[ U(w)=-\\frac{1}{\\gamma}e^{-\\gamma w}, \\] 其中\\(\\gamma&gt;0\\)是常数的绝对风险厌恶系数。基金经理的目标是最大化福利和最终的剩余财富。为了简便起见，我们定义 \\[ \\begin{align} C^{1,2,2,2,2}&amp;([0,T]\\times\\mathbb{R}^+\\times\\mathbb{R}^+\\times\\mathbb{R}^+\\times\\mathbb{R}^+)\\\\ =&amp;\\{V(t,w,l,y,P)|V(t,\\cdot,\\cdot,\\cdot,\\cdot) \\text{ is once continuously differentiable on }[0,T],\\\\ &amp;V(\\cdot,w,l,y,P)\\text{ is twice continuously differentiable for }\\\\ &amp;w\\in \\mathbb{R}^+,l\\in\\mathbb{R}^+,y\\in\\mathbb{R}^+,P\\in\\mathbb{R}^+.\\} \\end{align} \\] 使用标准方法即可得到下面的HJB方程满足$V(t,w,l,y,P)C^{1,2,2,2,2} $ 下面是推导过程及结果 上面最后一个式子记为\\((3.8)\\),再加上边界条件 \\[ V(T,w,l,y,P)=-e^{-rT}\\frac{\\lambda_1}{\\gamma}e^{-\\gamma w}.\\tag{3.9}\\label{3.9} \\] 下面的定理可以阐述这个随机控制问题的结果 Theorem 3.1 对于任意的\\(t\\in[0,T],\\)最优投资策略和福利调整策略分别由下式给出 \\[ \\begin{align} &amp;\\pi^*(t,w,l,y,P)=\\frac{\\mu-r}{\\gamma f_1(t)\\sigma^2 w},\\tag{3.10}\\label{3.10}\\\\ &amp;b^*(t,w,l,y,P)=\\frac{\\ln \\lambda_1+\\ln f_1(t)-\\gamma f_1(t)w-\\gamma f_2(t)-\\gamma f_5(P)}{-\\gamma F(t)l},\\label{3.11}\\tag{3.11} \\end{align} \\] 且最终相应的价值函数就是 \\[ V(t,w,l,y,P)=-\\frac{\\lambda_1}{\\gamma}e^{-\\gamma[f_1(t)w+f_2(t)+f_5(P)]-rt}, \\] 其中 \\[ \\begin{align} f_1(t)&amp;=\\left[ e^{-\\int_t^Trds}+\\int_t^Te^{-\\int_t^srdu}ds \\right]^{-1},\\\\ f_2(t)&amp;=\\int_t^Te^{-\\int_t^sf_1(u)du}\\times\\left[ f_1(s)\\left( C(s)-\\frac{1-\\ln f_1(s)-\\ln\\lambda_1}{\\gamma} \\right)+\\frac{1}{2}\\frac{(\\mu-r)^2}{\\gamma\\sigma^2}+\\frac{r}{\\gamma} \\right]ds-f_5(P(T)). \\end{align} \\] \\(f_5(P)\\)满足下面的常微分方程，其中\\(0&lt;P&lt;1,\\) \\[ f_1(t)f_5(P)=f_5&#39;(P)[p_2-(p_1+p_2)P]-\\frac{1}{2v_1^2}[\\gamma f&#39;_5(P)-f&#39;&#39;_5(P)]P^2(1-P)^2\\beta^2, \\] 其边界条件是 \\[ \\begin{cases} f_1(t)f_5(0)=p_2f&#39;_5(0),\\\\ f_(t)f_5(1)=-p_1f&#39;_5(1). \\end{cases} \\] Proof. 由HJB方程\\((3.8)\\)可以导出两个控制的一阶条件为 \\[ \\begin{align} &amp;0=wV_w(\\mu-r)+w^2V_{ww}\\sigma^2\\pi+wlV_{wl}\\sigma(\\sigma-v_D)-wV_{wy}\\sigma v_D,\\\\ &amp;0=-V_{w}F(t)l+F(t)le^{-rt}e^{-\\gamma F(t)lb}. \\end{align} \\] 因此最优投资策略和福利调整策略由下式给出 \\[ \\begin{align} &amp;\\pi^*=\\frac{V_w\\sigma v_D-V_w(\\mu-r)-lV_{wl}\\sigma(\\sigma-v_D)}{wV_{ww}\\sigma^2},\\\\ &amp;b^*=\\frac{\\ln V_w+rt}{-\\gamma F(t)l}. \\end{align} \\tag{3.13}\\label{3.13} \\] 猜测解的形式为 \\[ V(t,w,l,y,P)=-\\frac{\\lambda_1}{\\gamma}\\exp\\{ -\\gamma[f_1(t)w+f_2(t)+f_3(t,l)+f_4(t)y+f_5(P)]-rt \\}, \\] 且有边界条件\\(f_1(T)=1,f_2(T)=-f_5(P(T)),f_3(T)=f_4(T)=0.\\) 于是我们有下面的一系列式子 \\[ \\begin{align} &amp;V_t=-\\gamma [f_{1t}w+f_{2t}+f_{3t}(t,l)+f_{4t}y+\\frac{r}{\\gamma}]V,\\\\ &amp;V_w=-\\gamma f_1V,\\quad V_{ww}=\\gamma^2f_1^2V,\\\\ &amp;V_l=-\\gamma f_{3l}V,\\quad V_{ll}=\\gamma^2f_{3l}^2V-\\gamma f_{3ll}V,\\\\ &amp;V_y=-\\gamma f_4V,\\quad V_{yy}=0,\\\\ &amp;V_P=-\\gamma f_{5P}V,\\quad V_{PP}=\\gamma^2 f_{5P}V-\\gamma f_{5PP}V,\\\\ &amp;V_{wl}=\\gamma^2 f_1f_{3l}V,\\quad V_{wy}=\\gamma^2f_1f_4V,\\\\ &amp;V_{ly}=\\gamma^2f_{3l}f_4V,\\quad V_{lP}=\\gamma^2f_{3l}f_{5P}V. \\end{align}\\label{3.14}\\tag{3.14} \\] 将\\((\\ref{3.13}),(\\ref{3.14})\\)代入HJB方程\\((3.8)\\)​​，得 假设\\(f_3(t,l)=f_3(t)\\ln l,\\)​比较变量的系数可得下面的ODEs 通过ODEs我们可以得到\\(V\\)的表达式从而最优投资策略和福利策略就可以得到。 遵循 Controlled Markov Processes and Viscosity Solutions, 2nd ed. 中的标准方法，假设\\(\\hat{V}\\in C^{1,2,2,2,2}\\)是(3.8)的解且满足边界条件\\((\\ref{3.9})\\),于是对于任意固定的\\(t\\in[0,T]\\),对于可行的控制对\\((\\pi^*(t),b^*(t))\\),都有 \\[ \\hat{V}(t,w,l,y,P)\\geq J(t,w,l,y,P). \\] 如果存在可行控制对\\((\\pi^*(t),b^*(t))\\)使得(3.8)式取等号，则 \\[ \\hat{V}(t,w,l,y,P)= J(t,w,l,y,P). \\] 如果我们令\\(V=\\hat{V},\\)则我们的假设就成立，相应的控制对是可行的，于是最优的投资策略和福利率就可以得到了。\\(\\square\\) [!Note] 值得注意的是，最优投资策略\\(\\pi^*(t)\\)与劳动收入\\(L(t)\\)无关,这意味着投资决策不受退休成员薪资模型的影响，表明投资策略在劳动收入变化的情况下仍能保持一致。福利调整策略\\(b^*(t)\\)​则依赖于劳动收入，影响退休福利的值，这是因为福利支付水平依赖于最终的薪资水平。 注意给定最优投资策略\\((\\ref{3.10})\\)依赖于时间\\(t\\)的财富过程\\(W.\\)风险资产的回报率通常高于无风险利率，即\\(\\mu&gt;r.(\\ref{3.10})\\)显示投资于风险资产的最优配置策略是正的，这意味着随着预期回报率的增加，基金经理会进行更大的投资。 7.3.2 无特质性冲击时的最优策略 本小节考虑一种特殊情形：劳动收入与股票在没有特质性冲击的情况下是协整的。通过让劳动与股市之间的对数劳动-股息比率遵循均值回归过程来建模劳动收入和股市之间的长期关系。令\\(\\overline{L}(t)\\)为劳动收入过程，则对数劳动与对数股息之间的差值为\\(\\overline{l}(t).\\) 令\\(\\overline{\\pi}(t)\\)为投资于风险资产的比例，\\(\\overline{b}(t)\\)为此情况下的福利策略，则财富过程可以变为 \\[ \\begin{cases} \\frac{d\\overline{W}(t)}{\\overline{W}(t)}=\\left[ (\\mu-r)\\overline{\\pi}+r+\\frac{C(t)}{\\overline{W}(t)}-\\frac{F(t)\\overline{L}(t)\\overline{b}(t)}{\\overline{W}(t)} \\right]dt+\\overline{\\pi}(t)\\sigma dZ_D(t),\\\\ \\overline{W}(0)=\\overline{w}_0&gt;0, \\end{cases}\\label{3.22}\\tag{3.22} \\] 其中 \\[ \\begin{align} \\frac{d\\overline{L}(t)}{dt}&amp;=\\left[ -k\\overline{y}(t)+g_D+\\overline{\\lambda}-\\frac{\\sigma^2}{2}+\\frac{(\\sigma-v_D)^2}{2} \\right]dt+(\\sigma-v_D)dZ_D(t),\\\\ \\frac{d\\overline{y}(t)}{\\overline{y}(t)}&amp;=-k\\overline{y}(t)dt-v_DdZ_D(t). \\end{align} \\] 有如下对SDE\\((\\ref{3.22})\\)对应的可行策略的定义。 Definition 3.4 对于任意的\\(t\\in[0,T],\\)策略对\\((\\overline{\\pi}(t),\\overline{b}(t))\\)被称为可行的，如果策略对是\\(\\mathcal{F}_t\\)适应的且满足如下条件： \\[ \\mathbb{E}\\left[ \\int^T_t[\\overline{b}^2(s)]ds \\right]&lt;\\infin,\\\\ \\mathbb{E}\\left[ \\int^T_t[\\overline{\\pi}^2(s)]ds \\right]&lt;\\infin, \\] 且SDE\\((\\ref{3.22})\\)有唯一解。 基金经理的目标是最大化以下的期望效用 \\[ \\bar{J}(t,\\bar{w},\\bar{l},\\bar{y})=\\mathbb{E}_{\\bar{\\pi},\\bar{b}}\\left[ \\int_t^Te^{-rs}U(\\bar{b}(s)F(s)\\bar{L}(s))ds+\\lambda_1e^{-rT}U(\\bar{W}(T)) \\right], \\] 其中\\(\\lambda_1\\)是一个非负常数，表示终端财富效用的权重。然后，问题的价值函数由以下公式给出 \\[ \\bar{V}(t,\\bar{w},\\bar{l},\\bar{y})=\\sup_{\\bar{\\pi},\\bar{b}}\\bar{J}(t,\\bar{w},\\bar{l},\\bar{y}), \\] 效用函数为 \\[ U(\\bar{w})=-\\frac{1}{\\gamma_1}e^{-\\gamma_1\\bar{w}}, \\] 其中\\(\\gamma_1&gt;0\\)​是基金经理的常数绝对风险厌恶系数。 可以列出相应的HJB方程 最优投资策略和福利调整策略由下面的定理导出。 Theorem 3.5 对于任意的\\(t\\in[0,T],\\)最优投资问策略和福利调整政策由下式给出 \\[ \\begin{align} \\bar{\\pi}^*(t,\\bar{w},\\bar{l},\\bar{y})&amp;=\\frac{\\mu-r}{\\gamma_1\\bar{g}_1(t)\\sigma^2\\bar{w}},\\\\ \\bar{b}^*(t,\\bar{w},\\bar{l},\\bar{y})&amp;=\\frac{\\ln \\lambda_1+\\ln\\bar{g}_1(t)-\\gamma_1\\bar{g}_1(t)\\bar{w}-\\gamma_1\\bar{g}_2(t)}{-\\gamma_1F(t)\\bar{l}}, \\end{align} \\] 相应的边界条件为 \\[ \\bar{V}(t,\\bar{w},\\bar{l},\\bar{y})=-\\frac{\\lambda_1}{\\gamma_1}e^{-\\gamma_1[\\bar{g}_1(t)\\bar{w}+\\bar{g}_2(t)]-rt}, \\] 其中 \\[ \\begin{align} \\bar{g_1}(t)&amp;=\\left[ e^{-\\int_t^Trds}+\\int_t^Te^{-\\int_t^s rdu}ds \\right]^{-1},\\\\ \\bar{g}_2(t)&amp;=\\int_t^Te^{-\\int_t^s\\bar{g}_1(u)du}\\times\\left[ \\bar{g}_1(s)\\left( C(s)-\\frac{1-\\ln \\bar{g}_1(s)-\\ln \\lambda_1}{\\gamma_1} \\right)+\\frac{(\\mu-r)^2}{2\\gamma_1\\sigma^2}+\\frac{r}{\\gamma_1} \\right]ds. \\end{align} \\] [!Note] 比较两种情况下获得的最优投资和福利替代政策。我们发现最优的投资策略是保持不变的，不存在模型的模糊性。这是因为最优投资策略与劳动收入无关。劳动收入过程不影响投资策略。然而增长率的不确定性影响福利调整政策，两种情况下福利支付策略是不同的。 7.4 数值分析 本节探究金融市场中的参数对于最优投资分配和福利策略的影响。 首先假设\\(\\mu_1(x)=A+Bc^x\\)是年龄\\(x\\)岁时的死亡力(Force of Mortality)，于是存活函数就可以写为 \\[ \\begin{align} s(x)&amp;=e^{-\\int_0^{x-a_0}\\mu(a_0+s)ds}\\\\ &amp;=e^{-A(x-a_0)-\\frac{B}{\\ln c}(c^x-c^{a_0})},\\;a_0\\leq x\\leq a_1. \\end{align} \\] 基础参数设置为\\(A=0.00022,B=2.7\\times10^{-6},c=1.124,a_0=30,a_1=65,a_2=100,n=10,\\eta_0=0.01\\).结合函数\\(s(x),F(t)\\)的表达式 \\[ F(t)=\\int_{a_1}^{a_2}n(t-x+a_0)s(x)e^{-\\eta_0(x-a_1)}dx, \\] 可以得出累积的福利因子\\(F=188.8688\\)​​. 用R语言程序计算如下 Code A &lt;- 0.00022 B &lt;- 2.7e-6 c &lt;- 1.124 a_0 &lt;- 30 a_1 &lt;- 65 a_2 &lt;- 100 n &lt;- 10 eta_0 &lt;- 0.01 s &lt;- function(x) { exp(-A * (x - a_0) + (B / log(c)) * (c^x - c^a_0)) } integrand &lt;- function(x) { n * s(x) * exp(-eta_0 * (x - a_1)) } # 计算积分 F F &lt;- integrate(integrand, lower = a_1, upper = a_2) cat(&quot;积分 F 的值为:&quot;, F$value, &quot;\\n&quot;) # 输出结果为 ## 积分 F 的值为: 188.8688 又有参数\\(\\eta_1=0.02,\\lambda=0.3,g_D=0.01,w_0=150,y_0=0,\\lambda_m=1,\\lambda_1=0.3,\\gamma=0.3,C_0=0.1\\)无风险利率\\(r=0.01,\\)关键的协整系数的参数\\(k=0.15,v_D=0.16,\\sigma=0.16.\\) :question: 基于Benzoni et al. 2007 的工作，使用\\(\\mathbb{E}_t[\\bar{y}_s],\\mathbb{E}_t[\\bar{L}_s],s\\in[t,T]\\)的解析解。为了模拟\\(\\bar{y}(t),\\bar{L}(t)\\)的值，假设劳动收入服从正态分布，于是有 \\[ \\mathbb{E}_t[\\bar{L}(s)]=e^{\\mathbb{E}_t[\\bar{l}(s)]+\\frac{1}{2}Var_t[\\bar{l}(s)]}, \\] 其中 $$ \\[\\begin{align} \\mathbb{E}_t[\\bar{l}(s)]&amp;=\\bar{l}_0-\\bar{y}_0(1-e^{-ks})+(g_D-\\bar{\\lambda}-\\frac{\\sigma^2}{2})s,\\\\ Var_t[\\bar{l}(s)]&amp;=v_D^2[s-\\frac{1}{2k}(3-e^{-ks})(1-e^{-kt})]+(\\sigma-v_D)^2s+2[v_D(\\sigma-v_D)(s-\\frac{1}{k}(1-e^{-ks}))]. \\end{align}\\] $$ 图4.4展示了劳动收入的确定性模式\\(\\bar{L}(t)\\)随时间\\(t\\)​变化的情况，显然劳动收入随时间呈上升趋势。 图4.5展示了在参数\\(k\\)控制的不同协整条件下均值回归系数对薪资和福利支付比率的影响。该参数衡量劳动收入回归其平均值的快慢，提供了对收入稳定性或波动性的洞察。从图4.5.1中我们看到，对于任何固定时间，劳动收入随着的增加而增加。这是因为当增加时，劳动收入变得更具“股票特性”。参数决定了变量\\(\\bar{}()\\)向长期均值的回归速度，并且捕捉了与劳动收入和股息相关的协整时间尺度。此外，协整意味着长期依赖性，因此，劳动收入在不同的协整水平下在短期内不会发生变化。图4.5.2显示了福利调整支付率和增长减少。较低的福利支付比率\\(\\bar{b}^∗()\\)意味着更多的钱留在基金中。劳动收入的增加导致福利替代支付比率的增长下降，这反映了养老金制度对退休收入的支付比例。 图4.6展示了在每个时间点最优值\\(\\bar{}^∗()\\)和\\(\\bar{}^∗()\\)的变化。从图4.6.1可以观察到，最优投资配置\\(\\bar{}^∗()\\)减少，这意味着在给定的时期内，投资于风险资产的比例较低。这似乎符合投资智慧，年轻的决策者通常会在风险资产上投资较多。此外，从图4.6.2可以看到，福利调整支付比率\\(\\bar{}^∗()\\)​呈上升趋势，这表明随着时间的推移，基金经理提供的支付比例在增加。 接下来研究风险厌恶参数\\(\\gamma_1\\)对最优投资和福利策略的影响。 图4.7展示了风险厌恶对最优策略的影响。在图4.7.1中观察到\\(\\bar{\\pi}^*(t)\\)随风险厌恶系数增加而减少，意味着投资风险资产比例越来越少，风险厌恶系数增加导致基金经理会更谨慎进行投资。图4.7.2中我们看到最优福利调整比率几乎不随着不同的\\(\\gamma_1\\)变化，这表明养老金成员收到的收益几乎不会受到投资者风险偏好水平的影响。 7.5 总结 本文聚焦于一个持续时间内的集体定义缴款（CDC）养老金基金方案，其中股票与劳动收入是协整的。我们假设成员的特质性冲击的增长率未知，并通过一个连续时间的二态隐藏马尔可夫链来建模。利用标准的HJB方程，我们得到了最优投资策略和福利调整政策的闭式解。此外，我们还提供了数值实例，展示了参数如何影响最优策略。具体来说，我们发现信念水平\\(\\)会影响福利支付比率。基金经理对成员劳动收入未来增长率的信心越大，成员将更可能分配更高的福利支付比率。 如果考虑人力资本，扩展我们的模型可能会很有趣。当引入人力资本时，基金经理的最优投资策略可能会随着协整水平的变化而发生变化。在长期内，劳动收入在协整关系稳固时表现出“股票特性”，这减少了基金经理投资于风险资产的兴趣。因此，我们需要更复杂的技术来获得平衡策略的显式表达式。此外，研究在常数相对风险厌恶（CRRA）效用函数下的最优问题将具有重要意义。CRRA模型考虑到个体在财富相对变化的情况下的风险偏好，而不是绝对财富量，这增加了显著的复杂性，并反映了经济决策中的更现实的情景。 "],["mulreinsurance.html", "8 最优多维再保险与多元风险厌恶效用 8.1 共同冲击依赖结构下的最优多维再保险政策 8.2 应用于ESG投资的多元风险厌恶效用 8.3 Conclusion", " 8 最优多维再保险与多元风险厌恶效用 本章开始于2025年2月26日。 8.1 共同冲击依赖结构下的最优多维再保险政策 论文原文Optimal multidimensional reinsurance policies under a common shock dependency structure点击这里查看。 再保险是指以下保险公司为避免自己遭遇损失和风险而向另一家保险公司投保的过程。 在本文中，我们考虑一家在多个相关业务线上运营的保险公司。我们假设每条业务线的风险过程都服从Cramér–Lundberg风险模型，并采用共同冲击（common shock）依赖结构来描述不同业务线可能会同时发生索赔的情况。 作者考虑一种共同冲击依赖结构用于对再保险公司的盈余过程进行建模，目标是针对再保险策略向量最大化目标函数，即随时间积分的预期折现盈余水平。 保险公司通过再保险策略向再保险公司转移部分风险，本文的目标是利用动态规划方法，最大化某一目标函数（即随时间积分的期望贴现盈余）。我们将最优目标函数（价值函数）描述为满足相应Hamilton–Jacobi–Bellman（HJB）方程及一定边界条件的唯一解。此外文章还提出了一种数值算法，用于计算目标函数的最优解，并得到相应的最优再保险策略。 8.1.1 Common shock model 考虑一家运营于\\(n\\)个相互依赖保险业务线的保险公司。事实上，索赔可能同时发生在多个业务线上，例如，一次车祸可能对汽车和司机造成损害。因此，我们假设有\\(m\\)个源，其中每个源的发生会导致一个或多个业务线发生索赔。图8.1 图8.1: An example of common shock model with m = 4 sources and n = 3 lines. 需要注意的是源1和源4都会在第一条业务线发生索赔，但这些索赔的概率分布可能不同。 更准确地说，考虑一个概率空间\\((\\Omega,\\mathcal{E},P)\\),在此空间中我们考虑具有参数\\(\\beta_i\\)的独立Poisson过程\\(\\{N_i(t):t\\geq0\\}\\),用于描述源\\(i\\in\\{1,2,...,m\\}\\)中事件的发生频率。将源\\(i\\)在业务线\\(j\\)上的第\\(k\\)次索赔大小表示为随机变量\\(Y_{ijk}\\).假设\\(\\{Y_{ijk}:i\\in\\{1,2,...,m\\},j\\in A_i,k\\in\\mathbb{N}\\}\\)对于第\\(k\\)次是独立的随机变量，其中\\(A_i\\subset\\{1,2,...,n\\}\\)是源\\(i\\)影响的业务线的集合，\\(Y_{ijk}\\)具有累积分布函数\\(F^Y_{ij}\\)和有限的均值\\(\\mu_{ij}\\)。因此源\\(i\\)(固定\\(i\\))在时间\\(t\\)之前造成的总索赔金额为 \\[ \\sum_{k=1}^{N_i(t)}\\sum_{j\\in A_i}Y_{ijk},\\quad i=1,2,...,m \\] 给定初始资本\\(x\\),保险公司在时间\\(t\\)的盈余为 \\[ X(t)=x+pt-\\sum_{i=1}^m\\sum_{k=1}^{N_i(t)}\\sum_{j\\in A_i}Y_{ijk}, \\tag{8.1} \\] 其中\\(p\\)是保费率，并使用带有相对安全载荷因子\\(\\eta_j&gt;0\\)的期望值原则来计算 \\[ p=\\sum_{j=1}^np_j,\\quad p_j=(1+\\eta_j)\\sum_{i=1}^m\\beta_i\\mu_{ij}I_{A_i}(j), \\tag{8.2} \\] 其中\\(I_{A_i}(x)\\)是示性函数。上面这个式子也许需要更进一步理解：\\(\\beta_i\\)是源\\(i\\)发生的平均次数,\\(\\mu_{ij}\\)是所有\\(k\\)次索赔的平均索赔大小。 \\(p_j\\)是业务线\\(j\\)的保费； \\(\\eta_j\\)是与业务线\\(j\\)相关的相对安全载荷因子，表示对风险的附加费用； \\(\\beta_i\\)是源\\(i\\)的平均发生次数； \\(\\mu_{ij}\\)是源\\(i\\)在业务线\\(j\\)上的平均索赔大小。 \\(I_{A_i}(j)\\)是示性函数，表示如果业务线 \\(j\\) 受源 \\(i\\) 影响，则 \\(I_{A_i}(j)=1\\)，否则为 \\(0\\). 8.1.1.1 Reinsurance 在本小节中，将会定义一个拥有\\(n\\)条业务线的公司再保险策略。首先，考虑滤波\\(\\mathcal{F}=\\{\\mathcal{F}_t:t\\geq0\\}\\)其中\\(\\mathcal{F_t}\\)是由\\(\\{X(s):0\\leq s\\leq t\\}\\)生成的\\(\\sigma\\)-代数。 再保险策略是一个多维的随机过程\\(\\boldsymbol{U}=\\{\\boldsymbol{U}(t)=(U_1(t),...,U_n(t)):t\\geq0\\}.\\)如果在时刻\\(t=t_1\\)发生源\\(i\\)的索赔，大小为\\(Y=\\sum_{j\\in A_i}Y_{ijk}\\),那么再保险公司将承担\\(Y-\\sum_{j\\in A_i} r_j(U_j(t_1), Y_{ijk})\\),其中函数\\(0\\leq r_j(u,y)\\leq y\\)是连续且关于\\(y\\)是递增的。我们说\\(\\boldsymbol{U}\\)是可接受的，如果对于\\(j=1,2,...,n\\)和\\(t\\geq0,y\\geq0\\)函数\\((\\omega,t,y)\\rightarrow r_j(U_j(\\omega,t),y)\\)是\\(\\mathcal{E\\times B\\times B}\\)可测的，并且函数\\(\\omega\\rightarrow \\sum_{j\\in A_i}r_j(U_j(\\omega,t),y)\\)是\\(\\mathcal{F_t}\\)可测的。我们将所有可接受策略记为\\(\\boldsymbol{\\mathcal{R}}.\\)本文将考虑下面两种再保险合约： 比例再保险：再保险公司承担比例 \\(u\\) 的索赔金额，即\\(r^P(u,y)=uy,\\quad u\\in \\mathcal{U}^P=[0,1]\\). 损失超额再保险（XL）：再保险公司承担至 \\(u\\) 的索赔金额，但最多只承担 \\(y\\)，即\\(r^{XL}(u,y)=\\min(u,y),\\quad u\\in\\mathcal{U}^{XL}=[0,\\infty].\\) 由公式(8.1),由再保险策略\\(\\boldsymbol{U}\\)控制的盈余过程就写为 \\[ X_{\\boldsymbol{U}}(t)=x+\\int_0^tp(\\boldsymbol{U}(s))ds-\\sum_{i=1}^m\\sum_{k=1}^{N_i(t)}\\sum_{j\\in A_i} r_j(U_j(\\tau_{ik}^-),Y_{ijk}), \\tag{8.3} \\] 其中\\(\\tau_{ik}\\)是源\\(i\\)第\\(k\\)次发生索赔的时间。使用期望值原则计算保费 \\[ p(\\boldsymbol{u})=p-\\sum_{j=1}^nq_j(u_j),\\quad \\boldsymbol{u}\\in \\mathcal{U} \\] 并且业务线\\(j\\)的再保险保费是\\(q_j(u_j)\\) \\[ q_j(u_j)=(1+\\theta_j)\\sum_{i=1}^m\\beta_i\\mathbb{E}(Y_{ijk}-r_j(u_j,Y_{ijk}))I_{A_i}(j) \\] 其中\\(\\theta_j&gt;\\eta_j\\)是安全载荷因子，\\(p\\)在(8.2)中定义，\\(r_j(u_j,Y_{ijk})\\)是保险公司按照再保险策略需要承担的索赔金额，且\\(\\mathcal{U}=\\mathcal{U}_1\\times\\mathcal{U}_2\\times ...\\times\\mathcal{U_n}\\)是\\(\\mathbb{R}^n\\)中一个子集。对于比例再保险\\(U_j=[0,1]\\)而对于损失超额再保险\\(U_j=[0,\\infty].\\) 8.1.1.2 The value function 给定一个再保险策略\\(\\boldsymbol{U}\\)和初始财富盈余\\(x\\geq0\\),类似于Cani &amp; Thonhauser, 如下定义目标函数 \\[\\begin{align} V_{\\boldsymbol{U}}(x)&amp;=\\mathbb{E}\\left(\\int_0^{\\tau_{\\boldsymbol{U}}} e^{-\\delta x}X_{\\boldsymbol{U}}(s)ds \\mid X_{\\boldsymbol{U}}(0)=x \\right)\\\\ &amp;=\\mathbb{E}_x\\left(\\int_0^{\\tau_{\\boldsymbol{U}}} e^{-\\delta x}X_{\\boldsymbol{U}}(s)ds \\right), \\tag{8.4} \\end{align}\\] 其中\\(\\tau_{\\boldsymbol{U}}\\)是破产时刻 \\[ \\tau_{\\boldsymbol{U}}=\\inf\\{t\\geq0:X_{\\boldsymbol{U}}(t)&lt;0\\}, \\] \\(\\delta&gt;0\\)是折现率。该函数称为随时间积分的预期折现盈余水平，价值函数则由式(8.5)给出 \\[ V(x)=\\sup_{\\boldsymbol{U}\\in\\boldsymbol{R}} V_{\\boldsymbol{U}}(x), \\tag{8.5} \\] 由Cani &amp; Thonhauser的研究可以得知，价值函数\\(V(x)\\)是严格递增、局部Lipschitz连续，因此是绝对连续的，并且对于\\(x\\geq0,\\)有 \\[ \\frac{x}{\\delta}&lt;V(x)\\leq \\frac{x}{\\delta}+\\frac{P}{\\delta^2}. \\tag{8.6} \\] 8.1.2 HJB equation 为了能够使用动态规划的方法解决(8.5),并进一步确保解的最优性，我们首先需要找到与\\(V(x)\\)相关的Hamilton-Jacobi-Bellman方程。 引理1 价值函数\\(V(x)\\)也就是下面方程的解： \\[ \\sup_{\\{\\boldsymbol{u}\\in\\mathcal{U}:p(\\boldsymbol{u})\\geq0\\}} H_g(x,\\boldsymbol{u})=0, \\tag{8.7} \\] 其中 \\[ H_g(x,\\boldsymbol{u})=x+p(\\boldsymbol{u})g&#39;(x)-(\\delta+\\beta)g(x)+\\sum_{i=1}^m\\beta_i\\int_0^x g(x-z)d F_i^{\\boldsymbol{u}}(z). \\] 且\\(\\beta=\\sum_{i=1}^m\\beta_i\\),\\(F_i^{\\boldsymbol{u}}\\)是\\(\\sum_{j\\in A_i}r_j(u_j,Y_{ijk})\\)的累积分布函数。 8.2 应用于ESG投资的多元风险厌恶效用 论文原文Multivariate risk aversion utility, application to ESG investments点击这里查看。 ESG(Environmental, social, and governance) 是优先考虑环境问题、社会问题和公司治理的投资原则的简写。考虑ESG的投资有时被称为负责任的投资（responsible investing）或者影响力投资（impact investing）。 期望效用理论（EUT）的基础自从Von Neumann和Morgenstern于20世纪50年代初的开创性工作以来就存在了，Neumann 和 Morgenstern（1953）。该理论迅速成为投资组合优化的主要分支之一。Merton在70年代基于几何布朗运动并使用其中一个广泛的效用函数家庭——超双曲绝对风险厌恶（HARA）效用，提出了连续时间下的直观解，Merton（1971）。我们在EUT框架下使用一类多元效用，类似于Cobb-Douglas效用函数（参见Rasmussen（2011）第3.4节，Campi和Owen（2011）），我们称之为多元常数相对风险厌恶（M-CRRA）。我们认为投资者在财富的不同来源上可能不会以相同的风险厌恶水平来考虑或对待所有的风险来源。市场中的某些风险来源（例如投资组合中的行业或资产类别）可能受到更大的关注、反感或带来较低的满意度；因此，投资者应具备为任何给定的风险来源附加合适风险厌恶水平的灵活性。有关各种资产类别的实证风险厌恶水平，可以参考Conine、McDonald和Tamarkin（2017）表1中的研究。多属性效用函数中对不同来源和满意度水平的研究也已被探讨，参见Kihlstrom和Mirman（1974）以及Dorfleitner和Krapp（2007）。在这种背景下，我们可以将资产类别（或一组股票）的风险视为一个属性。 尽管我们的假设较为简单，但多元效用也可以解释为：一方面，投资者对资产类别的偏好；另一方面，对风险的偏好。这两者是不同的概念，但在效用理论中，风险偏好常常超越了资产类别偏好，这使得后者只是简单地在分析中包括或排除资产类别，这通常对于投资者来说并不理想。将资产类别偏好与风险偏好结合的一个有趣例子是环境、社会和公司治理（ESG）投资主题。在过去20年中，ESG驱动的投资规模已增长到超过30万亿美元的资产管理规模，全球可持续投资资产在2018年飙升至30万亿美元（2019年），并且未来呈现出明显的增长趋势。为了ESG的目的，企业在三个方面进行评估：首先是它们在环境目标方面的工作，其次是它们对某些社会运动的支持，最后是公司是否按照多样性、公平性和包容性运动进行治理，即ESG（环境、社会和治理）。许多方法已经被设计用来衡量企业对ESG目标的对齐情况，这被称为“ESG评分”，参见Whelan（2021）对超过1000项不同评分研究的回顾。除了企业/公司/股票的“ESG评分”，我们还应该考虑投资者的“ESG口味”（资产类别偏好，属性偏好），这可以解释为投资者对其投资组合中股票的ESG评分的喜好或反感。 本文使用多变量或多属性效用的概念，将不同的风险规避水平与不同的财富来源（例如行业、股票、资产类别）联系起来。 本文的目标是通过风险厌恶水平将投资者的ESG偏好纳入投资组合优化分析中。在ESG文献中，资产类别偏好和风险厌恶这两个话题通常被分开处理，本文提供一种更简化的替代方法，不仅具有闭式解还具备潜在的推广性。为了简化问题，将股票分为绿色股票和棕色股票（这两种属性的股票对投资者的吸引程度不同），因此有两种可能的风险厌恶水平。这里的“绿色”指的是公司为社会创造积极的外部效益（即具有良好的ESG评分），而“棕色”则带来负面外部效益，即低ESG评分。本文中的结果可以适应ESG评分与风险厌恶水平之间更灵活的关系。 关于ESG投资组合优化主要有两条研究路线，一条依赖于预期效用理论（EUT），另一条则基于均值方差理论（MVT）。于ESG投资组合优化这一主题的新颖性以及其不断发展，下面简要提及一些当前的研究。 在EUT分支中，作者假设代理人在喜欢财富的同时，也从持有绿色股票中获得效用，从持有棕色股票中获得负效用。该分支的一个例子是P'astor等人（2021）的研究，在离散时间中，作者考虑\\(N\\)股票和形如\\(u(x)=-\\exp(-\\gamma x-b&#39;\\pi)\\)的指数效用函数。这里的\\(x\\)代表财富，\\(\\gamma\\)是绝对风险厌恶系数，\\(\\pi\\)是投资于风险资产的财富比例向量，\\(b\\)是代理人从股票持有中获得的非金钱收益向量。收益向量\\(b\\)是代理人特定和公司特定的组成部分\\(b=dg\\)，其中\\(g\\)是每个公司特定的（即ESG评分）\\(N\\times 1\\)向量，\\(d\\geq0\\)是衡量代理人ESG偏好的标量。类似地，Dorfleitner and Nguyen（2017）使用了一个混合效用函数：\\(u(x,s)=(1-\\alpha)u_F(x)+\\alpha u_S(s),\\)其中\\(\\alpha\\in[0,1]\\)可以解释为相比于财务方面投资者的ESG目标的重要性，\\(x\\)是财富，\\(s\\)是ESG回报，计算方式为\\(s=x_0(\\sum\\pi_i(1+s_i)-1)\\),其中\\(s_i\\)是公司\\(i\\)的ESG评分。 至于MVT，Schmidt（2020）考虑了一个目标函数:\\(\\frac{1}{2}\\lambda\\sigma^2_p-\\mu_p+\\gamma\\delta&#39;\\pi\\),其中\\(\\mu_p,\\sigma_p\\)分别是投资组合的均值和标准差，\\(\\gamma\\)捕捉了ESG对投资者的重要性，而\\(\\delta\\)是一个包含投资组合成分ESG的向量。在Gasser et al. (2017) 中，作者使用了类似的目标函数:\\(-\\beta \\sigma_P^2+\\alpha\\mu_P+\\gamma\\theta,\\)其中\\(\\theta=\\sum\\pi_i\\theta_i\\)表示投资组合的社会责任评分，而\\(\\gamma\\)表示投资者的ESG偏好。De Spiegeleer et al. (2021) 中，作者将ESG偏好作为优化的约束条件，形式为\\(\\gamma\\cdot\\pi&lt;\\gamma_P\\),其中\\(\\gamma\\) 是投资组合成分的ESG评分向量。Pedersen et al. (2021) 则通过目标函数\\(\\frac{1}{2}\\lambda\\sigma^2_p+\\mu_p+xf(s)\\)获得了ESG的有效前沿，其中\\(f(s)\\)是ESG偏好函数，依赖于风险资产头寸中的平均ESG评分，\\(f(s)=f(\\frac{s&#39;\\pi}{1&#39;\\pi})\\). 8.2.1 Setting and theoretical results 为了简便起见，我们假设有两只股票\\(S_1,S_2\\)其动态过程如下： \\[\\begin{equation} \\begin{split} \\frac{dS_{1,t}}{S_{1,t}}&amp;=(r+\\lambda_1\\sigma_1^2)dt+\\sigma_1dW_{1,t} \\\\ \\frac{dS_{2,t}}{S_{2,t}}&amp;=\\left(r+\\lambda_1\\sigma_1\\sigma_2\\rho+\\lambda_2\\sigma_2^2\\sqrt{1-\\rho^2}\\right)dt+\\sigma_2\\left(\\rho dW_{1,t}+\\sqrt{1-\\rho^2}dW_{2,t}\\right)\\\\ &amp;=(r+\\lambda_{22}\\sigma^2_2)dt+\\sigma_2dW_{3,t} \\end{split} \\tag{8.8} \\end{equation}\\] 其中\\(W_{1,t},W_{2,t}\\)是无关的布朗运动，\\(\\rho\\in(-1,1),\\sigma_i&gt;0,i=1,2;\\lambda_1\\sigma_1\\)是与风险因子\\(W_1\\)相关的单位波动率的市场风险价格（Market Price of Risk, MPR），而\\(\\lambda_2\\sigma_2\\)是与风险因子\\(W_2\\)相关的市场风险价格。通过下面的式子可以将\\(W_1,W_2\\)的MRP与\\(W_3\\)的MRP相联系： \\[ \\lambda_{22}\\sigma_2=\\lambda_1\\sigma_1\\rho+\\lambda_2\\sigma_2\\sqrt{1-\\rho^2}. \\] 假设投资者财富为\\(X_t,\\)投资者将财富的比例\\(\\pi_{1,t}\\)分配给\\(S_{1,t};\\pi_{2,t}\\)分配给\\(S_{2,t}\\),其余比例\\((1-\\pi_{1,t}-\\pi_{2,t})\\)存入银行账户\\(B_t,\\)银行利率为常数\\(r.\\)投资者的自融条件为： \\[\\begin{align} \\frac{dX_t}{X_t}=&amp;\\pi_{1,t}\\frac{dS_{1,t}}{S_{1,t}}+\\pi_{2,t}\\frac{dS_{2,t}}{S_{2,t}}+(1-\\pi_{1,t}-\\pi_{2,t})\\frac{dB_t}{B_t}\\\\ =&amp;\\left(r+\\pi_{1,t}\\lambda_1\\sigma_1^2+\\pi_{2,t}\\left(\\lambda_1\\sigma_1\\sigma_2\\rho+\\lambda_2\\sigma_2^2\\sqrt{1-\\rho^2}\\right) \\right)dt\\\\ &amp;+\\pi_{1,t}\\sigma_1dW_{1,t}+\\pi_{2,t}\\sigma_2\\left(\\rho dW_{1,t}+\\sqrt{1-\\rho^2}dW_{2,t}\\right) \\end{align}\\] 投资者根据风险的来源,即\\(W_{1,t},W_{2,t}\\)表现出不同的偏好，即不同的风险厌恶水平。例如\\(W_{1,t}\\)代表绿色股票\\(S_1\\)的风险，投资者对该风险的厌恶程度较低，\\(W_{2,t}\\)是棕色股票\\(S_2\\)相应的与\\(S_1\\)不相关的风险，投资者对其表现出较高的风险厌恶，以捕捉投资者ESG偏好。 接下来在构建财富过程和效用函数时，将会把这两种风险来源分开。 \\[\\begin{align} d \\log X_t&amp;=rdt+d\\log X_{1,t}+d\\log X_{2,t}\\\\ d\\log X_{1,t}&amp;=\\left( \\pi_{1,t}\\lambda_1\\sigma_1^2+\\pi_{2,t}\\lambda_1\\sigma_1\\sigma_2\\rho-\\frac{1}{2}(\\pi_{1,t}\\sigma_1+\\pi_{2,t}\\sigma_2\\rho)^2 \\right)dt+(\\pi_{1,t}\\sigma_1+\\pi_{2,t}\\sigma_2\\rho)dW_{1,t}\\\\ d\\log X_{2,t}&amp;=\\left(\\pi_{2,t}\\lambda_2\\sigma_2^2\\sqrt{1-\\rho^2}-\\frac{1}{2}\\pi_{2,t}^2\\sigma_2^2(1-\\rho^2) \\right)dt+\\pi_{2,t}\\sigma_2\\sqrt{1-\\rho^2}dW_{2,t} \\end{align}\\] 通过以上设置，\\(X_{1,t}\\)捕捉了由绿色投资驱动的财富演化，而\\(X_{2,t}\\) 则代表与非跨期的棕色投资相关的财富，这些 \\(X_i\\) 可以被解释为导致投资者满意度不同的属性。具体地，我们有： \\[\\begin{align} \\frac{dX_{1,t}}{X_{1,t}}&amp;=(\\pi_{1,t}\\lambda_1\\sigma_1^2+\\pi_{2,t}\\lambda_1\\sigma_1\\sigma_2\\rho)dt+(\\pi_{1,t}\\sigma_1+\\pi_{2,t}\\sigma_2\\rho)dW_{1,t}\\\\ &amp;=(\\pi_{1,t}\\sigma_1+\\pi_{2,t}\\sigma_2\\rho)(\\lambda_1\\sigma_1 dt+dW_{1,t})\\\\ \\frac{dX_{2,t}}{X_{2,t}}&amp;=\\pi_{2,t}\\lambda_2\\sigma_2^2\\sqrt{1-\\rho^2} dt+\\pi_{2,t}\\sigma_2\\sqrt{1-\\rho^2} dW_{2,t}\\\\ &amp;=\\pi_{2,t}\\sigma_2\\sqrt{1-\\rho^2}(\\lambda_2\\sigma_2 dt+dW_{2,t}) \\end{align}\\] 与以下关系： \\[\\begin{align} \\log\\frac{X_T}{X_0}&amp;=rT+\\log\\frac{X_{1,T}}{X_{1,0}}\\log\\frac{X_{2,T}}{X_{2,0}}\\\\ X_0&amp;=X_{1,0}X_{2,0}\\\\ X_T&amp;=\\exp(rT)X_{1,T}X_{2,T} \\end{align}\\] 为了区分投资者的风险偏好，我们使用以下多重常数相对风险厌恶效用： \\[ u(X_{1,T},X_{2,T})=sign(\\alpha_1)\\frac{(X_{1,T})^{\\alpha_1}}{\\alpha_1}\\frac{(X_{2,T})^{\\alpha_2}}{\\alpha_2} \\] 其中\\(0&lt;\\alpha_1\\leq\\alpha_2&lt;1\\)或者\\(\\alpha_2\\leq\\alpha_1&lt;0.\\)注意，\\(u(X_{1,T}, X_{2,T})\\) 是凹的（Hessian 矩阵是半负定的），并且在每个变量上是递增的. Arrow–Pratt系数用于衡量绝对风险厌恶度。其依赖于变量的财富水平： 对绿色投资的风险厌恶系数为\\(-\\frac{\\frac{\\partial^2u}{\\partial X_1^2}}{\\frac{\\partial u}{\\partial X_1}}=\\frac{1-\\alpha_1}{X_1}\\); 对棕色投资的风险厌恶系数为\\(-\\frac{\\frac{\\partial^2u}{\\partial X_2^2}}{\\frac{\\partial u}{\\partial X_2}}=\\frac{1-\\alpha_2}{X_2}.\\) 这个问题的价值函数为： \\[ V(t,X_{1,0},X_{2,0})=\\max_{\\pi_{1,t},\\pi_{2,t}}\\mathbb{E}[u(X_{1,T},X_{2,T})]=\\max_{\\pi_{1,t},\\pi_{2,t}}\\mathbb{E} \\left[sign(\\alpha_1)\\frac{(X_{1,T})^{\\alpha_1}}{\\alpha_1}\\frac{(X_{2,T})^{\\alpha_2}}{\\alpha_2}\\right] \\tag{8.9} \\] 在不失一般性的情况下，我们选择初始值 \\(X_{1,0} = X_0\\), \\(X_{2,0} = 1\\)。接下来，我们给出论文的主要结果。 Proposition 假设\\(0&lt;\\alpha_1\\leq\\alpha_2&lt;1\\)或者\\(\\alpha_2\\leq\\alpha_1&lt;0,\\)则方程(8.9)的最优配置和价值函数为： \\[\\begin{align} \\pi_{2}^*&amp;=\\frac{\\lambda_2}{(1-\\alpha_2)\\sqrt{1-\\rho^2}},\\\\ \\pi_1^*&amp;=\\frac{\\lambda_1}{1-\\alpha_1}-\\frac{\\lambda_2\\sigma_2\\rho}{\\sigma_1(1-\\alpha_2)\\sqrt{1-\\rho^2}};\\\\ V(X_{1,t},X_{2,t})&amp;=sign(\\alpha_1)\\frac{X_{1,t}^{\\alpha_1}X_{2,t}^{\\alpha_2}}{\\alpha_1\\alpha_2}\\exp\\{b(T-t)\\},\\\\ \\text{where }b&amp;=\\frac{1}{2}\\left( \\frac{\\sigma_1^2\\lambda_1^2\\alpha_1}{1-\\alpha_1}+\\frac{\\sigma_2^2\\lambda_2^2\\alpha_2}{1-\\alpha_2}\\right). \\end{align}\\] Proof 图8.2: Proof of Proposition Remark 我们接下来强调一下这一结果和背景中的几个重要方面： 如果\\(\\rho\\)趋近于1（或 -1），\\(\\pi_2\\)会趋向于\\(\\infty\\)，而 \\(\\pi_1\\) 会趋向于\\(-\\infty\\)（或 \\(\\infty\\)）。这与\\(\\alpha_1=\\alpha_2\\)的情况相同。其背后的逻辑是，在极限情况下，可以创造出比现金账户更优的无风险投资组合。这也解释了协方差矩阵可逆的假设。 该工作可以扩展到 \\(N\\) 个资产，假设风险厌恶结构为 $0 &lt; _1 ⋯ _n &lt; 1 $或 \\(\\alpha_n \\leq ⋯ \\leq \\alpha_1&lt; 0，\\)并且效用函数为： \\[ u(X_{1,T}, … , X_{n,T}) = \\begin{cases} \\text{sign}(\\alpha₁) \\prod_{i=1}^{N}\\frac{ (X_{i,T})^{\\alpha_i}}{ \\alpha_i}, &amp; N\\text{ even} \\\\ \\quad\\quad\\prod_{i=1}^{N} \\frac{(X_{i,T})^{\\alpha_i}} { \\alpha_i}, &amp; N\\text{ odd} \\end{cases} \\] 对于股票，最方便的表示方法是使用下三角矩阵，以分离独立的风险因子\\(W_i,i=1,2,...,N\\)，这些因子驱动着各个属性 （记为\\(X_i,i=1,2,...,N\\)），其方程为： \\[ \\frac{dS_{i,t}}{S_{i,t}} = \\left( r + \\sum_{j=1}^{i} \\sigma_i \\lambda_j \\sigma_j \\rho_{ij} \\right) dt + \\sigma_i \\left( \\sum_{j=1}^{i} \\rho_{ij} dW_{j,t} \\right) \\] 用矩阵表示形式为： \\[ dS_t = diag(S_t) \\left( (r + diag(\\sigma) diag(\\lambda) A) dt + diag(\\sigma) A dW_t \\right) \\] 其中 \\(S_t,W_t,\\lambda\\)和\\(\\sigma\\)是向量，\\(A\\)代表协方差矩阵\\(\\rho=A&#39;A\\)的下三角分解。这导致了一个方便的自融资条件的表示： \\[\\begin{align} \\frac{dX_t}{X_t} &amp;= \\sum_{i=1}^{N} \\pi_{i,t} \\frac{dS_{i,t}}{S_{i,t}} + \\left( 1 - \\sum_{i=1}^{N} \\pi_{i,t} \\right) \\frac{dB_t}{B_t}\\\\ &amp;=\\sum_{i=1}^N\\pi_{i,t}\\sum_{j=1}^i\\sigma_i\\rho_{ij}(\\lambda_j\\sigma_j dt+d W_{j,t})+\\frac{dB_t}{B_t} \\end{align}\\] 通过使用 \\(d\\log X_t=rdt+\\sum_{j=1}^N d\\log X_{j,t}\\)ₜ，可以表示为各个独立财富（属性）\\(X_j:j=1,...,N\\)： \\[ \\frac{dX_{j,t}}{X_{j,t}} = \\bar{\\pi}_j (\\lambda_j \\sigma_j dt + dW_{j,t}) \\] 其中 \\(\\bar{\\pi}_j = \\sum_{i=j}^{N} \\pi_{i,t} \\sigma_i \\rho_{ij}\\)。 该问题现在可以通过 ${} = ({}_1, … , {}_N)’ $ 轻松求解，且 \\(\\bar{\\pi}_j = \\frac{\\lambda_j}{(1 - \\alpha_j) \\sigma_j},j=1,..,N\\)，然后通过矩阵表示法将其转换回\\(\\pi\\)，即 \\(\\bar{\\pi} = A&#39; diag(\\sigma) \\pi\\)。 在存在两组股票的情况下：\\(S_1,...,S_{N_1}\\)和\\(S_{N_1+1},...,S_N\\)，每组具有不同的风险厌恶系数 \\(\\alpha_A\\)和\\(\\alpha_B\\)，分别对应之前提到的符号表示方法，我们可以写出以下方程： \\[\\begin{align} &amp;d \\log X_{A,t} = \\sum_{j=1}^{N₁} \\log X_{j,t}\\\\ &amp;d \\log X_{B,t} = \\sum_{j=N₁+1}^{N} \\log X_{j,t}\\\\ &amp;d \\log X_t = r dt + d \\log X_{A,t} + d \\log X_{B,t} \\end{align}\\] 解决方法将与之前类似，通过分组\\(\\alpha_1=...=\\alpha_{N_1}=\\alpha_A\\) 和\\(\\alpha_{N_1+1}=...=\\alpha_N=\\alpha_B\\)。 有趣的是，我们可以重写(8.8)使用“棕色股票”作为“绿色股票”的驱动因素，模型为： \\[\\begin{equation} \\begin{split} \\frac{dS_{2,t}}{S_{2,t}}&amp; = (r + \\lambda_{22} \\sigma_2^2) dt + \\sigma_2 dW_{3,t}\\\\ \\frac{dS_{1,t}}{S_{1,t}} &amp;= \\left( r + \\lambda_{22} \\sigma_1 \\sigma_2 \\rho + \\lambda_{11} \\sigma_1^2 \\sqrt{1 - \\rho^2} \\right) dt + \\sigma_1 \\left( \\rho dW_{3,t} + \\sqrt{1 - \\rho^2} dW_{4,t} \\right)\\\\ &amp;= (r + \\lambda_1 \\sigma_1^2) dt + \\sigma_1 dW_{5,t}\\\\ \\end{split} \\tag{8.10} \\end{equation}\\] 其中关系为： \\[ \\lambda_1 \\sigma_1 = \\lambda_{22} \\sigma_2 \\rho + \\lambda_{11} \\sigma_1 \\sqrt{1 - \\rho^2} \\] 理论同样成立，但解决方法有所不同。两者的区别在于，投资者如何在风险厌恶的角度解释绿色股票和棕色股票之间的共同/共享风险（相关部分）。模型(8.8)将其视为绿色股票的较低风险厌恶，而模型(8.10)则假设它与棕色股票的风险厌恶相同（即更高的风险厌恶）。 8.2.2 Numerical analysis and discussion 本节研究不同风险厌恶程度和相关性对最优资产配置的影响，以及使用流行的次优策略所造成的财富等价损失（CEL）。我们假设标准的年化参数设置为\\(\\sigma_1=0.35,\\sigma_2=0.4,\\rho=0.5,r=0.01,\\lambda_1=0.8,\\lambda_2=0.5\\).这意味着资产1和资产2的预期回报分别为\\(\\mu_1=r+\\lambda_1\\sigma_1^2=0.108,\\mu_2=r+\\lambda_1\\sigma_1\\sigma_2\\rho+\\lambda_2\\sigma_2^2\\sqrt{1-\\rho^2}=0.1353.\\) Code knitr::include_graphics(&quot;https://Go9entle.github.io/picx-images-hosting/image.2obo7r6g6q.webp&quot;) knitr::include_graphics(&quot;https://Go9entle.github.io/picx-images-hosting/image.99thysar6u.webp&quot;) 图8.3: Optimal allocation versus changes in 2, left uses \\(\\alpha_1\\) = 0.6, right with \\(\\alpha_1\\) = −1. 图8.3左侧展示了固定\\(\\alpha_1=0.6\\)时\\(\\alpha_2\\)的变化对股票配置的影响，右侧则是固定\\(\\alpha_1=-1,\\alpha_2\\)从-1变动至-5.在这两种情况下，将更高的风险厌恶水平赋予棕色股票会显著增加对绿色股票的配置，同时棕色股票的投资大幅下降。 Code knitr::include_graphics(&quot;https://Go9entle.github.io/picx-images-hosting/image.2obo7r6g6q.webp&quot;) knitr::include_graphics(&quot;https://Go9entle.github.io/picx-images-hosting/image.99thysar6u.webp&quot;) 图8.4: Optimal allocation versus changes in correlation for, left uses \\(\\alpha_1\\) = 0.6, \\(\\alpha_2\\)= 0.3, right with \\(\\alpha_1\\)= −1, \\(\\alpha_2\\)= −5. 图8.4显示了两只股票之间的相关性对最优配置的影响。左侧固定\\(\\alpha_10=0.6,\\alpha_2=0.1,\\)右侧固定\\(\\alpha_1=-1,\\alpha_2=-5.\\)我们可以看到，负相关会导致绿色投资的配置显著增加。这一点特别重要，因为气候变化可能会导致绿色股票和棕色股票的表现出现负相关。 接下来，我们研究了由于缺乏构建最优解决方案的知识，投资者在保持相同的风险厌恶水平下，使用次优策略所产生的财富等价损失（CEL）。由于使用相同的风险厌恶水平得到的配置是次优的，因此会导致效用损失。我们将使用次优策略得到的价值函数记作\\(V^s\\)，然后定义 CEL 为满足以下方程的标量\\(q\\)： \\[ V(t,X_0(1-q),1)=V^s(t,X_0,1) \\] 我们使用HJB方程利用\\(\\pi_1,\\pi_2\\)作为次优的常数策略，得到： \\[ V^s(t,X,1)=sign(\\alpha_1)\\frac{X^{\\alpha_1}v^s(t)}{\\alpha_1\\alpha_2}\\\\ v^s(t)=\\exp\\{b^s(T-t)\\} \\] 其中 \\[ \\begin{align} b^s=&amp;(\\pi_1\\lambda_1\\sigma_1^2+\\pi_2\\lambda_1\\sigma_1\\sigma_2\\rho)\\alpha_1+\\frac{1}{2}(\\pi_1\\sigma_1+\\pi_2\\sigma_2\\rho)^2(\\alpha_1-1)\\alpha_1\\\\ &amp;+\\pi_2\\lambda_2\\sigma_2\\sqrt{1-\\rho^2}\\alpha_2+\\frac{1}{2}\\pi_2^2\\sigma_2^2(1-\\rho^2)(\\alpha_2-1)\\alpha_2 \\end{align} \\] 于是我们得到： \\[ q=1-\\exp\\left\\{ \\frac{1}{\\alpha_1}(b^s-b)(T-t)\\right\\}. \\] 其中\\(b=\\frac{1}{2}\\left(\\frac{\\alpha_1\\sigma_1^2\\lambda_1^2}{1-\\alpha_1}+\\frac{\\alpha_2\\sigma_2^2\\lambda_2^2}{1-\\alpha_2} \\right)\\) 图(暂时未能复现)显示，由于保持相同风险厌恶策略，投资者可能面临高达 65% 的财富损失。这可以从右侧图中看到，在考虑棕色投资的风险厌恶度为\\(\\alpha_2=-6\\) 时，投资者需要 65% 更少的初始财富来匹配一个风险厌恶度为\\(\\alpha_2=-6\\) 的投资者的表现，而后者使用的是由\\(\\alpha_1=-1\\) 推导出的配置。 8.3 Conclusion 本研究使用多重风险厌恶效用的概念，也称为多属性效用，开辟了 ESG 投资的新方向。在假设投资者对棕色投资赋予更高的风险厌恶而对绿色投资赋予较低风险厌恶的前提下，我们得出了封闭形式的直观解，解决了预期效用设置下的最优配置和价值函数。这使我们能够探索两种风险厌恶设置在 ESG 投资中的含义，且选取了合理的股票参数进行研究。 该研究可以扩展到多个方向，不仅可以考虑其他多变量效用、每个风险厌恶下的多资产情况以及更丰富、更现实的模型来描述基础资产，还可以将 ESG 应用扩展到其他提出的考虑 ESG 偏好和评分的形式。 "],["RLreinsurance.html", "9 A Hybrid Framework for Reinsurance Optimization: Integrating Generative Models and Reinforcement Learning 9.1 Abstract 9.2 Introduction 9.3 Model Description 9.4 A Hybrid Framework for Generative Models and Reinforcement Learning in Reinsurance Optimization 9.5 Comprehensive Evaluation of Optimization Frameworks 9.6 Applicability to Reinsurance Optimization 9.7 结论与未来工作", " 9 A Hybrid Framework for Reinsurance Optimization: Integrating Generative Models and Reinforcement Learning 再保险优化的混合框架：集成生成模型和强化学习 9.1 Abstract 再保险优化对于保险公司管理风险敞口、确保财务稳定和维持偿付能力至关重要。传统方法常常在应对动态索赔分布、高维约束和不断变化的市场条件时遇到困难。本文介绍了一种新颖的混合框架，将生成模型，特别是变分自编码器（VAE），与强化学习（RL）结合，使用近端策略优化（PPO）算法。该框架通过结合复杂索赔分布的生成建模与强化学习的自适应决策能力，实现了再保险策略的动态和可扩展优化。 VAE组件生成合成索赔数据，包括稀有和灾难性事件，解决了数据稀缺和波动性的问题，而PPO算法则动态调整再保险参数，以最大化盈余并最小化破产概率。通过广泛的实验验证该框架的性能，包括样本外测试、压力测试情景（例如疫情影响、灾难事件）和在不同投资组合规模下的可扩展性分析。结果表明，与传统优化技术相比，该框架具有更强的适应性、可扩展性和鲁棒性，能够实现更高的最终盈余和计算效率。 本文的主要贡献包括：开发了一种针对高维优化的混合方法，动态再保险参数化，以及在随机索赔分布下的验证。该框架为现代再保险挑战提供了变革性的解决方案，并具有在多险种保险操作、灾难建模和风险共担策略设计等领域的潜在应用。 9.2 Introduction 保险和再保险行业在管理金融风险和确保经济稳定方面发挥着关键作用。再保险是将风险从保险公司转移到再保险公司的过程，是旨在维持偿付能力和优化财务表现的风险管理战略的基石。然而，由于索赔的随机性、多维约束以及风险保留、盈利性和合规性之间的动态相互作用，设计有效的再保险策略依然是一个高度复杂的挑战。 传统的再保险优化方法，如经典的Cramér-Lundberg模型，提供了盈余动态和破产概率的基础性见解。这些模型尽管在数学上严格，但它们依赖于关于保费率和索赔分布的静态假设，这限制了它们在现代再保险实践中的适用性。这些模型的扩展，包括比例再保险和分层再保险结构，解决了其中的一些局限性，但往往计算量大，且不足以应对高维的实际场景。 近年来，机器学习和人工智能（AI）的进展展示了其在解决保险领域挑战方面的巨大潜力。生成性AI模型，如变分自编码器（VAE），已在捕捉复杂数据分布和生成反映稀有和灾难性事件的合成数据方面表现出色，这些事件在历史数据中往往被低估。与此同时，强化学习（RL）技术，特别是近端策略优化（PPO）算法，已成为应对不确定环境中的序列决策和动态优化的强大工具。 本文介绍了一种新颖的混合框架，将生成性AI与强化学习结合，动态和自适应地优化再保险策略。通过利用VAE建模索赔分布并生成合成场景，该框架克服了数据稀缺和波动性的问题。PPO算法则动态调整再保险参数——如保留率和分层边界——以应对不断变化的索赔分布、市场条件和监管约束。这一协同作用使得该框架能够实时评估和优化复杂的再保险策略，解决高维不确定性并确保财务稳定。 该混合框架通过全面的仿真验证，包括压力测试场景（如高频索赔、灾难尾事件和疫情影响），证明了其鲁棒性、可扩展性和适应性，远超传统优化方法，如动态规划、蒙特卡洛模拟和多目标优化。此外，敏感性分析和可扩展性测试也突出了该框架在不同索赔环境和不同投资组合规模下的韧性。 本文的主要贡献包括： 再保险优化的混合框架：结合生成性AI模型（VAE）和强化学习（PPO），解决再保险中的多维随机优化挑战。 再保险策略的动态参数化：结合自适应的保留率和分层边界，在不断变化的市场条件下确保风险分担机制的灵活性。 全面的验证与基准测试：通过与已建立的优化技术的对比，证明了框架在可扩展性、计算效率和鲁棒性方面的优越性。 本文其余部分的结构如下：第2节介绍盈余过程、再保险结构和优化目标的数学基础；第3节介绍混合计算框架，详细描述了生成性AI和强化学习的整合；第4节描述实验设置、结果和与替代方法的对比；第5节讨论所提出框架的意义和局限性，第6节总结了主要发现和未来的研究方向。 通过弥合传统精算方法和前沿AI技术之间的鸿沟，本研究提供了一种变革性的再保险优化框架，解决了金融风险管理中的紧迫挑战，并为保险行业未来的创新奠定了基础。 9.3 Model Description 本节介绍了一种稳健且是适应性强的框架，用于建模保险公司在有限计划期\\(T\\)内的运营。该模型通过引入离散时间建模、广义盈余过程和动态再保险机制，解决了风险管理、动态索赔过程和财务稳定性等关键挑战。该方法为在不确定性下优化决策提供了全面的基础。 9.3.1 离散时间框架 计划期\\(T\\)被划分为\\(n\\)个离散时间间隔，记为\\(0=t_1&lt;t_2&lt;...&lt;t_n=T.\\)每个时间间隔表示保险公司管理风险组合、生成保费收入并发生索赔的时间步。该时间结构反映了现实世界中的保险实践，其中财务表现和风险敞口会定期进行审查和调整。 离散时间的公式化使得对风险和财务指标进行精细评估成为可能，允许将随机因素融入操作决策中，这种粒度对于分析索赔、保费和再保险之间的动态相互作用至关重要。 9.3.2 盈余过程建模 财务盈余定义为保险公司资产与负债之间的差额，并随着时间推移在索赔发生和保费收取的基础上演变。盈余过程使用改进的Cramer-Lundberg框架进行建模，这是精算科学中的基础方法。 这\\(N_i\\)为第\\(i\\)个时间间隔\\([t_{i-1},t_i)\\)内的索赔数量，模型具有参数为\\(\\lambda\\Delta t_i\\)的Poisson随机变量，其中\\(\\Delta t_i=t_i-t_{i-1}.\\)每个索赔\\(X_{ij}\\)假定为独立同分布的，盈余过程定义为 \\[ S_{i+1}=S_i+c\\Delta t_i-\\sum_{j=1}^{N_i}X_{ij}, \\tag{9.1} \\] 其中 \\(S_t\\):时间\\(t\\)时的盈余， \\(c\\):单位时间的保费率，计算公式为： \\[ c=(1+\\theta)\\lambda \\mathbb{E}[X], \\tag{9.2} \\] 其中\\(\\theta&gt;0\\)为安全载荷系数，确保了盈利性和偿付能力。 该公式提供了盈余演变的动态表示，允许对财务稳定性和风险缓解策略进行严格分析。 9.3.3 引入再保险机制 再保险是风险分担的重要工具，使得保险公司能够将其部分责任转移给再保险公司。本框架结合了多种再保险安排，以适应不同的风险配置和操作需求。 比例再保险 在比例再保险中，保险公司保留每个索赔的固定比例\\(\\alpha,\\)剩余部分由再保险公司承担，此时盈余过程修改为 \\[ S_{i+1}=S_i+c\\Delta t_i-\\sum_{j=1}^{N_i}\\alpha X_{ij}, \\tag{9.3} \\] 其中\\(\\alpha\\in[0,1]\\)称为表示保留率，这种简单的方法在风险保留和成本效率之间取得平衡。 分层再保险 分层再保险将索赔分为预定义的层级，每个层级具有不同的保留率。对于索赔\\(X_{ij},\\)其保留的损失表示为： \\[ L_{ij}=\\sum_{k=1}^K\\alpha_k\\min(\\max(X_{ij}-a_k,0),b_k-a_k), \\tag{9.4} \\] 其中： \\([a_k,b_k]\\):第\\(k\\)层的边界， \\(\\alpha_k\\):第\\(k\\)层的保留率， \\(K:\\)层级总数 这种结构能够实现战略性的风险分担，在优化成本效益的同时缓解高严重度的损失。 动态再保险调整 为了应对市场条件和监管要求i的变化，本框架引入了动态调整保留率和层级边界： \\[\\begin{align} &amp;\\alpha_k(t_i)=\\alpha_k^{\\text{base}}+\\delta_k(t_i), \\tag{9.5}\\\\ &amp;a_k(t_i)=a_k^{\\text{base}}+\\Delta a_k(t_i),b_k(t_i)=b_k^{\\text{base}}+\\Delta b_k(t_i), \\tag{9.6} \\end{align}\\] 其中\\(\\delta_k(t_i),\\Delta a_k(t_i),\\Delta b_k(t_i)\\)是由强化学习代理提供的时间依赖性调整。这些动态能力增强了框架对随机和时间不确定性的适应性。 9.3.4 优化目标 主要目标是最大化终期盈余\\(S_n,\\)考虑保费、索赔和再保险成本： \\[ \\max_{\\alpha_k,a_k,b_k}\\mathbb{E}[U(S_n)]. \\tag{9.7} \\] 该优化受到以下约束的限制： 破产概率约束：财务破产的概率不得超过预定的阈值： \\[ \\mathbb{P}(S_i&lt;0 \\text{ for any }i=0,1,...,n)\\leq \\psi_{target}. \\tag{9.8} \\] 预算约束：总再保险保费成本必须保持在指定的限额内： \\[ P=\\sum_{k=1}^K(1+\\theta_k)\\beta_k\\mathbb{E}[r_k(X)]\\leq P_{max}. \\tag{9.9} \\] 其中\\(\\beta_k=1-\\alpha_k,r_k(X)\\)表示由第\\(k\\)层覆盖的索赔。 层级结构约束：层级边界必须保持不重叠： \\[ a_{k+1}\\geq b_k,\\quad \\forall k. \\tag{9.10} \\] 保留率边界：保留率必须满足\\(0\\leq \\alpha_k\\leq 1,\\forall k\\),这些约束确保了财务稳定性、合规性和资源分配的效率。 9.4 A Hybrid Framework for Generative Models and Reinforcement Learning in Reinsurance Optimization （生成模型与强化学习在再保险优化中的混合框架） 再保险组合管理需要在不确定的条件下平衡财务稳定性、合规性和风险缓解。本节介绍了一个混合框架，将生成式 AI 模型，特别是变分自编码器（VAE），与强化学习（RL）结合，使用近端策略优化（PPO）进行优化。该框架通过结合合成数据生成和序列决策，解决了数据稀缺、随机索赔动态和市场条件变化等挑战。这种混合方法能够实现适应性强且动态的再保险优化，同时确保在管理多样化风险配置时具备可扩展性和稳健性。 9.4.1 使用VAE生成索赔模型 变分自编码器（VAE）作为所提框架的生成核心，旨在建模历史保险索赔的统计特性。VAE 通过生成合成索赔场景来解决数据稀缺问题，这些场景能够捕捉到观察到的数据模式和潜在的极端事件。 9.4.1.1 机器学习架构和组成部分 VAE架构包含三个核心组件： 编码器：将高维历史索赔数据映射到低维潜在空间。这个神经网络提取了显著特征同时减少了噪声，提供了一个以均值\\(\\mu\\)和对数方差\\(\\log(\\sigma^2)\\)为参数的概率表示。这种潜在表示保证了在建模多样数据分布时的灵活性。 潜在空间：潜在空间将每个索赔编码为一个概率分布。通过从这个空间中采样，VAE 生成的合成索赔能够超越观察数据集，同时遵循其统计特性。这种能力对于压力测试和探索稀有、高严重度的损失场景至关重要。 解码器：解码器从潜在空间中重建索赔，确保生成的数据与历史索赔的统计特征一致。这个过程保持了现实性，同时引入了受控的变异性，从而能够在多种场景下进行全面的政策测试。 9.4.1.2 训练目标和损失函数 VAE的训练过程平衡了两个关键目标： 重建保真度：重建损失确保了解码器生成的索赔与输入的历史索赔尽可能接近。这种保真度通过均方误差MSE来量化： \\[ \\mathcal{L}_{reconstruction}=\\mathbb{E}\\left[ \\Vert X-\\hat{X}\\Vert^2 \\right]. \\] 潜在空间正则化：Kullback-Leibler（KL）散度项正则化潜在空间，促进生成的索赔在平滑性和多样性方面的提升： \\[ \\mathcal{L}_{KL}=-\\frac{1}{2}\\sum(1+\\log \\sigma^2-\\mu^2-\\sigma^2). \\] 总体损失函数为 \\[ \\mathcal{L}_{VAE}= \\mathcal{L}_{reconstruction}+\\beta\\cdot \\mathcal{L}_{KL}. \\] 其中\\(\\beta\\)调节正则化的相对权重。 9.4.1.3 在再保险优化中的应用 VAE 生成的合成索赔通过引入多样化和极端场景，丰富了 RL 训练环境。这扩展的数据集使得 RL 智能体能够在高维、随机的条件下制定有效的策略。通过结合稀有事件，该框架确保了对灾难性损失的抗压能力，并增强了动态市场中的决策制定。 9.4.2 强化学习用于序列决策 强化学习（RL）是框架的决策核心，通过序列学习来优化再保险策略。该 RL 环境基于 OpenAI Gym 框架，模拟现实的保险操作，包括随机索赔动态、盈余演变和再保险合同调整。 9.4.2.1 状态观察和行动空间 在每个时间步\\(t,\\)RL智能体观察一个状态向量\\(s_t,\\)该向量捕捉了关键的操作指标： \\[ s_t=(S_t,\\lambda,\\{\\alpha_k\\}_{k=1}^K,\\{a_k,b_k\\}_{k=1}^K), \\] 其中\\(S_t\\)是财富盈余，\\(\\lambda\\)表示索赔强度（频率），\\(\\{\\alpha_k\\}\\)是保留率，\\(\\{a_k,b_k\\}\\)定义了再保险层的边界。 智能体通过选择行动\\(a_t\\)来调整这些参数，以优化长期的风险分担和财务稳定性。 9.4.2.2 奖励结构和策略优化 奖励函数鼓励偿付能力和营利性，同时惩罚不利结果： \\[ r_t=\\log(S_t+\\epsilon), \\] 其中\\(\\epsilon&gt;0\\)确保了数值稳定性。RL智能体通过近端策略优化（PPO）来改进策略\\(\\pi(a_t|s_t),\\)最大化累积折现奖励： \\[ J(\\pi)=\\mathbb{E}_{\\pi}\\left[ \\sum_{t=0}^T\\gamma^t r_t \\right], \\] 其中\\(\\gamma\\in[0,1]\\)用来平衡短期和长期目标。 9.4.2.3 与VAE生成的场景的集成 VAE生成的索赔引入了变异性并进行压力测试，检验 RL 智能体的策略。通过让智能体暴露于极端和稀有事件中，这种集成确保了在高维和不确定环境中的稳健性。 9.5 Comprehensive Evaluation of Optimization Frameworks 本节全面评估了所提出的混合再保险优化框架，重点讨论了仿真设置、训练指标和与现有方法的基准表现比较。 9.5.1 仿真配置和初始参数 仿真环境设计旨在模拟现实的再保险情景。下表总结了初始参数，保险人的初始盈余设置为20000美元，索赔使用具有平均频率\\(\\lambda=10\\)次/年的泊松过程进行建模，索赔大小从对数正态分布中进行抽样，参数为\\(\\mu=3.5,\\sigma=1.\\)这些合成的索赔数据用于训练VAE，从而生成真实的场景供强化学习使用。表9.1展示了仿真初始设置及参数 表9.1: 仿真设置和初始参数 参数 值 描述 时间跨度（\\(T\\)） 10年 总仿真持续时间 时间步长（\\(n\\)） 200 离散时间区间数 初始盈余（\\(S_0\\)） 20,000美元 初始财务盈余 索赔频率（\\(\\lambda\\)） 10次/年 采用泊松过程建模 索赔大小分布 对数正态（\\(\\mu\\) = 3.5, \\(\\sigma\\) = 1.0） 用于RL训练的合成索赔数据 保留率范围 [0.2, 0.5] 再保险保留率的约束 再保险层数（\\(K\\)） 5 风险分担的层数 预算限制（Budget max） 150,000美元 最大再保险预算 9.5.2 训练指标和盈余轨迹分析 训练指标和盈余轨迹突出展示了PPO智能体优化再保险策略的能力，表9.2列出了关键指标，图9.1展示了经过6144个时间步的盈余轨迹。 表9.2: PPO智能体训练指标 指标 值 总时间步数 6144 平均每回合奖励 -1070 策略梯度损失 -0.00615 熵损失 -21.2 图9.1显示了PPO智能体的学习过程，初期的波动反映了探索阶段，而随着时间的推移，盈余逐渐稳定，表明智能体在收敛到有效策略。 图9.1: Surplus Trajectory Over Time. Early fluctuations diminish as the PPO agent stabilizes surplus above the ruin threshold (red dashed line). 平均每回合奖励: -1,070的负值表明盈余波动带来了惩罚，并强调框架对财务稳定性的重视。 策略梯度损失: -0.00615的低值显示出策略网络更新稳定且一致，表明学习过程有效。 熵损失: -21.2的值表明决策过程中的随机性减少，智能体正从探索阶段转向利用阶段。 9.5.3 基准表现和比较分析 框架的表现与现有方法进行了基准对比，包括动态规划、蒙特卡洛模拟、混合深度蒙特卡洛、多目标优化和混合强化学习与生成模型。表9.3总结了结果。 Code # | echo = False # 创建表格的数据 benchmark_data &lt;- data.frame( Method = c(&quot;Dynamic Programming&quot;, &quot;Monte Carlo Simulation&quot;, &quot;Hybrid Deep Monte Carlo&quot;, &quot;Multi-Objective Optimization&quot;, &quot;Hybrid RL with Generative Models&quot;), `Final Surplus ($)` = c(12487.71, 12803.21, 12973.67, 12467.12, 14280.64), `Ruin Probability` = c(0.0, 0.0, 0.0, 0.0, 0.0), `Time (s)` = c(7.96, 414.27, 411.29, 8.52, 7.92), `Budget Utilization ($)` = c(NA, NA, NA, NA, 259.99), `Efficiency` = c(1568.63, 30.91, 31.54, 1462.96, 1802.60) ) # 使用 knitr::kable 创建表格 knitr::kable( benchmark_data, booktabs = TRUE, caption = &quot;各再保险优化方法基准表现&quot; ) 表9.3: 各再保险优化方法基准表现 Method Final.Surplus…. Ruin.Probability Time..s. Budget.Utilization…. Efficiency Dynamic Programming 12487.71 0 7.96 NA 1568.63 Monte Carlo Simulation 12803.21 0 414.27 NA 30.91 Hybrid Deep Monte Carlo 12973.67 0 411.29 NA 31.54 Multi-Objective Optimization 12467.12 0 8.52 NA 1462.96 Hybrid RL with Generative Models 14280.64 0 7.92 259.99 1802.60 9.6 Applicability to Reinsurance Optimization 再保险优化是一个复杂且动态的领域，需要强大、可扩展且适应性强的模型来有效管理财务风险。本研究中提出的混合框架集成了生成建模与强化学习，旨在解决这些挑战。本节评估了该框架在再保险中的适用性，通过分析其在不同索赔分布下的表现，测试其通过样本外和敏感性分析的适应性，并通过压力测试和灾难性事件仿真考察其在极端条件下的稳健性。此外，还评估了框架在不同投资组合规模下的可扩展性，以了解其在大规模再保险运营中的局限性和潜力。 分析结果显示，框架能够在典型的操作情境下保持盈余稳定并避免破产，同时也识别出改进的空间，特别是在尾部事件建模和大规模投资组合管理方面。该评估为框架在现实世界中的适用性提供了有价值的见解，并为未来的改进方向提供了思路。 9.6.1 生成模型在不同分布下的表现分析 生成的索赔模型在对数正态、帕累托和组合对数正态-帕累托分布下的表现进行了评估，重点考察其再现关键统计特性。通过Kolmogorov-Smirnov（KS）检验和视觉比较，突出了模型在捕捉中心趋势方面的优势，以及在建模尾部行为时的局限性。准确的尾部建模对于再保险应用至关重要，因为极端索赔对风险评估的影响不成比例。 9.6.1.1 整体模型表现 KS检验结果表明，训练数据与生成数据之间存在显著差异，KS统计量为0.6264，p值为0.0000。最大差异位置（D）为14.7174，表明模型在捕捉极端索赔时存在困难，极端事件对再保险的风险评估至关重要。图4显示了这些差异，尾部区域的低估反映了模型在捕捉高严重性索赔时的不足。 9.6.1.2 对数正态分布 对于对数正态分布，KS统计量为0.5896，p值为0.0000，表明训练数据与生成数据之间存在显著差异，尤其是在尾部区域。最大差异位置（D）为12.0666，突出表明模型在再现分布的长尾特性时的挑战。图5显示，虽然模型能够准确再现中心趋势，但大额索赔却未能得到充分体现。针对尾部事件的损失函数和数据增强策略可能会提升模型性能。 9.6.1.3 帕累托分布 对于帕累托分布，KS检验结果表明统计量为0.6230，p值为0.0000，反映出模型未能充分表现数据的重尾特性。图6揭示了这些局限，尤其是在再保险应用中至关重要的罕见、高严重性的索赔。通过定制损失函数和在训练时对尾部区域进行过采样，可能会缓解这些不足之处。 9.6.1.4 组合对数正态与帕累托分布 组合对数正态-帕累托分布进一步揭示了模型的局限性。KS统计量为0.4438，p值为0.0000，证实了尾部的差异，正如图7所示，尽管模型能够有效捕捉分布的中心特性，但罕见和极端事件仍然被低估。通过增加潜在空间维度和引入注重尾部准确性的损失函数，可能会增强模型的稳健性。这对于确保在再保险应用中进行精确建模至关重要，因为极端事件对财务稳定性有重要影响。 9.6.2 样本外表现和敏感性分析 通过样本外测试、敏感性分析和结果可视化，评估了生成索赔模型的表现。这一全面评估突出了模型的鲁棒性、对未见数据的泛化能力以及在现实应用中的局限性。样本外测试显示，平均盈余为16,686.73美元，破产概率为0.00%，表明模型能够在仿真保险环境中有效管理盈余。 图8展示了盈余动态，模型能够保持稳定，不会突破破产阈值-100。这表明模型在平衡保费收取和损失管理方面的成功。 图9展示了索赔大小分布的比较，表明模型能够再现训练数据的中心趋势。然而，尾部区域的差异表明，在极端事件的建模上仍需要进一步改进。尾部建模对再保险应用至关重要，因为罕见、高严重的索赔会显著影响风险管理和偿付能力评估。 表4总结了敏感性分析结果，显示了模型在不同参数设置下的稳健性，并保持了较低的破产概率。 9.7 结论与未来工作 所提出的混合框架结合了生成模型和强化学习，代表了再保险优化领域的重大进展。通过将变分自编码器（VAE用于建模索赔分布，并通过近端策略优化（PPO动态调整再保险策略，框架有效应对了高维和随机索赔环境中的关键挑战。实验结果展示了框架在典型操作情境下的稳健性、对变化索赔分布的适应能力和对不同投资组合规模的可扩展性。 框架在管理财务稳定性和最小化破产概率方面表现突出，通过样本外测试和敏感性分析验证了其稳健性。它能够在适度的压力条件下稳定盈余，并能适应不同的索赔分布，强调了其在现实世界应用中的潜力。压力测试场景，包括高频索赔、类疫情条件和灾难性事件，突出了框架在处理典型索赔和短期财务冲击方面的优势。然而，在持续的灾难性事件下，框架的局限性也显现出来，强调了需要量身定制的再保险结构和动态保费调整。 可扩展性分析进一步揭示了框架在管理更大规模投资组合时的挑战。尽管模型在小型投资组合中表现强劲，但随着投资组合规模的扩大，由于风险暴露的加剧，其维持盈余的能力有所下降。这些发现强调了适应性再保险策略和风险分担机制的重要性，以确保可扩展性和财务稳定性。 尽管结果令人鼓舞，框架在极端尾部事件建模上仍存在局限性，这在再保险应用中至关重要。生成模型输出中的尾部差异表明，需要进行改进。潜在的提升包括开发优先考虑尾部准确性的定制损失函数、过采样技术和增强的潜在空间表示。此外，集成动态参数调节和多智能体强化学习等先进优化技术，可能会进一步提高框架的适应性和稳健性。 未来的研究将重点解决这些局限性，设计更复杂的尾部建模方法，并将框架扩展到多条保险业务。整合市场波动、监管动态和宏观经济条件等外部因素，将进一步增强框架的现实适用性。此外，在大规模、现实世界的保险数据集上部署并验证该框架的实时设置，将对评估其可扩展性和实际相关性至关重要。 "],["datawhale.html", "10 Datawhale Quant 10.1 投资与量化投资 10.2 金融市场的基本概念 10.3 股票数据获取 10.4 量化选股策略", " 10 Datawhale Quant 本篇笔记基于(Datawhale 量化开源课程)[https://datawhalechina.github.io/whale-quant/#/]整理而成。 10.1 投资与量化投资 量化投资的一般流程 策略设计：基于金融理论、历史数据或其他分析方法，构建量化投资策略的想法 回测验证：使用历史数据对策略进行回测，检验策略的有效性和可行性，以及找到优化策略的方法。 模拟盘验证：使用虚拟账户和投资进行模拟交易，检验策略在实际市场中的表现，调整和优化策略。 实盘交易：经过验证和优化，将策略投入实际交易中执行。 10.2 金融市场的基本概念 10.2.1 货币金融学 概述 金融市场：资金从剩余方转向短缺方的市场。资金供求双方通过金融市场进行资金融通，实现金融资源有效配置，提高经济效率。 债券市场： 含义：发放和买卖债券的场所。 债券就是“借款的欠条”，借款人承诺在未来定期支付一定比例的利息，并在到期偿还本金的一种有价证券。 股票市场： 含义：已经发行的股票进行买卖、转让和流通的场所。 股票是“所有权的凭证“，持有股票者拥有索取公司一定收益、资产的权力。 货币与通货膨胀 通货膨胀：指物价水平（一个经济中商品和服务的平均价格的持续性上涨，可以以CPI(消费者物价指数)、PPI(生产者物价指数)指标加以衡量。 货币政策与财政政策 货币政策是一个国家的中央银行（美国是美联储）用以管理该国货币和利率的政策。 外汇市场 汇率是以一种货币表示的另一种货币的价格。报告汇率时，常用单位外币兑换多少本币来计算。 金融体系 直接融资和间接融资 直接融资：借款方在金融市场上，直接卖出有价证券（如股票）给贷款方，直接获得资金。 间接融资：存在金融中介机构（如银行）。中介机构向借款方发放贷款，向贷款方借入资金。 金融市场的结构 通过筹集资金资金的方式来划分：债券市场和股票市场。 通过层次结构来划分：以及市场和二级市场。 一级市场：发行市场或初级市场。是借贷人首次出售证券时形成的市场。 二级市场：证券流通市场或次级市场。是对已发行证券进行买卖、转让、流通的市场。 通过交易证券的期限划分：货币市场（交易一年内到期的短期债务工具）和资本市场（交易一年后到期的长期债务工具） 金融市场工具 货币市场工具：距离到期日不到一年，风险较低（如短期国债）。 资本市场工具：距离到期日一年以上，风险较高（如股票、公司债券）。 金融中介机构 分为：存款机构（商业银行等）、契约型储蓄机构（保险公司等）、投资中介机构（共同基金等） 功能：降低交易成分；分担风险；减少信息不对称问题 利率的风险结构 违约风险：借款人无力支付利息或本金的风险。国债一般被认为是无违约风险的。 风险溢价：指到期期限相同的有违约风险债券与无违约风险债券之间的利差。也即，为了让贷款人愿意承担本金或利息收不回来的风险，期限相同情况下，有违约风险的债券需要提供更高的收益率。 具有违约风险的债券的风险溢价总为正；且风险溢价随着违约风险的上升而增加。 流动性：指以较低成本迅速转化为现金的能力。流动性越差的债券越不好变现，因此越不受欢迎。这需要流动性较差的债券提供更高的收益率（与风险溢价同理），产生的利差称为流动性溢价。 所得税因素：税率越高的债券税前利率一般越高；存在税收优惠的债券利率一般相对低。 10.2.2 投资学 10.2.2.1 金融市场与投资环境 金融资产 实物资产：取决于该社会经济的生产能力，为经济创造净利润，如土地、建筑物、机器以及可用于生产产品和提供服务的知识。 金融资产：对实物资产的索取权 金融资产主要分为四大类： 固定收益型或债务型证券 普通股或权益型证券 基金 衍生产品（期货、期权、互换） 债券市场的意义 债券市场具有直接融资的功能，具有交易流动性（非单纯货币流动性，被央行利用，如QE），是重要的投资标的（大类资产配置中重要一员），债券市场是市场利率的均衡价格产生中心。 普通债券：普通股证券代表了证券持有者对公司的权益或所有权，证券持有者的收益并不固定, 权益投资的绩效取决于公司运营的成败。 衍生证券：其收益取决于其他资产（股票和债券）的价格，通常用来规避风险。 投资过程 资产配置：对投资大类的选择 证券选择：在每一资产大类中选择特定的证券，证券分析包括对证券进行估价和决定使投资组合集中在那些最具吸引力的资产上。 市场参与者 公司——净借款者 家庭——净储蓄者 政府——既可能是借款者又可能是储蓄者 金融中介：集中资金进行投资。包含：投资公司（公募、私募、财务公司）、商业银行、保险公司 投资银行 风险投资与私募股权 主要市场 主要分为债券市场、外汇市场、贵金属市场、大宗商品市场和股票市场 10.3 股票数据获取 股票数据根据信息来源和分析方法的不同可以分为技术面数据和基本面数据。 技术面数据 技术面数据是通过股票的历史价格和交易量等市场数据进行计算和分析得出的指标。它的核心观点是市场行为会在价格上留下痕迹，通过这些痕迹可以预测未来的价格走势。技术面数据主要关注股票价格的变动和市场趋势，常用的技术指标包括移动平均线、相对强弱指标、MACD指标等。技术面分析认为市场上已有的信息都会反映在股票价格中，因此通过分析股票价格图表和技术指标，可以尽可能准确地判断价格的走势和市场趋势。 基本面数据 基本面数据是通过分析公司的财务状况、业绩表现、竞争力等基本信息得出的评估。基本面分析认为股票的价格是与公司的基本面因素相关的，包括公司的营业收入、盈利能力、资产负债情况、市场份额、竞争优势等。基本面分析的目标是评估公司的内在价值，并基于这些评估来判断股票的投资潜力。常用的基本面指标包括市盈率、市净率、股息率等。基本面数据通常需要通过公司公开的财务报告和公告来获取。 10.3.1 股票数据常见指标介绍 10.3.1.1 技术面数据常见指标 移动平均线（Moving Average, MA） 移动平均线是通过计算一段时间内的股票平均价格来平滑价格波动。常见的移动平均线有简单移动平均线（SMA）和指数移动平均线（EMA）。 简单移动平均线是通过将一段时间内的股票收盘价相加，然后除以时间段的天数来计算的。简单移动平均线可以平滑价格波动，显示出长期趋势。 指数移动平均线对近期价格给予更高的权重，反映了市场更近期的变化。计算指数移动平均线时，当前价格会根据选定的时间段和权重系数，与之前的移动平均线值相结合。 移动平均线的应用主要包括以下几个方面： 确定趋势：投资者可以使用不同期限的移动平均线来确定趋势的强度和方向。较短期的移动平均线（如5日或10日）反映了近期的价格走势，较长期的移动平均线（如50日或200日）则更能反映长期趋势。 交叉信号：移动平均线的交叉可以提供买入或卖出的信号。例如，当短期移动平均线从下方穿过长期移动平均线时，被称为“黄金交叉”，可能暗示着价格上涨的趋势。相反，当短期移动平均线从上方穿过长期移动平均线时，被称为“死亡交叉”，可能暗示着价格下跌的趋势。 支撑与阻力线：移动平均线经常被用作支撑和阻力线的参考。当股票价格接近或穿过移动平均线时，可能会在此处遇到阻力或支撑，进而影响价格的反弹或下跌。 相对强弱指数（Relative Strength Index, RSI） RSI是一种用于衡量股票价格变动强度和速度的技术指标，可以帮助投资者判断股票市场的超买和超卖情况，以及价格的反转和确认信号。 RSI的计算基于一定时期内股票价格的平均涨跌，通常情况下取值在\\([0,100]\\). RSI指标计算常用参数为14，计算步骤如下: 首先，计算14个交易周期内涨幅和跌幅的平均值。 计算涨幅平均值与跌幅平均值的相对强弱比率（Relative Strength, RS）。 \\(\\text{RS} = \\frac{\\text{14个交易周期内涨幅平均值}}{\\text{14个交易周期内跌幅平均值}}\\)​。 计算相对强弱指数RSI:\\(\\text{RSI}=100-\\left( \\frac{100}{1+\\text{RS}} \\right)\\). RSI的数值解读如下： \\(\\text{RSI}\\in(0,30]\\): 表示股票市场被超卖，可能存在价格反弹的机会。 \\(\\text{RSI}\\in(70,100)\\): 表示股票市场被超买，可能存在价格下跌的机会。 \\(\\text{RSI}\\in[30,70)\\): 表示股票市场相对平稳，没有明显的超买或超卖信号。 投资者通常会关注RSI的超买和超卖区域，并结合其他技术指标和价格走势来辅助判断市场趋势和交易信号。例如，当RSI进入超买区域并且价格形成拐头下跌时，可能暗示进一步的价格下跌。相反，当RSI进入超卖区域并且价格形成拐头上涨时，可能暗示价格反弹或反转的机会。 随机指标（Stochastic Oscillator） 随机指标用于判断股票价格的超买和超卖情况，以及价格反转的可能性。它可以帮助投资者确定适合买入或卖出股票的时机。 MACD指标（Moving Average Convergence Divergence） MACD 指标是股票技术分析中常用的趋势追踪和买卖信号指标。它通过比较两条移动平均线的差异，来判断股票价格的趋势以及价格的买卖信号。 MACD指标由以下几个元素组成 DIF线（Difference Line）：是短期指数移动平均线（如12日EMA）减去长期指数移动平均线（如26日EMA）得到的差值线。DIF线可以较为敏感地反映价格的短期波动 DEA线（Signal Line）：是对DIF线进行平滑处理，一般使用DIF线的9日移动平均线得到。DEA 线可以平滑 DIF 线的波动，更好地体现价格的中期趋势。 MACD 柱（MACD Histogram）：是 DIF 线与 DEA 线的差值，可将价格的快速波动变化显示为柱状图。柱状图的红色柱代表 DIF 线在 DEA 线上方，表示价格可能上涨；绿色柱代表 DIF 线在 DEA 线下方，表示价格可能下跌。 MACD指标的应用主要包括以下几个方面： 趋势判断：当 DIF 线与 DEA 线发生金叉（DIF 线向上穿过 DEA 线）时，表示价格可能出现上涨趋势；当 DIF 线与 DEA 线发生死叉（DIF 线向下穿过 DEA线）时，表示价格可能出现下跌趋势。 买卖信号：当 MACD 柱由负值转为正值时，被视为买入信号；当 MACD 柱由正值转为负值时，被视为卖出信号。这些转折点可能表示价格快速波动的转变。 背离信号：观察价格和 MACD 指标的背离情况。例如，当价格创新高而 MACD 指标未能创新高时，可能表示价格上涨动能下降，可能出现价格回调。 10.3.2 Baostock的基础数据获取 证券宝是开源免费的证券数据平台。通过python API获取数据信息，该工具包返回的数据格式为pandas DataFrame类型，方便使用pandas/Numpy/Matplotlib进行数据分析和可视化。本节主要介绍如何利用Baostock获取历史A股K线数据、指数数据以及上证50成分股。 历史A股K线数据 日线是股票、期货等市场中的一种技术分析图表，每个数据点代表一天的交易信息。它记录了该品种在一天内的开盘价、最高价、最低价和收盘价等数据，并通过连续的数据点形成一条曲线，以反映该品种价格的走势。 日线可以帮助投资者识别出股票或期货的价格趋势和支撑阻力位，判断未来价格走势的可能性，从而制定相应的投资策略。除此之外，日线还能够辅助投资者进行风险控制，识别出市场中的买卖信号，更好地把握交易机会。以下是通过调用query_history_k_data_plus()方法来对日线进行数据获取。 Code import baostock as bs import pandas as pd from IPython.display import display #### 登陆系统 #### lg = bs.login() # 显示登陆返回信息 print(&#39;login respond error_code:&#39;+lg.error_code) print(&#39;login respond error_msg:&#39;+lg.error_msg) #### 获取沪深A股历史K线数据 #### # 详细指标参数，参见“历史行情指标参数”章节；“分钟线”参数与“日线”参数不同。“分钟线”不包含指数。 # 分钟线指标：date,time,code,open,high,low,close,volume,amount,adjustflag # 周月线指标：date,code,open,high,low,close,volume,amount,adjustflag,turn,pctChg rs = bs.query_history_k_data_plus(&quot;sh.600000&quot;, &quot;date,code,open,high,low,close,preclose,volume,amount,adjustflag,turn,tradestatus,pctChg,isST&quot;, start_date=&#39;2022-07-01&#39;, end_date=&#39;2022-12-31&#39;, frequency=&quot;d&quot;, adjustflag=&quot;3&quot;) print(&#39;query_history_k_data_plus respond error_code:&#39;+rs.error_code) print(&#39;query_history_k_data_plus respond error_msg:&#39;+rs.error_msg) #### 打印结果集 #### data_list = [] while (rs.error_code == &#39;0&#39;) &amp; rs.next(): # 获取一条记录，将记录合并在一起 data_list.append(rs.get_row_data()) result = pd.DataFrame(data_list, columns=rs.fields) #### 结果集输出到csv文件 #### result.to_csv(&quot;./history_A_stock_k_data.csv&quot;, index=False) display(result) #### 登出系统 #### bs.logout() 使用 bs.query_history_k_data_plus() 函数查询指定股票在指定时间范围内的 K 线数据，其中第一个参数为要查询的股票代码，第二个参数为要查询的 K 线数据字段列表，第三个参数为开始日期，第四个参数为结束日期，第五个参数为查询频率，第六个参数为复权类型。该函数返回一个对象 rs，其中包含了查询返回的错误代码和错误信息以及查询结果集。 将查询结果集转换成 Pandas DataFrame 格式，并输出到 CSV 文件中。首先创建一个空列表 data_list，然后使用 rs.next() 循环获取查询结果集里的每一行数据，将每一行数据存入 data_list 列表中。最后使用 Pandas 的 DataFrame 函数将 data_list 转换成 DataFrame 格式并输出到 CSV 文件中。 分钟线 Code rs = bs.query_history_k_data_plus(&quot;sh.600000&quot;, &quot;date,time,code,open,high,low,close,volume,amount,adjustflag&quot;, start_date=&#39;2022-07-01&#39;, end_date=&#39;2022-07-31&#39;, frequency=&quot;5&quot;, adjustflag=&quot;3&quot;) frequency：这个参数用于指定查询的数据类型。可以选择返回日线、周线、月线或分钟线的K线数据。其中，大写字母表示周期，如”D”代表日线，“W”代表周线，“M”代表月线，数字表示分钟线的周期，如”5”代表5分钟线，“15”代表15分钟线等。指数只支持日线数据。周线数据只能选择每周最后一个交易日查询，月线数据只能选择每月最后一个交易日查询。 adjustflag：这个参数用于指定查询数据是否需要进行复权处理。如果需要进行复权处理，则可以选择前复权或后复权。复权类型，默认不复权：3；1：后复权；2：前复权。已支持分钟线、日线、周线、月线前后复权。复权是一种调整股票价格的方法，以考虑公司发生的分红和配股等因素，以及股票本身的拆细和合并等因素。复权的目的是消除这些因素对股票价格的影响，使投资者能够更准确地分析股票的真实表现。 如果想将换手率转换为浮点数类型，则可以使用列表推导式，并将空字符串转换为0，最终将结果存储在”dataframe“（名为result）中的”turn“列中： result[\"turn\"] = [0 if x == \"\" else float(x) for x in result[\"turn\"]] 返回数据说明 参数名称 参数描述 算法说明 date 交易所行情日期 code 证券代码 open 开盘价 high 最高价 low 最低价 close 收盘价 preclose 前收盘价 见表格下方详细说明 volume 成交量（累计 单位：股） amount 成交额（单位：人民币元） adjustflag 复权状态(1：后复权， 2：前复权，3：不复权） turn 换手率 [指定交易日的成交量(股)/指定交易日的股票的流通股总股数(股)]*100% tradestatus 交易状态(1：正常交易 0：停牌） pctChg 涨跌幅（百分比） 日涨跌幅=[(指定交易日的收盘价-指定交易日前收盘价)/指定交易日前收盘价]*100% peTTM 滚动市盈率 (指定交易日的股票收盘价/指定交易日的每股盈余TTM)=(指定交易日的股票收盘价*截至当日公司总股本)/归属母公司股东净利润TTM pbMRQ 市净率 (指定交易日的股票收盘价/指定交易日的每股净资产)=总市值/(最近披露的归属母公司股东的权益-其他权益工具) psTTM 滚动市销率 (指定交易日的股票收盘价/指定交易日的每股销售额)=(指定交易日的股票收盘价*截至当日公司总股本)/营业总收入TTM pcfNcfTTM 滚动市现率 (指定交易日的股票收盘价/指定交易日的每股现金流TTM)=(指定交易日的股票收盘价*截至当日公司总股本)/现金以及现金等价物净增加额TTM isST 是否ST股 1是，0否 注意“前收盘价”说明： 当股票在指定交易日发生除权除息时，其前收盘价的计算方法有所不同。一般而言，前收盘价是指前一个交易日的实际收盘价，但当股权登记日与分红现金数量、配送股数和配股价等因素相结合时，前收盘价需要根据一定的计算方法得出。具体来说，需要先计算除息价，然后再计算送红股后的除权价和配股后的除权价，最后得出除权除息价。除权除息价就是指定交易日的前收盘价。该价格由交易所计算并公布，而在首发日，则将首发价格作为前收盘价。 具体计算方法如下: 1、计算除息价: 除息价=股息登记日的收盘价-每股所分红利现金额 2、计算除权价: 送红股后的除权价=股权登记日的收盘价/(1+每股送红股数) 配股后的除权价=(股权登记日的收盘价+配股价*每股配股数)/(1+每股配股数) 3、计算除权除息价 ：除权除息价=(股权登记日的收盘价-每股所分红利现金额+配股价*每股配股数)/(1+每股送红股数+每股配股数) 指数数据 指数数据是用来表示某个经济或金融市场的整体表现的数字指标，通常由一组代表该市场重要公司股票价格的股票指数构成。指数数据的意义在于让人们更好地了解市场的走势和趋势。它可以显示出市场整体的涨跌情况，帮助投资者评估其投资组合的表现，并作为制定投资决策的参考。此外，指数数据还可以被用来创建各种金融衍生品产品，例如期货和期权等。 我们可以通过API接口获取不同类型的指数K线数据，包括综合指数（如上证指数、深证综指）、规模指数（如上证50、沪深300）、行业指数（一级行业、二级行业等）、策略指数、成长指数、价值指数、主题指数、基金指数和债券指数。每种指数都有其对应的代码，例如上证指数的代码为sh.000001。下面以沪深指数K线数据为例解释说明。 沪深指数K线数据是描述中国上海证券交易所和深圳证券交易所股市行情的一种图表表示方法。它通过显示每个交易周期（如日、周或月）的四个价格点：开盘价、最高价、最低价和收盘价，来展示股市的波动情况。 Code # 获取指数(综合指数、规模指数、一级行业指数、二级行业指数、策略指数、成长指数、价值指数、主题指数)K线数据 # 综合指数，例如：sh.000001 上证指数，sz.399106 深证综指 等； # 规模指数，例如：sh.000016 上证50，sh.000300 沪深300，sh.000905 中证500，sz.399001 深证成指等； # 一级行业指数，例如：sh.000037 上证医药，sz.399433 国证交运 等； # 二级行业指数，例如：sh.000952 300地产，sz.399951 300银行 等； # 策略指数，例如：sh.000050 50等权，sh.000982 500等权 等； # 成长指数，例如：sz.399376 小盘成长 等； # 价值指数，例如：sh.000029 180价值 等； # 主题指数，例如：sh.000015 红利指数，sh.000063 上证周期 等； # 详细指标参数，参见“历史行情指标参数”章节；“周月线”参数与“日线”参数不同。 # 周月线指标：date,code,open,high,low,close,volume,amount,adjustflag,turn,pctChg rs = bs.query_history_k_data_plus(&quot;sh.000001&quot;, &quot;date,code,open,high,low,close,preclose,volume,amount,pctChg&quot;, start_date=&#39;2022-01-01&#39;, end_date=&#39;2022-06-30&#39;, frequency=&quot;d&quot;) print(&#39;query_history_k_data_plus respond error_code:&#39;+rs.error_code) print(&#39;query_history_k_data_plus respond error_msg:&#39;+rs.error_msg) # 打印结果集 data_list = [] while (rs.error_code == &#39;0&#39;) &amp; rs.next(): # 获取一条记录，将记录合并在一起 data_list.append(rs.get_row_data()) result = pd.DataFrame(data_list, columns=rs.fields) # 结果集输出到csv文件 result.to_csv(&quot;./history_Index_k_data.csv&quot;, index=False) display(result) 上证50成分股 上证50成分股指的是上海证券交易所（Shanghai Stock Exchange）挑选出来的50家规模最大、流动性最好的公司，这些公司在中国A股市场中具有较高的影响力和代表性。上证50指数是由这些50家公司的股票组成的指数。这个指数通常被视为中国股市的核心指标之一，因为它覆盖了50家规模大、具有代表性的公司，在市场风险和涨跌幅方面具有重要的参考意义。此外，由于该指数的成分股通常由市值较大、经营稳定的公司组成，因此被认为是一种相对较为稳健的投资方式。 query_sz50_stocks()：这个方法通过API接口获取上证50成分股的信息，更新频率为每周一更新。这个方法返回一个pandas的DataFrame类型，即一个二维表格数据结构，其中包含了上证50成分股的详细信息。 Code # 获取上证50成分股 rs = bs.query_sz50_stocks() # 调用 query_sz50_stocks 方法获取上证50成分股信息 print(&#39;query_sz50 error_code:&#39;+rs.error_code) # 打印方法返回错误码 print(&#39;query_sz50 error_msg:&#39;+rs.error_msg) # 打印方法返回错误信息 # 打印结果集 sz50_stocks = [] # 创建一个空列表，用于存储查询结果 while (rs.error_code == &#39;0&#39;) &amp; rs.next(): # 如果查询没有出错且还有数据 sz50_stocks.append(rs.get_row_data()) # 将获取到的数据添加到列表中 result = pd.DataFrame(sz50_stocks, columns=rs.fields) # 使用 pandas 将数据转换为 DataFrame 格式 # 结果集输出到csv文件 result.to_csv(&quot;D:/sz50_stocks.csv&quot;, encoding=&quot;gbk&quot;, index=False) # 将结果保存为 csv 文件 display(result) # 打印结果 10.4 量化选股策略 10.4.1 因子选股模型 10.4.1.1 效用模型与风险模型 效用函数 效用函数\\(U(\\cdot)\\)一般满足两个性质 若\\(x\\leq y, U(x)\\leq U(y)\\).（钱多总比钱少好） 若\\(d\\geq 0,\\)且\\(x\\leq y\\),则\\(U(x+d)-U(x)\\geq U(y+d)-U(y)\\).（给两人同样的钱，较穷的人获得的效用更高） 期望效用假说 如果一个投资者的效用函数是\\(U,\\)面对\\(n\\)种选项，并且这些选项的财富值结果可以用随机变量\\(X_1,..,X_n\\)表示，那么该投资者会选择\\(\\mathbb{E}[U(X)]\\)最大的那个选项。 损失厌恶 假设有个项目，一半的概率失败，一半概率成功。如果失败的话，投资者损失十万元，成功的话投资者获利十万元。根据效用函数的第二个性质，对于大部分人来说，效用是亏损的。在行为经济学中，这个现象叫做损失厌恶。 转换为数学语言，假设一项投资\\(A\\)的回报可以用随机变量\\(X\\)表示，这项投资的回报预期是\\(\\mathbb{E}[X]\\)。再假设一个投资者具备效用函数\\(U\\)，并且现有财富是\\(x_0\\)。那么，该投资者投资于\\(A\\)后的财富值可以用随机变量\\(x_0+X\\)表示，并且他进行该投资的效用是\\(U(x_0+X)\\)，这项投资带给他的额外效用是\\(U(x_0+X)-U(x_0)\\)。因此，投资于\\(A\\)带给投资者的预期效用收益是\\(\\mathbb{E}[U(x_0+X)-U(x_0)]\\)。 若\\(\\mathbb{E}[X]=0,\\)则该投资为零收益投资，那么 \\[ \\mathbb{E}[U(x_0+X)]\\leq U(x_0) \\] 这是因为大多数投资者的效用函数是凹的，即\\(U&#39;&#39;(X)\\leq0\\)，也可以说是 \\[ U(\\alpha x+(1-\\alpha)y)\\geq \\alpha U(x)+(1-\\alpha)U(y) \\] 这意味着 边际效用递减：随着财富增加，每增加一单位财富带来的效用增加逐渐减少； 风险厌恶：投资者更倾向于确定性收益，而不是具有相同期望值的风险收益。 则由Jensen不等式 \\[ \\mathbb{E}[U(x_0+X)]\\leq U(\\mathbb{E}[x_0+X]) \\] 又因为\\(\\mathbb{E}[X]=0,\\mathbb{E}[x_0+X]=x_0,\\)因此 \\[ \\mathbb{E}[U(x_0+X)]\\leq U(x_0). \\] 进一步来说，投资的风险就是它的收益不确定性。任何投资的回报\\(X\\)都可以写作预期收益\\(\\mathbb{E}[X]\\)和零收益投资\\(X-\\mathbb{E}[X]\\)两部分的家和。其中零收益部分带来预期效用下降，于是需要足够大的\\(\\mathbb{E}[X]\\)来弥补；这里\\(\\mathbb{E}[X]\\)被定义为这项投资的风险溢价（risk premium），只有当风险溢价高于风险带来的效用折损时投资者才愿意进行投资。 分散风险 经济学中的风险指的是未来的不确定性；而从概率学的角度来说，一个随机变量的分布越散开，它的确定性就越低。因此，有一个简易的衡量风险的标准，就是收益变量的标准差\\(\\sigma_X\\)。一般来讲，在保持\\(\\mathbb{E}[X]\\)不变的情况下，我们希望\\(\\sigma_X\\)越低越好。 理智的投资者 对于一个投资者，如果任意两个投资回报率的随机变量\\(X\\)和\\(Y\\)满足 \\(\\mathbb{E}[X]\\geq \\mathbb{E}[Y]\\)并且\\(\\sigma_X&lt;\\sigma_Y\\)（也就是说预期收益更大但是风险更小），该投资者会选择XX，那么我们说这个投资者是理智的。 10.4.1.2 MPT模型 金融资产配置的目标是将投资资金合理地分配在多种资产上，将风险控制在一定范围内的同时把收益率最大化。其中最著名的理论时现代资产配置理论（Modern Portfolio Theory, MPT），由Markowitz在1952年提出。MPT的核心思想是以最小化标准差并最大化预期收益为目标来进行资产配置，有时也成为均值—方差分析，是金融经济学一个重要基础理论。 "],["causal.html", "11 格兰格因果性 11.1 介绍 11.2 格兰格因果性的定义", " 11 格兰格因果性 11.1 介绍 考虑两个时间序列之间的因果性。 这里的因果性指的是时间顺序上的关系， 如果\\(X_{t-1}, X_{t-2}, \\dots\\)对\\(Y_t\\)有作用， 而\\(Y_{t-1}, Y_{t-2}, \\dots\\)对\\(X_t\\)没有作用， 则称\\(\\{X_t \\}\\)是\\(\\{ Y_t \\}\\)的格兰格原因， 而\\(\\{ Y_t \\}\\)不是\\(\\{ X_t \\}\\)的格兰格原因。 如果\\(X_{t-1}, X_{t-2}, \\dots\\)对\\(Y_t\\)有作用， \\(Y_{t-1}, Y_{t-2}, \\dots\\)对\\(X_t\\)也有作用， 则在没有进一步信息的情况下无法确定两个时间序列的因果性关系。 注意这种因果性与采样频率有关系， 在日数据或者月度数据中能发现的领先——滞后性质的因果关系， 到年度数据可能就以及混杂在以前变成同步的关系了。 11.2 格兰格因果性的定义 设\\(\\{ \\xi_t \\}\\)为一个时间序列， \\(\\{ \\boldsymbol{\\eta}_t \\}\\)为向量时间序列， 记 \\[\\begin{aligned} \\bar{\\boldsymbol{\\eta}}_t =&amp; \\{ \\boldsymbol{\\eta}_{t-1}, \\boldsymbol{\\eta}_{t-2}, \\dots \\} \\end{aligned}\\] 记 \\(\\text{Pred}(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t)\\)为基于 \\(\\boldsymbol{\\eta}_{t-1}, \\boldsymbol{\\eta}_{t-2}, \\dots\\) 对\\(\\xi_t\\)作的最小均方误差无偏预报， 其解为条件数学期望\\(E(\\xi_t | \\boldsymbol{\\eta}_{t-1}, \\boldsymbol{\\eta}_{t-2}, \\dots)\\)， 在一定条件下可以等于\\(\\xi_t\\)在\\(\\boldsymbol{\\eta}_{t-1}, \\boldsymbol{\\eta}_{t-2}, \\dots\\)张成的线性Hilbert空间的投影 （比如，\\((\\xi_t, \\boldsymbol{\\eta}_t)\\)为平稳正态多元时间序列）， 即最优线性预测。 直观理解成基于过去的\\(\\{\\boldsymbol{\\eta}_{t-1}, \\boldsymbol{\\eta}_{t-2}, \\dots \\}\\)的信息对当前的\\(\\xi_t\\)作的最优预测。 令一步预测误差为 \\[ \\varepsilon(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t) = \\xi_t - \\text{Pred}(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t) \\] 令一步预测误差方差，或者均方误差， 为 \\[ \\sigma^2(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t) = \\text{Var}(\\varepsilon_t(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t) ) = E \\left[ \\xi_t - \\text{Pred}(\\xi_t | \\bar{\\boldsymbol{\\eta}}_t) \\right]^2 \\] 考虑两个时间序列\\(\\{ X_t \\}\\)和\\(\\{ Y_t \\}\\)， \\(\\{(X_t, Y_t) \\}\\)宽平稳或严平稳。 如果 \\[ \\sigma^2(Y_t | \\bar Y_t, \\bar X_t) &lt; \\sigma^2(Y_t | \\bar Y_t) \\] 则称\\(\\{ X_t \\}\\)是\\(\\{ Y_t \\}\\)的格兰格原因， 记作\\(X_t \\Rightarrow Y_t\\)。 这不排除\\(\\{ Y_t \\}\\)也可以是\\(\\{ X_t \\}\\)的格兰格原因。 如果\\(X_t \\Rightarrow Y_t\\)，而且\\(Y_t \\Rightarrow X_t\\)， 则称互相有反馈关系， 记作\\(X_t \\Leftrightarrow Y_t\\)。 如果 \\[ \\sigma^2(Y_t | \\bar Y_t, X_t, \\bar X_t) &lt; \\sigma^2(Y_t | \\bar Y_t, \\bar X_t) \\] 即除了过去的信息， 增加同时刻的\\(X_t\\)信息后对\\(Y_t\\)预测有改进， 则称\\(\\{X_t \\}\\)对\\(\\{Y_t \\}\\)有瞬时因果性。 这时\\(\\{Y_t \\}\\)对\\(\\{X_t \\}\\)也有瞬时因果性。 如果\\(X_t \\Rightarrow Y_t\\)， 则存在最小的正整数\\(m\\)， 使得 \\[ \\sigma^2(Y_t | \\bar Y_t, X_{t-m}, X_{t-m-1}, \\dots) &lt; \\sigma^2(Y_t | \\bar Y_t, X_{t-m-1}, X_{t-m-2}, \\dots) \\] 称\\(m\\)为因果性滞后值(causality lag)。 如果\\(m&gt;1\\)， 这意味着在已有\\(Y_{t-1}, Y_{t-2}, \\dots\\)和\\(X_{t-m}, X_{t-m-1}, \\dots\\)的条件下， 增加\\(X_{t-1}\\), , \\(X_{t-m+1}\\)不能改进对\\(Y_t\\)的预测。 例11.1 设\\(\\{ \\varepsilon_t, \\eta_t \\}\\)是相互独立的零均值白噪声列， \\(\\text{Var}(\\varepsilon_t)=1\\), \\(\\text{Var}(\\eta_t)=1\\), 考虑 \\[\\begin{aligned} Y_t =&amp; X_{t-1} + \\varepsilon_t \\\\ X_t =&amp; \\eta_t + 0.5 \\eta_{t-1} \\end{aligned}\\] 用\\(L(\\cdot|\\cdot)\\)表示最优线性预测，则 \\[\\begin{aligned} &amp; L(Y_t | \\bar Y_t, \\bar X_t) \\\\ =&amp; L(X_{t-1} | X_{t-1}, \\dots, Y_{t-1}, \\dots) + L(\\varepsilon_t | \\bar Y_t, \\bar X_t) \\\\ =&amp; X_{t-1} + 0 \\\\ =&amp; X_{t-1} \\\\ \\sigma(Y_t | \\bar Y_t, \\bar X_t) =&amp; \\text{Var}(\\varepsilon_t) = 1 \\end{aligned}\\] 而 \\[ Y_t = \\eta_{t-1} + 0.5\\eta_{t-2} + \\varepsilon_t \\] 有 \\[\\begin{aligned} \\gamma_Y(0) = 2.25, \\gamma_Y(1) = 0.5, \\gamma_Y(k) = 0, k \\geq 2 \\end{aligned}\\] 所以\\(\\{Y_t \\}\\)是一个MA(1)序列， 设其方程为 \\[ Y_t = \\zeta_t + b \\zeta_{t-1}, \\zeta_t \\sim \\text{WN}(0, \\sigma_\\zeta^2) \\] 可以解出 \\[\\begin{aligned} \\rho_Y(1) =&amp; \\frac{\\gamma_Y(1)}{\\gamma_Y(0)} = \\frac{2}{9} \\\\ b =&amp; \\frac{1 - \\sqrt{1 - 4 \\rho_Y^2(1)}}{2 \\rho_Y(1)} \\approx 0.2344 \\\\ \\sigma_\\zeta^2 =&amp; \\frac{\\gamma_Y(1)}{b} \\approx 2.1328 \\end{aligned}\\] 于是 \\[\\begin{aligned} \\sigma(Y_t | \\bar Y_t) =&amp; \\sigma_\\zeta^2 = 2.1328 \\end{aligned}\\] 所以 \\[\\begin{aligned} \\sigma(Y_t | \\bar Y_t, \\bar X_t) = 1 &lt; 2.1328 = \\sigma(Y_t | \\bar Y_t) \\end{aligned}\\] 即\\(X_t\\)是\\(Y_t\\)的格兰格原因。 反之， \\(X_t\\)是MA(1)序列， 有 \\[ \\eta_t = \\frac{1}{1 + 0.5 B} X_t = \\sum_{j=0}^\\infty (-0.5)^j X_{t-j} \\] 其中\\(B\\)是推移算子（滞后算子）。 于是 \\[\\begin{aligned} L(X_t | \\bar X_t) =&amp; L(\\eta_t | \\bar X_t) + 0.5 L(\\eta_{t-1} | \\bar X_t) \\\\ =&amp; 0.5 \\sum_{j=0}^\\infty (-0.5)^j X_{t-1-j} \\\\ =&amp; - \\sum_{j=1}^\\infty (-0.5)^j X_{t-j} \\\\ \\sigma(X_t | \\bar X_t) =&amp; \\text{Var}(X_t - L(X_t | \\bar X_t)) \\\\ =&amp; \\text{Var}(\\eta_t) = 1 \\end{aligned}\\] 而 \\[\\begin{aligned} L(X_t | \\bar X_t, \\bar Y_t) =&amp; L(\\eta_t | \\bar X_t, \\bar Y_t) + 0.5 L(\\eta_{t-1} | \\bar X_t, \\bar Y_t) \\\\ =&amp; 0 + 0.5 L(\\sum_{j=0}^\\infty (-0.5)^j X_{t-1-j} | \\bar X_t, \\bar Y_t) \\\\ =&amp; -\\sum_{j=1}^\\infty (-0.5)^j X_{t-j} \\\\ =&amp; L(X_t | \\bar X_t) \\end{aligned}\\] 所以\\(Y_t\\)不是\\(X_t\\)的格兰格原因。 考虑瞬时因果性。 \\[\\begin{aligned} L(Y_t | \\bar X_t, \\bar Y_t, X_t) =&amp; X_{t-1} + 0 (\\text{注意}\\varepsilon_t\\text{与}\\{X_s, \\forall s\\}\\text{不相关} \\\\ =&amp; L(Y_t | \\bar X_t, \\bar Y_t) \\end{aligned}\\] 所以\\(X_t\\)不是\\(Y_t\\)的瞬时格兰格原因。 ○○○○○ 例11.2 在例11.1中，如果模型改成 \\[\\begin{aligned} Y_t =&amp; X_{t} + \\varepsilon_t \\\\ X_t =&amp; \\eta_t + 0.5 \\eta_{t-1} \\end{aligned}\\] 有怎样的结果？ 这时 \\[ Y_t = \\varepsilon_t + \\eta_t + 0.5 \\eta_{t-1} \\] 仍有 \\[\\begin{aligned} \\gamma_Y(0) = 2.25, \\gamma_Y(1) = 0.5, \\gamma_Y(k) = 0, k \\geq 2 \\end{aligned}\\] 所以\\(Y_t\\)还服从MA(1)模型 \\[ Y_t = \\zeta_t + b \\zeta_{t-1}, b \\approx 0.2344, \\sigma^2_\\zeta \\approx 2.1328 \\] \\[\\begin{aligned} L(Y_t | \\bar Y_t, \\bar X_t) =&amp; L(X_t | \\bar Y_t, \\bar X_t) + 0 \\\\ =&amp; L(\\eta_t | \\bar Y_t, \\bar X_t) + 0.5 L(\\eta_{t-1} | \\bar Y_t, \\bar X_t) \\\\ =&amp; 0 + 0.5 L(\\sum_{j=0}^\\infty (-0.5)^j X_{t-1-j} | \\bar Y_t, \\bar X_t) \\\\ =&amp; - \\sum_{j=1}^\\infty (-0.5)^j X_{t-j} \\\\ =&amp; X_t - \\eta_t \\\\ \\sigma(Y_t | \\bar Y_t, \\bar X_t) =&amp; \\text{Var}(\\varepsilon_t + \\eta_t) = 2 \\end{aligned}\\] 而 \\[ \\sigma(Y_t | \\bar Y_t) = \\sigma^2_\\zeta \\approx 2.1328 &gt; \\sigma(Y_t | \\bar Y_t, \\bar X_t) = 2 \\] 所以\\(X_t\\)是\\(Y_t\\)的格兰格原因。 反之， \\[\\begin{aligned} L(X_t | \\bar X_t, \\bar Y_t) =&amp; - \\sum_{j=1}^\\infty (-0.5)^j X_{t-j} \\\\ =&amp; L(X_t | \\bar X_t) \\end{aligned}\\] 所以\\(Y_t\\)不是\\(X_t\\)的格兰格原因。 考虑瞬时因果性。 \\[\\begin{aligned} L(Y_t | \\bar X_t, \\bar Y_t, X_t) =&amp; X_{t} + 0 (\\text{注意}\\varepsilon_t\\text{与}\\{X_s, \\forall s\\}\\text{不相关} \\\\ =&amp; X_t \\\\ \\sigma(Y_t | \\bar X_t, \\bar Y_t, X_t) =&amp; \\text{Var}(\\varepsilon) \\\\ =&amp; 1 &lt; 2 = \\sigma(Y_t | \\bar X_t, \\bar Y_t) \\end{aligned}\\] 所以\\(X_t\\)是\\(Y_t\\)的瞬时格兰格原因。 \\[\\begin{aligned} [aaa] \\end{aligned}\\] "],["usage.html", "12 中文图书Bookdown模板的基本用法 12.1 安装设置 12.2 编写自己的内容 12.3 转换", " 12 中文图书Bookdown模板的基本用法 12.1 安装设置 使用RStudio软件完成编辑和转换功能。 在RStudio中，安装bookdown等必要的扩展包。 本模板在安装之前是一个打包的zip文件， 在适当位置解压（例如，在C:/myproj下）， 得到MathJax, Books/Cbook, Books/Carticle等子目录。 本模板在Books/Cbook中。 为了利用模板制作自己的中文书， 将Books/Cbook制作一个副本， 改成适当的子目录名，如Books/Mybook。 打开RStudio软件， 选选单“File - New Project - Existing Directory”， 选中Books/Mybook子目录，确定。 这样生成一本书对应的R project（项目）。 为了将模板内容替换成自己的内容， 可以删除文件0101-usage.Rmd， 然后将1001-chapter01.Rmd制作几份副本， 如1001-chapter01.Rmd, 2012-chapter02.Rmd， 3012-chapter03.Rmd。 各章的次序将按照前面的数值的次序排列。 将每个.Rmd文件内的{#chapter01}, {#chapter02-sec01}修改能够反映章节内容的标签文本。 所有的标签都不允许重复。 参见本模板中的0101-usage.Rmd文件。 后面的§12.3.1 和§12.3.2 给出了将当前的书转换为网页和PDF的命令， 复制粘贴这些命令到RStudio命令行可以进行转换。 12.2 编写自己的内容 12.2.1 文档结构 除了index.Rmd以外， 每个.Rmd文件是书的一章。 每章的第一行是用一个井号（#）引入的章标题。 节标题用两个井号开始， 小节标题用三个井号开始。 标题后面都有大括号内以井号开头的标签， 标签仅用英文大小写字母和减号。 12.2.2 图形自动编号 用R代码段生成的图形， 只要具有代码段标签， 且提供代码段选项fig.cap=\"图形的说明文字\"， 就可以对图形自动编号， 并且可以用如\\@ref(fig:label)的格式引用图形。 如： Code plot(1:10, main=&quot;程序生成的测试图形&quot;) 图12.1: 图形说明文字 引用如：参见图12.1。 引用中的fig:是必须的。 在通过LaTeX转换的PDF结果中， 这样图形是浮动的。 12.2.3 表格自动编号 用R代码knitr::kable()生成的表格， 只要具有代码段标签， 并且在knitr::kable()调用时加选项caption=\"表格的说明文字\"， 就可以对表格自动编号， 并且可以用如\\@ref(tab:label)的格式引用表格。 如： Code d &lt;- data.frame(&quot;自变量&quot;=1:10, &quot;因变量&quot;=(1:10)^2) knitr::kable(d, caption=&quot;表格说明文字&quot;) 表12.1: 表格说明文字 自变量 因变量 1 1 2 4 3 9 4 16 5 25 6 36 7 49 8 64 9 81 10 100 引用如：参见表12.1。 引用中的tab:是必须的。 在通过LaTeX转换的PDF结果中， 这样的表格是浮动的。 12.2.4 数学公式编号 不需要编号的公式， 仍可以按照一般的Rmd文件中公式的做法。 需要编号的公式， 直接写在\\begin{align}和\\end{align}之间， 不需要编号的行在末尾用\\nonumber标注。 需要编号的行用(\\#eq:mylabel)添加自定义标签， 如 \\[\\begin{align} \\Sigma =&amp; (\\sigma_{ij})_{n\\times n} \\nonumber \\\\ =&amp; E[(\\boldsymbol{X} - \\boldsymbol{\\mu}) (\\boldsymbol{X} - \\boldsymbol{\\mu})^T ] \\tag{12.1} \\end{align}\\] 引用如：协方差定义见(12.1)。 12.2.5 文献引用与文献列表 将所有文献用bib格式保存为一个.bib文献库， 如模板中的样例文件mybib.bib。 可以用JabRef软件来管理这样的文献库， 许多其它软件都可以输出这样格式的文件库。 为了引用某一本书， 用如：参见(Wichmann and Hill 1982)。 被引用的文献将出现在一章末尾以及全书的末尾， 对PDF输出则仅出现在全书末尾。 12.3 转换 12.3.1 转换为网页 用如下命令将整本书转换成一个每章为一个页面的网站， 称为gitbook格式： Code bookdown::render_book(&quot;index.Rmd&quot;, output_format=&quot;bookdown::gitbook&quot;, encoding=&quot;UTF-8&quot;) 为查看结果， 在_book子目录中双击其中的index.html文件， 就可以在网络浏览器中查看转换的结果。 重新编译后应该点击“刷新”图标。 在章节和内容较多时， 通常不希望每次小修改之后重新编译整本书， 这时类似如下的命令可以仅编译一章， 可以节省时间， 缺点是导航目录会变得不准确。 命令如： Code bookdown::preview_chapter(&quot;1001-chapter01.Rmd&quot;, output_format=&quot;bookdown::gitbook&quot;, encoding=&quot;UTF-8&quot;) 单章的网页可以通过网络浏览器中的“打印”功能， 选择一个打印到PDF的打印机， 可以将单章转换成PDF格式。 12.3.2 生成PDF 如果想将R Markdown文件借助于LaTeX格式转换为PDF， 需要在系统中安装一个TeX编译器。 现在的rmarkdown包要求使用tinytex扩展包以及配套的TinyTeX软件包， 好像不再支持使用本机原有的LaTex编译系统， 如果不安装tinytex，编译为PDF格式时会出错。 TinyTeX优点是直接用R命令就可以安装， 更新也由R自动进行，不需要用户干预。 但是，安装时需要从国外网站下载许多文件， 有因为网络不畅通而安装失败的危险。 为了安装R的tinytex扩展包和单独的TinyTeX编译软件，应运行： Code install.packages(&#39;tinytex&#39;) tinytex::install_tinytex() 安装过程需要从国外的服务器下载许多文件， 在国内的网络环境下有可能因为网络超时而失败。 如果安装成功， TinyTeX软件包在MS Windows系统中一般会安装在 C:\\Users\\用户名\\AppData\\Roaming\\MikTex目录中， 其中“用户名”应替换成系统当前用户名。 如果需要删除TinyTeX软件包， 只要直接删除那个子目录就可以。 为了判断TinyTeX是否安装成功， 在RStudio中运行 Code tinytex::is_tinytex() 结果应为TRUE, 出错或者结果为FALSE都说明安装不成功。 在编译pdf_book时，可能会需要联网下载LaTeX所需的格式文件。 Bookdown借助操作系统中安装的LaTeX编译软件TinyTeX将整本书转换成一个PDF文件， 这需要用户对LaTeX有一定的了解， 否则一旦出错， 就完全不知道如何解决。 用户如果需要进行LaTeX定制， 可修改模板中的preamble.tex文件。 转换为PDF的命令如下： Code bookdown::render_book(&quot;index.Rmd&quot;, output_format=&quot;bookdown::pdf_book&quot;, encoding=&quot;UTF-8&quot;) 在_book子目录中找到CBook.pdf文件， 这是转换的结果。 CBook.tex是作为中间结果的LaTeX文件， 如果出错可以从这里查找错误原因。 转换PDF对于内容多的书比较耗时， 不要过于频繁地转换PDF， 在修改书的内容时， 多用bookdown::preview_chapter和转换为gitbook的办法检验结果。 定期地进行转换PDF的测试。 每增加一章后都应该试着转换成PDF看有没有错误。 12.3.3 上传到网站 如果书里面没有数学公式， 则上传到网站就只要将_book子目录整个地用ftp软件传送到自己的网站主目录下的某个子目录即可。 但是，为了支持数学公式，就需要进行如下的目录结构设置： 设自己的网站服务器目录为/home/abc， 将MathJax目录上传到这个目录中。 在/home/abc中建立新目录Books/Mybook。 将_book子目录上传到/home/abc/Books/Mybook中。 这时网站链接可能类似于http://dept.univ.edu.cn/~abc/Books/Mybook/_book/index.html, 具体链接地址依赖于服务器名称与主页所在的主目录名称。 如果有多本书， MathJax仅需要上传一次。 因为MathJax有三万多个文件， 所以上传MathJax会花费很长时间。 References Wichmann, B. A., and I. D. Hill. 1982. “Algorithm as 183: An Efficient and Portable Pseudo-Random Number Generator.” Applied Statistics 31: 188–190. Remarks: 34, 198 and 35, 89. "],["references.html", "References", " References Wichmann, B. A., and I. D. Hill. 1982. “Algorithm as 183: An Efficient and Portable Pseudo-Random Number Generator.” Applied Statistics 31: 188–190. Remarks: 34, 198 and 35, 89. "]]
