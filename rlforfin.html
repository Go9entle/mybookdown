<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK</title>
  <meta name="description" content="这是用R的bookdown功能制作的课堂笔记。" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="这是用R的bookdown功能制作的课堂笔记。" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK" />
  
  <meta name="twitter:description" content="这是用R的bookdown功能制作的课堂笔记。" />
  

<meta name="author" content="gogentle" />


<meta name="date" content="2025-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="关于涛哥.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/codefolding-lua-1.1/codefolding-lua.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX","output/SVG"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
<script type="text/javascript"
   src="../../../MathJax/MathJax.js">
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GZT FOR REAL</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>写在前面</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#为什么要写课程笔记"><i class="fa fa-check"></i>为什么要写课程笔记</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="关于涛哥.html"><a href="关于涛哥.html"><i class="fa fa-check"></i>关于涛哥</a></li>
<li class="part"><span><b>I 课程笔记</b></span></li>
<li class="chapter" data-level="1" data-path="rlforfin.html"><a href="rlforfin.html"><i class="fa fa-check"></i><b>1</b> 强化学习在金融中的应用</a>
<ul>
<li class="chapter" data-level="1.1" data-path="rlforfin.html"><a href="rlforfin.html#Markov"><i class="fa fa-check"></i><b>1.1</b> Markov过程</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="rlforfin.html"><a href="rlforfin.html#过程中的状态概念"><i class="fa fa-check"></i><b>1.1.1</b> 过程中的状态概念</a></li>
<li class="chapter" data-level="1.1.2" data-path="rlforfin.html"><a href="rlforfin.html#通过股票价格的例子理解markov性"><i class="fa fa-check"></i><b>1.1.2</b> 通过股票价格的例子理解Markov性</a></li>
<li class="chapter" data-level="1.1.3" data-path="rlforfin.html"><a href="rlforfin.html#markov过程的正式定义"><i class="fa fa-check"></i><b>1.1.3</b> Markov过程的正式定义</a></li>
<li class="chapter" data-level="1.1.4" data-path="rlforfin.html"><a href="rlforfin.html#markov过程的稳态分布"><i class="fa fa-check"></i><b>1.1.4</b> Markov过程的稳态分布</a></li>
<li class="chapter" data-level="1.1.5" data-path="rlforfin.html"><a href="rlforfin.html#markov奖励过程的形式主义"><i class="fa fa-check"></i><b>1.1.5</b> Markov奖励过程的形式主义</a></li>
<li class="chapter" data-level="1.1.6" data-path="rlforfin.html"><a href="rlforfin.html#markov奖励过程的价值函数"><i class="fa fa-check"></i><b>1.1.6</b> Markov奖励过程的价值函数</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="rlforfin.html"><a href="rlforfin.html#MDP"><i class="fa fa-check"></i><b>1.2</b> Markov决策过程</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="rlforfin.html"><a href="rlforfin.html#不确定性下的序列决策难题"><i class="fa fa-check"></i><b>1.2.1</b> 不确定性下的序列决策难题</a></li>
<li class="chapter" data-level="1.2.2" data-path="rlforfin.html"><a href="rlforfin.html#markov决策过程的正式定义"><i class="fa fa-check"></i><b>1.2.2</b> Markov决策过程的正式定义</a></li>
<li class="chapter" data-level="1.2.3" data-path="rlforfin.html"><a href="rlforfin.html#策略"><i class="fa fa-check"></i><b>1.2.3</b> 策略</a></li>
<li class="chapter" data-level="1.2.4" data-path="rlforfin.html"><a href="rlforfin.html#markov决策过程策略-markov奖励过程"><i class="fa fa-check"></i><b>1.2.4</b> [Markov决策过程，策略]:= Markov奖励过程</a></li>
<li class="chapter" data-level="1.2.5" data-path="rlforfin.html"><a href="rlforfin.html#固定策略下的mdp价值函数"><i class="fa fa-check"></i><b>1.2.5</b> 固定策略下的MDP价值函数</a></li>
<li class="chapter" data-level="1.2.6" data-path="rlforfin.html"><a href="rlforfin.html#最优价值函数和最优策略"><i class="fa fa-check"></i><b>1.2.6</b> 最优价值函数和最优策略</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="rlforfin.html"><a href="rlforfin.html#dynamicprogram"><i class="fa fa-check"></i><b>1.3</b> 动态规划算法</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="rlforfin.html"><a href="rlforfin.html#planning-vs-learning"><i class="fa fa-check"></i><b>1.3.1</b> Planning vs Learning</a></li>
<li class="chapter" data-level="1.3.2" data-path="rlforfin.html"><a href="rlforfin.html#不动点理论"><i class="fa fa-check"></i><b>1.3.2</b> 不动点理论</a></li>
<li class="chapter" data-level="1.3.3" data-path="rlforfin.html"><a href="rlforfin.html#贝尔曼策略算子以及策略评估算法"><i class="fa fa-check"></i><b>1.3.3</b> 贝尔曼策略算子以及策略评估算法</a></li>
<li class="chapter" data-level="1.3.4" data-path="rlforfin.html"><a href="rlforfin.html#贪心策略"><i class="fa fa-check"></i><b>1.3.4</b> 贪心策略</a></li>
<li class="chapter" data-level="1.3.5" data-path="rlforfin.html"><a href="rlforfin.html#策略提升"><i class="fa fa-check"></i><b>1.3.5</b> 策略提升</a></li>
<li class="chapter" data-level="1.3.6" data-path="rlforfin.html"><a href="rlforfin.html#策略迭代算法"><i class="fa fa-check"></i><b>1.3.6</b> 策略迭代算法</a></li>
<li class="chapter" data-level="1.3.7" data-path="rlforfin.html"><a href="rlforfin.html#贝尔曼最优性算子与值迭代算法"><i class="fa fa-check"></i><b>1.3.7</b> 贝尔曼最优性算子与值迭代算法</a></li>
<li class="chapter" data-level="1.3.8" data-path="rlforfin.html"><a href="rlforfin.html#从最优值函数到最优策略"><i class="fa fa-check"></i><b>1.3.8</b> 从最优值函数到最优策略</a></li>
<li class="chapter" data-level="1.3.9" data-path="rlforfin.html"><a href="rlforfin.html#广义策略迭代"><i class="fa fa-check"></i><b>1.3.9</b> 广义策略迭代</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="rlforfin.html"><a href="rlforfin.html#dynassall"><i class="fa fa-check"></i><b>1.4</b> 动态资产配置和消费</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="rlforfin.html"><a href="rlforfin.html#个人财务的优化"><i class="fa fa-check"></i><b>1.4.1</b> 个人财务的优化</a></li>
<li class="chapter" data-level="1.4.2" data-path="rlforfin.html"><a href="rlforfin.html#merton投资组合问题及其解决"><i class="fa fa-check"></i><b>1.4.2</b> Merton投资组合问题及其解决</a></li>
<li class="chapter" data-level="1.4.3" data-path="rlforfin.html"><a href="rlforfin.html#merton投资组合问题解的直觉"><i class="fa fa-check"></i><b>1.4.3</b> Merton投资组合问题解的直觉</a></li>
<li class="chapter" data-level="1.4.4" data-path="rlforfin.html"><a href="rlforfin.html#离散时间资产配置"><i class="fa fa-check"></i><b>1.4.4</b> 离散时间资产配置</a></li>
<li class="chapter" data-level="1.4.5" data-path="rlforfin.html"><a href="rlforfin.html#现实世界的应用"><i class="fa fa-check"></i><b>1.4.5</b> 现实世界的应用</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>2</b> 广义线性模型</a>
<ul>
<li class="chapter" data-level="2.1" data-path="glm.html"><a href="glm.html#glmexp"><i class="fa fa-check"></i><b>2.1</b> 广义线性模型的指数族分布</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="RFWLFR.html"><a href="RFWLFR.html"><i class="fa fa-check"></i><b>3</b> Random forest weighted local Frechet regression with random objects</a>
<ul>
<li class="chapter" data-level="3.1" data-path="RFWLFR.html"><a href="RFWLFR.html#rfwlfr2"><i class="fa fa-check"></i><b>3.1</b> 提出的方法</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="RFWLFR.html"><a href="RFWLFR.html#rfwlfr21"><i class="fa fa-check"></i><b>3.1.1</b> 预备知识</a></li>
<li class="chapter" data-level="3.1.2" data-path="RFWLFR.html"><a href="RFWLFR.html#rfwlfr22"><i class="fa fa-check"></i><b>3.1.2</b> Local constant method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="qrm.html"><a href="qrm.html"><i class="fa fa-check"></i><b>4</b> 量化风险管理</a>
<ul>
<li class="chapter" data-level="4.1" data-path="qrm.html"><a href="qrm.html#rmconcept"><i class="fa fa-check"></i><b>4.1</b> 风险管理的基本概念</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="qrm.html"><a href="qrm.html#建模价值和价值变动"><i class="fa fa-check"></i><b>4.1.1</b> 建模价值和价值变动</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stochastic.html"><a href="stochastic.html"><i class="fa fa-check"></i><b>5</b> 随机分析</a>
<ul>
<li class="chapter" data-level="5.1" data-path="stochastic.html"><a href="stochastic.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="stochastic.html"><a href="stochastic.html#最优停时optimal-stopping"><i class="fa fa-check"></i><b>5.1.1</b> 1.4 最优停时（Optimal Stopping）</a></li>
<li class="chapter" data-level="5.1.2" data-path="stochastic.html"><a href="stochastic.html#随机控制stochastic-control"><i class="fa fa-check"></i><b>5.1.2</b> 1.5 随机控制（Stochastic Control）</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="stochastic.html"><a href="stochastic.html#预备知识"><i class="fa fa-check"></i><b>5.2</b> 预备知识</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="stochastic.html"><a href="stochastic.html#概率空间-随机变量-随机过程"><i class="fa fa-check"></i><b>5.2.1</b> 概率空间 随机变量 随机过程</a></li>
<li class="chapter" data-level="5.2.2" data-path="stochastic.html"><a href="stochastic.html#布朗运动brownian-motion"><i class="fa fa-check"></i><b>5.2.2</b> 布朗运动（Brownian Motion）</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II 语言基础</b></span></li>
<li class="chapter" data-level="6" data-path="english.html"><a href="english.html"><i class="fa fa-check"></i><b>6</b> 英语</a>
<ul>
<li class="chapter" data-level="6.1" data-path="english.html"><a href="english.html#week2"><i class="fa fa-check"></i><b>6.1</b> week2</a></li>
<li class="chapter" data-level="6.2" data-path="english.html"><a href="english.html#week3"><i class="fa fa-check"></i><b>6.2</b> week3</a></li>
</ul></li>
<li class="part"><span><b>III 讨论班报告</b></span></li>
<li class="chapter" data-level="7" data-path="cdc.html"><a href="cdc.html"><i class="fa fa-check"></i><b>7</b> Optimal strategies for collective defined contribution plans when the stock and labor markets are co-integrated（股票和劳动力市场协同整合时集体确定缴款计划的最优策略）</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cdc.html"><a href="cdc.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="cdc.html"><a href="cdc.html#相关工作"><i class="fa fa-check"></i><b>7.1.1</b> 相关工作</a></li>
<li class="chapter" data-level="7.1.2" data-path="cdc.html"><a href="cdc.html#主要区别"><i class="fa fa-check"></i><b>7.1.2</b> 主要区别</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cdc.html"><a href="cdc.html#模型的公式化"><i class="fa fa-check"></i><b>7.2</b> 模型的公式化</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="cdc.html"><a href="cdc.html#金融市场"><i class="fa fa-check"></i><b>7.2.1</b> 金融市场</a></li>
<li class="chapter" data-level="7.2.2" data-path="cdc.html"><a href="cdc.html#劳动收入"><i class="fa fa-check"></i><b>7.2.2</b> 劳动收入</a></li>
<li class="chapter" data-level="7.2.3" data-path="cdc.html"><a href="cdc.html#养老金系统"><i class="fa fa-check"></i><b>7.2.3</b> 养老金系统</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="cdc.html"><a href="cdc.html#养老金基金计划的最优策略"><i class="fa fa-check"></i><b>7.3</b> 养老金基金计划的最优策略</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="cdc.html"><a href="cdc.html#具有特质性冲击的最优策略"><i class="fa fa-check"></i><b>7.3.1</b> 具有特质性冲击的最优策略</a></li>
<li class="chapter" data-level="7.3.2" data-path="cdc.html"><a href="cdc.html#无特质性冲击时的最优策略"><i class="fa fa-check"></i><b>7.3.2</b> 无特质性冲击时的最优策略</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cdc.html"><a href="cdc.html#数值分析"><i class="fa fa-check"></i><b>7.4</b> 数值分析</a></li>
<li class="chapter" data-level="7.5" data-path="cdc.html"><a href="cdc.html#总结"><i class="fa fa-check"></i><b>7.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mulreinsurance.html"><a href="mulreinsurance.html"><i class="fa fa-check"></i><b>8</b> 最优多维再保险与多元风险厌恶效用</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#OMR"><i class="fa fa-check"></i><b>8.1</b> 共同冲击依赖结构下的最优多维再保险政策</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#common-shock-model"><i class="fa fa-check"></i><b>8.1.1</b> Common shock model</a></li>
<li class="chapter" data-level="8.1.2" data-path="mulreinsurance.html"><a href="mulreinsurance.html#hjb-equation"><i class="fa fa-check"></i><b>8.1.2</b> HJB equation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mulreinsurance.html"><a href="mulreinsurance.html#mrau"><i class="fa fa-check"></i><b>8.2</b> 应用于ESG投资的多元风险厌恶效用</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#setting-and-theoretical-results"><i class="fa fa-check"></i><b>8.2.1</b> Setting and theoretical results</a></li>
<li class="chapter" data-level="8.2.2" data-path="mulreinsurance.html"><a href="mulreinsurance.html#numerical-analysis-and-discussion"><i class="fa fa-check"></i><b>8.2.2</b> Numerical analysis and discussion</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="mulreinsurance.html"><a href="mulreinsurance.html#conclusion"><i class="fa fa-check"></i><b>8.3</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="RLreinsurance.html"><a href="RLreinsurance.html"><i class="fa fa-check"></i><b>9</b> A Hybrid Framework for Reinsurance Optimization: Integrating Generative Models and Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="9.1" data-path="RLreinsurance.html"><a href="RLreinsurance.html#rlreabstract"><i class="fa fa-check"></i><b>9.1</b> Abstract</a></li>
<li class="chapter" data-level="9.2" data-path="RLreinsurance.html"><a href="RLreinsurance.html#rlreintro"><i class="fa fa-check"></i><b>9.2</b> Introduction</a></li>
<li class="chapter" data-level="9.3" data-path="RLreinsurance.html"><a href="RLreinsurance.html#rlremodel"><i class="fa fa-check"></i><b>9.3</b> Model Description</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="RLreinsurance.html"><a href="RLreinsurance.html#离散时间框架"><i class="fa fa-check"></i><b>9.3.1</b> 离散时间框架</a></li>
<li class="chapter" data-level="9.3.2" data-path="RLreinsurance.html"><a href="RLreinsurance.html#盈余过程建模"><i class="fa fa-check"></i><b>9.3.2</b> 盈余过程建模</a></li>
<li class="chapter" data-level="9.3.3" data-path="RLreinsurance.html"><a href="RLreinsurance.html#引入再保险机制"><i class="fa fa-check"></i><b>9.3.3</b> 引入再保险机制</a></li>
<li class="chapter" data-level="9.3.4" data-path="RLreinsurance.html"><a href="RLreinsurance.html#优化目标"><i class="fa fa-check"></i><b>9.3.4</b> 优化目标</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="RLreinsurance.html"><a href="RLreinsurance.html#a-hybrid-framework-for-generative-models-and-reinforcement-learning-in-reinsurance-optimization"><i class="fa fa-check"></i><b>9.4</b> A Hybrid Framework for Generative Models and Reinforcement Learning in Reinsurance Optimization</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="RLreinsurance.html"><a href="RLreinsurance.html#使用vae生成索赔模型"><i class="fa fa-check"></i><b>9.4.1</b> 使用VAE生成索赔模型</a></li>
<li class="chapter" data-level="9.4.2" data-path="RLreinsurance.html"><a href="RLreinsurance.html#强化学习用于序列决策"><i class="fa fa-check"></i><b>9.4.2</b> 强化学习用于序列决策</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="RLreinsurance.html"><a href="RLreinsurance.html#comprehensive-evaluation-of-optimization-frameworks"><i class="fa fa-check"></i><b>9.5</b> Comprehensive Evaluation of Optimization Frameworks</a>
<ul>
<li class="chapter" data-level="9.5.1" data-path="RLreinsurance.html"><a href="RLreinsurance.html#仿真配置和初始参数"><i class="fa fa-check"></i><b>9.5.1</b> 仿真配置和初始参数</a></li>
<li class="chapter" data-level="9.5.2" data-path="RLreinsurance.html"><a href="RLreinsurance.html#训练指标和盈余轨迹分析"><i class="fa fa-check"></i><b>9.5.2</b> 训练指标和盈余轨迹分析</a></li>
<li class="chapter" data-level="9.5.3" data-path="RLreinsurance.html"><a href="RLreinsurance.html#基准表现和比较分析"><i class="fa fa-check"></i><b>9.5.3</b> 基准表现和比较分析</a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="RLreinsurance.html"><a href="RLreinsurance.html#applicability-to-reinsurance-optimization"><i class="fa fa-check"></i><b>9.6</b> Applicability to Reinsurance Optimization</a>
<ul>
<li class="chapter" data-level="9.6.1" data-path="RLreinsurance.html"><a href="RLreinsurance.html#生成模型在不同分布下的表现分析"><i class="fa fa-check"></i><b>9.6.1</b> 生成模型在不同分布下的表现分析</a></li>
<li class="chapter" data-level="9.6.2" data-path="RLreinsurance.html"><a href="RLreinsurance.html#样本外表现和敏感性分析"><i class="fa fa-check"></i><b>9.6.2</b> 样本外表现和敏感性分析</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="RLreinsurance.html"><a href="RLreinsurance.html#结论与未来工作"><i class="fa fa-check"></i><b>9.7</b> 结论与未来工作</a></li>
</ul></li>
<li class="part"><span><b>IV 课外学习笔记</b></span></li>
<li class="chapter" data-level="10" data-path="datawhale.html"><a href="datawhale.html"><i class="fa fa-check"></i><b>10</b> Datawhale Quant</a>
<ul>
<li class="chapter" data-level="10.1" data-path="datawhale.html"><a href="datawhale.html#investquant"><i class="fa fa-check"></i><b>10.1</b> 投资与量化投资</a></li>
<li class="chapter" data-level="10.2" data-path="datawhale.html"><a href="datawhale.html#finmarket"><i class="fa fa-check"></i><b>10.2</b> 金融市场的基本概念</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="datawhale.html"><a href="datawhale.html#monetaryfin"><i class="fa fa-check"></i><b>10.2.1</b> 货币金融学</a></li>
<li class="chapter" data-level="10.2.2" data-path="datawhale.html"><a href="datawhale.html#investfin"><i class="fa fa-check"></i><b>10.2.2</b> 投资学</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="datawhale.html"><a href="datawhale.html#stockdataget"><i class="fa fa-check"></i><b>10.3</b> 股票数据获取</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="datawhale.html"><a href="datawhale.html#股票数据常见指标介绍"><i class="fa fa-check"></i><b>10.3.1</b> 股票数据常见指标介绍</a></li>
<li class="chapter" data-level="10.3.2" data-path="datawhale.html"><a href="datawhale.html#baostock的基础数据获取"><i class="fa fa-check"></i><b>10.3.2</b> Baostock的基础数据获取</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="datawhale.html"><a href="datawhale.html#quantselect"><i class="fa fa-check"></i><b>10.4</b> 量化选股策略</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="datawhale.html"><a href="datawhale.html#fctsctm"><i class="fa fa-check"></i><b>10.4.1</b> 因子选股模型</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="causal.html"><a href="causal.html"><i class="fa fa-check"></i><b>11</b> 格兰格因果性</a>
<ul>
<li class="chapter" data-level="11.1" data-path="causal.html"><a href="causal.html#causal-intro"><i class="fa fa-check"></i><b>11.1</b> 介绍</a></li>
<li class="chapter" data-level="11.2" data-path="causal.html"><a href="causal.html#causal-def"><i class="fa fa-check"></i><b>11.2</b> 格兰格因果性的定义</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="usage.html"><a href="usage.html"><i class="fa fa-check"></i><b>12</b> 中文图书Bookdown模板的基本用法</a>
<ul>
<li class="chapter" data-level="12.1" data-path="usage.html"><a href="usage.html#usage-ins"><i class="fa fa-check"></i><b>12.1</b> 安装设置</a></li>
<li class="chapter" data-level="12.2" data-path="usage.html"><a href="usage.html#usage-writing"><i class="fa fa-check"></i><b>12.2</b> 编写自己的内容</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="usage.html"><a href="usage.html#usage-writing-struct"><i class="fa fa-check"></i><b>12.2.1</b> 文档结构</a></li>
<li class="chapter" data-level="12.2.2" data-path="usage.html"><a href="usage.html#usage-writing-fig"><i class="fa fa-check"></i><b>12.2.2</b> 图形自动编号</a></li>
<li class="chapter" data-level="12.2.3" data-path="usage.html"><a href="usage.html#usage-writing-tab"><i class="fa fa-check"></i><b>12.2.3</b> 表格自动编号</a></li>
<li class="chapter" data-level="12.2.4" data-path="usage.html"><a href="usage.html#usage-writing-math"><i class="fa fa-check"></i><b>12.2.4</b> 数学公式编号</a></li>
<li class="chapter" data-level="12.2.5" data-path="usage.html"><a href="usage.html#文献引用与文献列表"><i class="fa fa-check"></i><b>12.2.5</b> 文献引用与文献列表</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="usage.html"><a href="usage.html#usage-output"><i class="fa fa-check"></i><b>12.3</b> 转换</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="usage.html"><a href="usage.html#usage-gitbook"><i class="fa fa-check"></i><b>12.3.1</b> 转换为网页</a></li>
<li class="chapter" data-level="12.3.2" data-path="usage.html"><a href="usage.html#usage-pdfbook"><i class="fa fa-check"></i><b>12.3.2</b> 生成PDF</a></li>
<li class="chapter" data-level="12.3.3" data-path="usage.html"><a href="usage.html#usage-website"><i class="fa fa-check"></i><b>12.3.3</b> 上传到网站</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://go9entle.github.io" target="_blank">Proudly Presented by GZT</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">GOGENTLE’s NOTEBOOK</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rlforfin" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> 强化学习在金融中的应用<a href="rlforfin.html#rlforfin" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="Markov" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Markov过程<a href="rlforfin.html#Markov" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>本书的主题是“序列不确定下的序列决策”，在本章中将暂时忽略“序列决策”方面而只关注”序列不确定性“。</p>
<div id="过程中的状态概念" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> 过程中的状态概念<a href="rlforfin.html#过程中的状态概念" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(S_t\)</span>是过程在时间<span class="math inline">\(t\)</span>时的状态。特别地，我们对于下一时刻的状态<span class="math inline">\(S_{t+1}\)</span>的概率感兴趣，如果已知现在的状态<span class="math inline">\(S_t\)</span>和过去的状态<span class="math inline">\(S_0,S_1,...,S_{t-1}\)</span>，我们对<span class="math inline">\(P\{S_{t+1}|S_t,S_{t-1},...,S_0\}\)</span>感兴趣。</p>
</div>
<div id="通过股票价格的例子理解markov性" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> 通过股票价格的例子理解Markov性<a href="rlforfin.html#通过股票价格的例子理解markov性" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>为了帮助理解，我们假设股票价格只取整数值，并且零或负股票价格是可以接受的。我们将时间<span class="math inline">\(t\)</span>的股票价格表示为<span class="math inline">\(X_t\)</span>.假设从时间<span class="math inline">\(t\)</span> 到下一个时间步骤 <span class="math inline">\(t + 1\)</span>,股票价格可以上涨<span class="math inline">\(1\)</span>或下跌<span class="math inline">\(1\)</span>,即<span class="math inline">\(X_{t+1}\)</span>的唯一两个结果是<span class="math inline">\(X_t + 1\)</span>或<span class="math inline">\(X_t − 1\)</span>.要了解股票价格随时间的随机演变，我们只需要量化上涨的概率 <span class="math inline">\(P[X_{t+1} =X_t+ 1]\)</span>.我们将考虑股票价格演变的 3 个不同过程。</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P[X_{t+1}=X_t+1]=\frac{1}{1+e^{-\alpha_1(L-X_t)}}\)</span>.</p>
<p>这意味着股票的价格倾向于均值回归(mean-reverting),均值即为参考水平<span class="math inline">\(L,\)</span>拉力系数为<span class="math inline">\(\alpha.\)</span></p>
<p>我们不妨设<span class="math inline">\(S_t=X_t,\)</span>且可以看到下一时刻的状态<span class="math inline">\(S_{t+1}\)</span>只与<span class="math inline">\(S_t\)</span>有关而与<span class="math inline">\(S_0,S_1,...,S_{t-1}\)</span>无关。即可写作
<span class="math display">\[
P[S_{t+1}|S_t,S_{t-1},...,S_0]=P[S_{t+1}|S_t]\text{ for all }t\geq 0.
\]</span>
这就被成为Markov性。</p>
<p>书中还给出了相应的代码</p>
<details class=chunk-details><summary class=chunk-summary><span class=chunk-summary-text>Code</span></summary>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="rlforfin.html#cb3-1" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb3-2"><a href="rlforfin.html#cb3-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="rlforfin.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="rlforfin.html#cb3-4" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-5"><a href="rlforfin.html#cb3-5" tabindex="-1"></a><span class="kw">class</span> Process1:</span>
<span id="cb3-6"><a href="rlforfin.html#cb3-6" tabindex="-1"></a>    <span class="at">@dataclass</span></span>
<span id="cb3-7"><a href="rlforfin.html#cb3-7" tabindex="-1"></a>    <span class="kw">class</span> State:</span>
<span id="cb3-8"><a href="rlforfin.html#cb3-8" tabindex="-1"></a>        price: <span class="bu">int</span></span>
<span id="cb3-9"><a href="rlforfin.html#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="rlforfin.html#cb3-10" tabindex="-1"></a>    level_param: <span class="bu">int</span>  <span class="co"># level to which price mean-reverts</span></span>
<span id="cb3-11"><a href="rlforfin.html#cb3-11" tabindex="-1"></a>    alpha1: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.25</span>  <span class="co"># strength of mean-reversion (non-negative value)</span></span>
<span id="cb3-12"><a href="rlforfin.html#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="rlforfin.html#cb3-13" tabindex="-1"></a>    <span class="kw">def</span> up_prob(<span class="va">self</span>, state: State) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-14"><a href="rlforfin.html#cb3-14" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span><span class="va">self</span>.alpha1<span class="op">*</span>(<span class="va">self</span>.level_param<span class="op">-</span>state.price)))</span>
<span id="cb3-15"><a href="rlforfin.html#cb3-15" tabindex="-1"></a>    <span class="kw">def</span> next_state(<span class="va">self</span>, state:State) <span class="op">-&gt;</span> State:</span>
<span id="cb3-16"><a href="rlforfin.html#cb3-16" tabindex="-1"></a>        up_move: <span class="bu">int</span> <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="va">self</span>.up_prob(state),<span class="dv">1</span>)[<span class="dv">0</span>] <span class="co">#生成随机移动 up_move = 0 or 1</span></span>
<span id="cb3-17"><a href="rlforfin.html#cb3-17" tabindex="-1"></a>        <span class="cf">return</span> Process1.State(price<span class="op">=</span>state.price <span class="op">+</span> up_move <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>) <span class="co"># 若up_move = 1, 则价格上升1，若为0价格下降1</span></span></code></pre></div>
</details>
<p>接下来，我们使用 Python 的生成器功能（使用<code>yield</code>）编写一个简单的模拟器，如下所示：</p>
<details class=chunk-details><summary class=chunk-summary><span class=chunk-summary-text>Code</span></summary>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="rlforfin.html#cb4-1" tabindex="-1"></a><span class="kw">def</span> simulation(process, start_state):</span>
<span id="cb4-2"><a href="rlforfin.html#cb4-2" tabindex="-1"></a>    state <span class="op">=</span> start_state</span>
<span id="cb4-3"><a href="rlforfin.html#cb4-3" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb4-4"><a href="rlforfin.html#cb4-4" tabindex="-1"></a>        <span class="cf">yield</span> state</span>
<span id="cb4-5"><a href="rlforfin.html#cb4-5" tabindex="-1"></a>        state <span class="op">=</span> process.next_state(state)</span></code></pre></div>
</details>
<p>现在我们可以使用此模拟器函数生成采样轨迹。在下面的代码中，我们从 <code>start_price</code>的价格<span class="math inline">\(X_0\)</span>​开始，在<code>time_steps</code>时间步长内生成<code>num_traces</code>个采样轨迹。使用 Python 的生成器功能，我们可以使用<code>itertools.islice</code>函数“懒惰地”执行此操作。</p>
<details class=chunk-details><summary class=chunk-summary><span class=chunk-summary-text>Code</span></summary>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="rlforfin.html#cb5-1" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb5-2"><a href="rlforfin.html#cb5-2" tabindex="-1"></a><span class="kw">def</span> process1_price_traces(</span>
<span id="cb5-3"><a href="rlforfin.html#cb5-3" tabindex="-1"></a> start_price: <span class="bu">int</span>,</span>
<span id="cb5-4"><a href="rlforfin.html#cb5-4" tabindex="-1"></a>    level_param: <span class="bu">int</span>,</span>
<span id="cb5-5"><a href="rlforfin.html#cb5-5" tabindex="-1"></a>    alpha1: <span class="bu">float</span>,</span>
<span id="cb5-6"><a href="rlforfin.html#cb5-6" tabindex="-1"></a>    time_steps: <span class="bu">int</span>,</span>
<span id="cb5-7"><a href="rlforfin.html#cb5-7" tabindex="-1"></a>    num_traces: <span class="bu">int</span></span>
<span id="cb5-8"><a href="rlforfin.html#cb5-8" tabindex="-1"></a>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb5-9"><a href="rlforfin.html#cb5-9" tabindex="-1"></a>    process <span class="op">=</span> Process1(level_param<span class="op">=</span>level_param, alpha1<span class="op">=</span>alpha1)</span>
<span id="cb5-10"><a href="rlforfin.html#cb5-10" tabindex="-1"></a>    start_state <span class="op">=</span> Process1.State(price<span class="op">=</span>start.price)</span>
<span id="cb5-11"><a href="rlforfin.html#cb5-11" tabindex="-1"></a>    <span class="cf">return</span> np.vstack([</span>
<span id="cb5-12"><a href="rlforfin.html#cb5-12" tabindex="-1"></a>        np.fromiter((s.price <span class="cf">for</span> s <span class="kw">in</span> itertools.islice(</span>
<span id="cb5-13"><a href="rlforfin.html#cb5-13" tabindex="-1"></a>         simulation(process, start_state),</span>
<span id="cb5-14"><a href="rlforfin.html#cb5-14" tabindex="-1"></a>            time_steps <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="rlforfin.html#cb5-15" tabindex="-1"></a>     )), <span class="bu">float</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_traces)])</span></code></pre></div>
</details></li>
<li><p><span class="math display">\[
P[X_{t+1}=X_t+1]=\begin{cases}
0.5(1-\alpha_2(X_t-X_{t-1}))&amp;\text{  if }t&gt;0\\
0.5&amp;\text{ if }t=0
\end{cases}
\]</span></p>
<p>其中<span class="math inline">\(\alpha_2\)</span>是一个“拉力强度”参数，取值在<span class="math inline">\([0,1]\)</span>之间。这里的直觉时下一步的移动的方向偏向于前一次移动的反方向。我们注意到如果依然按照前文建模则无法满足Markov性质，因为<span class="math inline">\(X_{t+1}\)</span>取值的概率不仅依赖于<span class="math inline">\(X_t,\)</span>还依赖于<span class="math inline">\(X_{t-1}.\)</span>不过我们可以在这里做一个小技巧，即创建一个扩展状态<span class="math inline">\(S_t\)</span>由一对<span class="math inline">\((X_t,X_{t-1})\)</span>组成。当<span class="math inline">\(t=0\)</span>时状态<span class="math inline">\(S_0\)</span>可以取值<span class="math inline">\((X_0,null)\)</span>,这里的null只是一个符号。通过将状态<span class="math inline">\(S_t\)</span>视为<span class="math inline">\((X_t,X_t-X_{t-1})\)</span>建模可以发现Markov性质得到了满足。
<span class="math display">\[
\begin{aligned}
&amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1}),...,(X_0,null)]\\
=&amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1})]\\
=&amp;0.5(1-\alpha_2(X_{t+1}-X_t)(X_t-X_{t-1}))
\end{aligned}
\]</span>
关于上面的式子deepseek给出了证明。</p>
<p>人们自然会想知道，为什么状态不单单由<span class="math inline">\(X_t - X_{t-1}\)</span> 组成——换句话说，为什么 <span class="math inline">\(X_t\)</span> 也需要作为状态的一部分。确实，单独知道<span class="math inline">\(X_t - X_{t-1}\)</span>可以完全确定 <span class="math inline">\(X_{t+1}-X_t\)</span>的概率。因此，如果我们将状态设定为在任意时间步 <span class="math inline">\(t\)</span>仅为 <span class="math inline">\(X_t - X_{t-1}\)</span>，那么我们确实会得到一个只有两个状态 +1 和 -1 的马尔可夫过程（它们之间的概率转移）。然而，这个简单的马尔可夫过程并不能通过查看时间<span class="math inline">\(t\)</span>的状态 <span class="math inline">\(X_t - X_{t-1}\)</span> 来告诉我们股票价格 <span class="math inline">\(X_t\)</span> 的值。在这个应用中，我们不仅关心马尔可夫状态转移概率，还关心从时间 <span class="math inline">\(t\)</span> 的状态中获取任意时间 <span class="math inline">\(t\)</span> 的股票价格信息。因此，我们将状态建模为对 <span class="math inline">\(( X_t, X_{t-1} )\)</span>。</p>
<p>请注意，如果我们将状态 <span class="math inline">\(S_t\)</span> 建模为整个股票价格历史 <span class="math inline">\(( X_0, X_1,..., X_t )，\)</span>那么马尔可夫性质将显然得到满足，将<span class="math inline">\(S_t\)</span>建模为对<span class="math inline">\((X_t,X_{t-1})\)</span>Markov性质也会得到满足。然而，我们选择 <span class="math inline">\(S_t := (X_t, X_t - X_{t-1})\)</span> 是“最简单”的内部表示。实际上，在整本书中，我们对各种过程建模状态的努力是确保马尔可夫性质，同时使用“最简单/最小”的状态表示。</p></li>
<li><p>Process3是Process2的扩展，其中下一个移动的概率不仅依赖于上一时刻的移动还依赖于过去所有的移动。具体来说，它依赖于过去上涨次数的数量记为<span class="math inline">\(U_t=\sum_{i=1}^t\max(X_i-X_{i-1},0)\)</span>,与过去下跌次数的数量，记为<span class="math inline">\(D_t=\sum_{i=1}^t\max(X_{i-1}-X_i,0)\)</span>之间的关系。表示为
<span class="math display">\[
P[X_{t+1}=X_t+1]=\begin{cases}
\frac{1}{1+(\frac{U_t+D_t}{D_t}-1)^{\alpha_3}}&amp;\text{ if }t&gt;0\\
0.5&amp;\text{ if }t=0
\end{cases}
\]</span>
其中<span class="math inline">\(\alpha_3\in\mathbb{R}_{\geq0}\)</span>是一个拉力强度参数，将上述概率表达式视为<span class="math inline">\(f(\frac{D_t}{U_t+D_t};\alpha_3)\)</span>其中<span class="math inline">\(f:[0,1]\rightarrow[0,1]\)</span>是一个sigmoid型函数
<span class="math display">\[
f(x;\alpha)=\frac{1}{1+(\frac{1}{x}-1)^{\alpha}}.
\]</span>
下一个上涨移动的概率基本依赖<span class="math inline">\(\frac{U_t}{U_t+D_t}\)</span>即过去时间步中下跌次数的比例。因此，如果历史上的下跌次数大于上涨次数，那么下一个价格移动<span class="math inline">\(X_{t+1}-X_t\)</span>将会有更多的向上拉力，反之亦然。</p>
<p>我们将<span class="math inline">\(S_t\)</span>建模为由对<span class="math inline">\((U_t,D_t)\)</span>组成，这样<span class="math inline">\(S_t\)</span>的Markov性质可以得到满足
<span class="math display">\[
\begin{aligned}
&amp;P[(U_{t+1},D_{t+1})|(U_t,D_t),...,(U_0,D_0)]=P[(U_{t+1},D_{t+1})|(U_t,D_t)]\\
&amp;=\begin{cases}
f(\frac{D_t}{U_t+D_t};\alpha_3)&amp;\text{ if }U_{t+1}=U_t+1,D_{t+1}=D_t\\
f(\frac{U_t}{U_t+D_t};\alpha_3)&amp;\text{ if }U_{t+1}=U_t,D_{t+1}=D_t+1
\end{cases}
\end{aligned}
\]</span>
重要的是与前面两个过程不同，股票价格<span class="math inline">\(X_t\)</span>实际上并不是过程3中状态<span class="math inline">\(S_t\)</span>的一部分，这是因为<span class="math inline">\(U_t,D_t\)</span>共同包含了捕捉<span class="math inline">\(X_t\)</span>的足够信息，因为<span class="math inline">\(X_t=X_0+U_t-D_t.\)</span></p></li>
</ol>
</div>
<div id="markov过程的正式定义" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Markov过程的正式定义<a href="rlforfin.html#markov过程的正式定义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>书中的定义和定理将由限制在离散时间和可数状态集合。</p>
<blockquote>
<p><strong>Def 3.3.1</strong></p>
<p>Markov过程由以下组成</p>
<ul>
<li>一个可数状态集合<span class="math inline">\(\mathcal{S}\)</span>（称为状态空间）和一个子集<span class="math inline">\(\mathcal{T}\subset \mathcal{S}\)</span>​（称为终止状态集合）。</li>
<li>一个时间索引的随即状态序列<span class="math inline">\(S_t\in S,\)</span>时间步为<span class="math inline">\(t=0,1,2,...\)</span>,每个状态转移都满足Markov性质:<span class="math inline">\(P[S_{t+1}|S_t,...,S_0]=P[S_{t+1}|S_t],\text{for all }t\geq0.\)</span></li>
<li>终止：如果某个时间步<span class="math inline">\(T\)</span>的结果<span class="math inline">\(S_T\)</span>是集合<span class="math inline">\(\mathcal{T}\)</span>中的一个状态，则该序列的结果在时间步<span class="math inline">\(T\)</span>终止。</li>
</ul>
<p>将<span class="math inline">\(P[S_{t+1}|S_t]\)</span>称为时间<span class="math inline">\(t\)</span>的转移概率。</p>
<p><strong>Def 3.3.2</strong></p>
<p>一个时间齐次Markov过程是一个Markov过程且<span class="math inline">\(P[S_{t+1}|S_t]\)</span>与<span class="math inline">\(t\)</span>无关。</p>
</blockquote>
<p>这意味着时间齐次Markov过程的动态可以通过下面的函数完全指定：
<span class="math display">\[
P:(\mathcal{S}-\mathcal{T})\times\mathcal{S}\rightarrow[0,1]
\]</span>
定义为<span class="math inline">\(P(s&#39;,s)=P[S_{t+1}=s&#39;|S_t=s]\)</span>使得<span class="math inline">\(\sum_{s&#39;\in S}P(s,s&#39;)=1,\text{for all}s\in\mathcal{S-T}.\)</span>​</p>
<p>注意上述规范中<span class="math inline">\(P\)</span>的参数没有时间索引<span class="math inline">\(t\)</span>（因此称为时间齐次）。此外注意到一个非时间齐次的Markov过程可以通过将所有状态和时间索引<span class="math inline">\(t\)</span>来结合转换为齐次Markov过程。这意味着如果一个非时间齐次的Markov过程的原始状态空间是<span class="math inline">\(\mathcal{S}\)</span>，那么对应的时间齐次Markov过程的状态空间是<span class="math inline">\(\mathbb{Z}_{\geq0}\times\mathcal{S}.\)</span></p>
</div>
<div id="markov过程的稳态分布" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Markov过程的稳态分布<a href="rlforfin.html#markov过程的稳态分布" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p><strong>Def 3.7.1</strong>
对于状态空间<span class="math inline">\(\mathcal{S}=\mathbb{N}\)</span>的离散、时间齐次的Markov过程及其转移概率函数<span class="math inline">\(P:\mathbb{N}\times\mathbb{N}\rightarrow [0,1]\)</span>,稳态分布是一个概率分布函数<span class="math inline">\(\pi:\mathbb{N}\rightarrow [0,1]\)</span>,满足
<span class="math display">\[
\pi(s&#39;)=\sum_{s\in\mathbb{N}}\pi(s)\cdot P(s,s&#39;),\text{ for all }s&#39;\in\mathbb{N}
\]</span></p>
</blockquote>
<p>稳态分布<span class="math inline">\(\pi\)</span>的直观理解是，在特定条件下如果我们让Markov过程无限运行，那么在长期内，状态在特定步出现频率（概率）由分布<span class="math inline">\(\pi\)</span>给出，该分布与时间步无关。</p>
<p>如果将稳态分布的定义专门化为有限状态、离散时间、时间齐次的Markov过程，状态空间为<span class="math inline">\(S=\{s_1,...,s_2\}=\mathbb{N},\)</span>那么我们可以将稳态分布<span class="math inline">\(\pi\)</span>​表示为
<span class="math display">\[
\pi(s_j)=\sum_{i=1}^n\pi(s_i)\cdot P(s_i,s_j),\text{ for al }j=1,2,...,n
\]</span>
下面使用粗体符号表示向量和矩阵。故<span class="math inline">\(\boldsymbol{\pi}\)</span>是一个长度为<span class="math inline">\(n\)</span>的列向量，<span class="math inline">\(\boldsymbol{\mathcal{P}}\)</span>是<span class="math inline">\(n\times n\)</span>的转移概率矩阵，其中行是原状态，列为目标状态，每行的和为1。那么上述定义的表述就可以简洁地表示为：
<span class="math display">\[
\boldsymbol{\pi}^T=\boldsymbol{\pi}^T\cdot\boldsymbol{\mathcal{P}},\text{ or }\boldsymbol{\mathcal{P}}^T\cdot\boldsymbol{\pi}=\boldsymbol{\pi}
\]</span>
后一个式子可以说明<span class="math inline">\(\boldsymbol{\pi}\)</span>是矩阵<span class="math inline">\(\boldsymbol{\mathcal{P}}\)</span>​的特征值为1对应的特征向量。</p>
</div>
<div id="markov奖励过程的形式主义" class="section level3 hasAnchor" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Markov奖励过程的形式主义<a href="rlforfin.html#markov奖励过程的形式主义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>我们之所以讲述Markov过程是因为希望通过为Markov过程添加增量特性来逐步进入Markov决策过程，也就是强化学习的算法框架。现在开始讲述介于二者之间的中间框架即Markov奖励过程。基本上我们只是为每次从一个状态转移到下一个状态时引入一个数值奖励的概念。这些奖励是随机的，我们需要做的就是在进行状态转移时指定这些奖励的概率分布。</p>
<p>Markov奖励过程的主要目的是计算如果让过程无限运行（期望从每个非终止状态获得的奖励总和）我们将累积多少奖励，考虑到未来的奖励需要适当地折现。</p>
<blockquote>
<p><strong>Def 3.8.1</strong></p>
<p>Markov奖励过程是一个Markov过程以及一个时间索引序列的奖励随机变量<span class="math inline">\(R_t\in\mathcal{D},\mathcal{D}\)</span>是<span class="math inline">\(\mathbb{R}\)</span>中一个可数子集，<span class="math inline">\(t=1,2,...,\)</span>满足Markov性质：
<span class="math display">\[
P[(R_{t+1},S_{t+1})|S_{t},S_{t-1},...,S_0]=P[(R_{t+1},S_{t+1})|S_t]\text{ for all }t\geq0
\]</span></p>
</blockquote>
<p>我们将<span class="math inline">\(P[(R_{t+1},S_{t+1})|S_t]\)</span>称为Markov Reward Process在时间<span class="math inline">\(t\)</span>地转移概率。由于我们通常假设Markov的时间齐次性，我们将假设MRP具有时间齐次性，即<span class="math inline">\(P[(R_{t+1},S_{t+1})|S_t]\)</span>与<span class="math inline">\(t\)</span>​无关。</p>
由时间齐次性的假设，MRP的转移概率可以表示为转移概率函数
<span class="math display">\[
\mathcal{P}_R:\mathcal{N\times D\times S}\rightarrow[0,1]
\]</span>
定义为<br />
$$
<span class="math display">\[\begin{aligned}
&amp;\mathcal{P}_R(s,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|S_t=s]\text{ for }t=0,1,2,...,\\
&amp;\text{for all }s\in\mathcal{N},r\in\mathcal{D},s&#39;\in\mathcal{S},\text{ s.t. }\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)=1,\text{ for all }s\in \mathcal{N}

\end{aligned}\]</span>
<p>$$
当涉及模拟时我们需要单独指定起始状态的概率分布。</p>
<p>现在可以扩展更多理论。给定奖励转移函数<span class="math inline">\(\mathcal{P}_R\)</span>，我们可以得到</p>
<ul>
<li><p>隐式Markov过程的概率转移函数<span class="math inline">\(P:\mathbb{N}\times S\rightarrow [0,1]\)</span>可以定义为<br />
<span class="math display">\[
\mathcal{P}(s,s&#39;)=\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)
\]</span></p></li>
<li><p>奖励转移函数<span class="math inline">\(\mathcal{R}_T:\mathcal{N\times S}\rightarrow \mathbb{R}\)</span>定义为<br />
<span class="math display">\[
\mathcal{R}_T(s,s&#39;)=\mathbb{E}[R_{t+1}|S_{t+1}=s&#39;,S_t=s]=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,r,s&#39;)}{\mathcal{P}(s,s&#39;)}=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,r,s&#39;)}{\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)}\cdot r
\]</span></p></li>
</ul>
<p>我们在实践中遇到的大多数MRP奖励规范可以直接表示为奖励转移函数<span class="math inline">\(\mathcal{R}_T\)</span>.最后我们想强调的是，可以将<span class="math inline">\(\mathcal{P}_R\)</span>或<span class="math inline">\(\mathcal{R}_T\)</span>转换为一种更紧凑的奖励函数。该函数足以执行涉及MRP的关键计算，这个奖励函数<span class="math inline">\(\mathcal{R}:\mathcal{N}\rightarrow \mathbb{R}\)</span>定义为<br />
<span class="math display">\[
\mathcal{R}(s)=\mathbb{E}[R_{t+1}|S_t=s]=\sum_{s&#39;\in\mathcal{S}}\mathcal{P}(s,s&#39;)\cdot\mathcal{R}_T(s,s&#39;)=\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)\cdot r
\]</span></p>
</div>
<div id="markov奖励过程的价值函数" class="section level3 hasAnchor" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Markov奖励过程的价值函数<a href="rlforfin.html#markov奖励过程的价值函数" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在，我们准备正式定义涉及MRP的主要问题，我们希望计算从任何非终止状态出发的“期望累积奖励”。允许在奖励累积时使用贴现因子<span class="math inline">\(\gamma\in[0,1]\)</span>,我们将回报<span class="math inline">\(G_t\)</span>定义为时间<span class="math inline">\(t\)</span>之后的“未来奖励的贴现累积”。形式上：<br />
<span class="math display">\[
G_t=\sum_{i=t+1}^\infty \gamma^{i-t-1}\cdot R_i=R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2\cdot R_{t+3}+....
\]</span>
即使对于终止序列（例如<span class="math inline">\(t=T\)</span>时终止，即<span class="math inline">\(S_T\in\mathcal{T}\)</span>）我们只需将<span class="math inline">\(i&gt;T\)</span>的<span class="math inline">\(R_i=0\)</span>.<br />
我们希望识别具有较大期望回报的非终止状态和具有较小期望回报的非终止状态。事实上，这是涉及MRP的主要问题——计算MRP中每个非终止状态的期望回报。形式上，我们感兴趣的是计算价值函数：<br />
<span class="math display">\[
V:\mathcal{N}\rightarrow\mathbb{R}
\]</span>
定义为<br />
<span class="math display">\[
V(s)=\mathbb{E}[G_t|S_t=s]\text{ for all }s\in\mathcal{N},\text{ for all }t=0,1,2,...
\]</span>
贝尔曼指出价值函数具有递归结构，具体来说</p>
<p><span class="math display" id="eq:bellman">\[\begin{align}
V(s)=&amp;\mathbb{E}[R_{t+1}|S_t=s]+\gamma\cdot\mathbb{E}[R_{t+2}|S_t=s]+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\cdot\mathbb{E}[R_{t+2}|S_{t+1}=s&#39;]\\
&amp;+\gamma^2\sum_{s&#39;\in\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\sum_{s&#39;&#39;\in\mathcal{N}}P[S_{t+2}=s&#39;&#39;|S_{t+1}=s&#39;]\cdot\mathbb{E}[R_{t+3}|S_{t+2}=s&#39;&#39;]\\
&amp;+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot\mathcal{R}(s&#39;)+\gamma^2\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,s&#39;&#39;)\mathcal{R}(s&#39;&#39;)+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot(R(s&#39;)+\gamma\cdot\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,s&#39;&#39;)\cdot\mathcal{R}(s&#39;&#39;)+...)\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot V(s&#39;) \text{ for all }s\in\mathcal{N} \tag{1.1}
\end{align}\]</span></p>
<p><a href="mailto:我们将这个价值函数的递归方程@ref" class="email">我们将这个价值函数的递归方程@ref</a>(eq:bellman)称为<strong>Markov奖励过程的Bellman方程</strong>。</p>
</div>
</div>
<div id="MDP" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Markov决策过程<a href="rlforfin.html#MDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="不确定性下的序列决策难题" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> 不确定性下的序列决策难题<a href="rlforfin.html#不确定性下的序列决策难题" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>通常，MDP具有两个截然不同且相互依赖的高级特征<br />
1. 在每个时间步<span class="math inline">\(t,\)</span>观察到状态<span class="math inline">\(S_t\)</span>后，从指定的动作集合中选择一个动作<span class="math inline">\(A_t.\)</span>
2. 给定观察到的状态<span class="math inline">\(S_t\)</span>和执行的动作<span class="math inline">\(A_t,\)</span>下一个时间步的状态<span class="math inline">\(S_{t+1}\)</span>和奖励<span class="math inline">\(R_{t+1}\)</span>的概率通常不仅取决于状态<span class="math inline">\(S_t\)</span>还取决于动作<span class="math inline">\(A_t.\)</span><br />
我们的任务是最大化每个状态的期望回报（即最大化价值函数）。在一般情况下，这似乎是一个非常困难的问题，因为存在循环的相互作用。一方面，动作依赖于状态；另一方面，下一个状态/奖励的概率依赖于动作和状态。此外，动作可能会对奖励产生延迟影响，如何区分不同时间步的动作对未来奖励的影响也是一个挑战。如果没有动作和奖励之间的直接对应关系，我们如何控制动作以最大化期望累积奖励？为了回答这个问题，我们需要建立一些符号和理论。在我们正式定义马尔可夫决策过程框架及其相关（优雅的）理论之前，让我们先设定一些术语。</p>
<p><strong>人工智能视角下的MDP</strong><br />
使用人工智能的语言，我们说在每个时间步<span class="math inline">\(t,\)</span><strong>智能体（Agent）</strong>（我们设计的算法）观察到状态<span class="math inline">\(S_t,\)</span>然后智能体执行动作<span class="math inline">\(A_t,\)</span>之后环境（Environment）（在看到<span class="math inline">\(S_t,A_t\)</span>后）生成一个随机对<span class="math inline">\((S_{t+1},R_{t+1})\)</span>。接着，智能体观察到下一个状态<span class="math inline">\(S_{t+1}\)</span>，循环重复直到达到终止状态。这种循环相互作用如图<a href="rlforfin.html#fig:mdp">1.1</a>所示</p>
<details class=chunk-details><summary class=chunk-summary><span class=chunk-summary-text>Code</span></summary>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="rlforfin.html#cb6-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;https://Go9entle.github.io/picx-images-hosting/image.3yekzoaheh.webp&quot;</span>)</span></code></pre></div>
</details>
<div class="figure"><span style="display:block;" id="fig:mdp"></span>
<img src="https://Go9entle.github.io/picx-images-hosting/image.3yekzoaheh.webp" alt="Markov Decision Process"  />
<p class="caption">
图1.1: Markov Decision Process
</p>
</div>
</div>
<div id="markov决策过程的正式定义" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Markov决策过程的正式定义<a href="rlforfin.html#markov决策过程的正式定义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>与Markov过程和Markov奖励过程的定义类似，为了便于阐述，以下Markov决策过程的定义和理论将针对离散时间、可数状态空间和可数的下一个状态与奖励转移对。</p>
<p><strong>定理</strong></p>
<p>Markov决策过程包括以下内容：</p>
<ul>
<li><p>可数状态集合<span class="math inline">\(\mathcal{S}\)</span>（称为状态空间），终止状态集合<span class="math inline">\(\mathcal{T}\subset \mathcal{S}\)</span>,以及可数动作集合<span class="math inline">\(\mathcal{A}\)</span>（称为动作空间）。</p></li>
<li><p>时间索引的环境生成随机状态序列<span class="math inline">\(S_t\in\mathcal{S}\)</span>（时间步<span class="math inline">\(t=0,1,2,...\)</span>），时间索引的环境生成奖励随机变量序列<span class="math inline">\(R_t\in\mathcal{D}\)</span>（<span class="math inline">\(\mathcal{D}\)</span>是<span class="math inline">\(\mathbb{R}\)</span>的可数子集），以及时间索引的智能体可控动作序列<span class="math inline">\(A_t\in\mathcal{A}\)</span></p></li>
<li><p>Markov性<br />
<span class="math display">\[\begin{equation*}
P[(R_{t+1},S_{t+1})|(S_t,A_t,S_{t-1},A_{t-1},...,S_0,A_0)]=P[   (R_{t+1},S_{t+1})|(S_t,A_t)]\text{ for all }t\geq 0
\end{equation*}\]</span></p></li>
<li><p>终止：如果某个时间步<span class="math inline">\(T\)</span>的状态<span class="math inline">\(S_T\in\mathcal{T}\)</span>,则该序列结果在时间步<span class="math inline">\(T\)</span>终止。</p></li>
</ul>
<p>在更一般的情况下，如果状态或奖励是不可数的，相同的概念仍然适用，只是数学形式需要更加详细和谨慎。具体来说，我们将使用积分代替求和，使用概率密度函数（用于连续概率分布）代替概率质量函数（用于离散概率分布）。为了符号的简洁性，更重要的是为了核心概念的理解（而不被繁重的数学形式分散注意力），我们选择默认使用离散时间、可数<span class="math inline">\(\mathcal{S}\)</span>、可数<span class="math inline">\(\mathcal{A}\)</span>和可数<span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>我们将<span class="math inline">\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\)</span>称为Markov决策过程在时间<span class="math inline">\(t\)</span>的转移概率。</p>
<p>与Markov过程和MRP一样，我们默认Markov决策过程是时间其次的，即<span class="math inline">\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\)</span>与<span class="math inline">\(t\)</span>无关。这意味着Markov决策过程的转移概率在最一般情况下可以表示为<strong>状态-奖励转移概率函数</strong>：</p>
<p><span class="math display">\[
\mathcal{P}_R:\mathcal{N\times A\times D\times S}\rightarrow [0,1]
\]</span>
定义为</p>
<p><span class="math display">\[
\mathcal{P}_R(s,a,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|(S_t=s,A_t=a)]
\]</span>
对于时间步<span class="math inline">\(t=0,1,2,...\)</span>，对于所有的<span class="math inline">\(s,s&#39;\in\mathcal{N},a\in\mathcal{A},r\in\mathcal{D}\)</span>满足</p>
<p><span class="math display">\[
\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)=1\text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
这又可以通过状态-奖励转移概率函数<span class="math inline">\(\mathcal{P}_R\)</span>来表征，给定<span class="math inline">\(\mathcal{P}_R\)</span>的规范，我们可以构造</p>
<ul>
<li><p>状态转移概率函数：<br />
<span class="math display">\[
  \mathcal{P}:\mathcal{N\times A\times S}\rightarrow [0,1]
\]</span>
定义为<br />
<span class="math display">\[
\mathcal{P}(s,a,s&#39;)=\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)
\]</span></p></li>
<li><p>奖励转移函数：<br />
<span class="math display">\[
\mathcal{R}_T:\mathcal{N\times A\times S}\rightarrow \mathbb{R}
\]</span>
定义为</p>
<p><span class="math display">\[\begin{align}
\mathcal{R}_T(s,a,s&#39;)&amp;=\mathbb{E}[R_{t+1}|(S_{t+1}=s&#39;,S_t=s,A_t=a)]\\
&amp;=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,a,r,s&#39;)}{\mathcal{P}(s,a,s&#39;)}\cdot r\\
&amp;=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,a,r,s&#39;)}{\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)}\cdot r

\end{align}\]</span></p></li>
</ul>
<p>在实践中，我们遇到的大多数Markov决策过程的奖励规范可以直接表示为奖励转移函数<span class="math inline">\(\mathcal{R}_T\)</span>而不是更一般的<span class="math inline">\(\mathcal{P}_R\)</span>.最后我们想强调的是可以将<span class="math inline">\(\mathcal{P}_R\)</span>或<span class="math inline">\(\mathcal{R}_T\)</span>转换为“更紧凑”的奖励函数，该函数足以执行设计MDP的关键计算，这个奖励函数为：</p>
<p><span class="math display">\[
\mathcal{R}:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为：</p>
<span class="math display">\[\begin{aligned}
\mathcal{R}(s,a)&amp;=\mathbb{E}[R_{t+1}|(S_t=s,A_t=a)]\\
&amp;=\sum_{s\in\mathcal{S}}\mathcal{P}(s,a,s&#39;)\cdot\mathcal{R}_T(s,a,s&#39;)\\
&amp;=\sum_{s\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)\cdot r
\end{aligned}\]</span>
</div>
<div id="策略" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> 策略<a href="rlforfin.html#策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>理解了MDP的动态后，我们现在转向智能体动作的规范，即作为当前状态函数的动作选择。在一般情况下，我们假设智能体将根据当i请安状态<span class="math inline">\(S_t\)</span>的概率分布执行动作<span class="math inline">\(A_t\)</span>,我们将此函数称为策略（Policy）。<br />
形式上，策略是一个函数：</p>
<p><span class="math display">\[
\pi:\mathcal{N\times A}\rightarrow [0,1]
\]</span>
定义为:</p>
<p><span class="math display">\[
\pi(s,a)=P[A_t=a|S_t=s]\text{ for }t=0,1,2,...\text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
使得</p>
<p><span class="math display">\[
\sum_{a\in\mathcal{A}}\pi(s,a)=1\text{ for all }s\in\mathcal{N}
\]</span></p>
<p>需要注意的是，上述定义假设策略是Markov的，即动作概率仅依赖于当前状态，而不依赖于历史状态。上述定义还假设策略是平稳的，即<span class="math inline">\(P[A_t=a|S_t=s]\)</span>在时间<span class="math inline">\(t\)</span>上是不变的。如果我们遇到策略需要依赖于时间<span class="math inline">\(t\)</span>的情况，我们可以简单地将<span class="math inline">\(t\)</span>包含在状态中，从而使策略变得平稳（尽管这会增加状态空间的规模，从而导致计算成本的增加）。<br />
当策略对每个状态的动作概率集中在单个动作上（即只要到达一个状态，动作是确定的）时，我们称之为<strong>确定性策略</strong>。形式上，确定性策略<span class="math inline">\(\pi_D:\mathcal{N}\rightarrow \mathcal{A}\)</span>具有以下性质：对所有的<span class="math inline">\(s\in\mathcal{N},\)</span></p>
<p><span class="math display">\[
\pi(s,\pi_D(s))=1\text{ and }\pi(s,a)=0, \text{ for all }a\ne \pi_D(s)
\]</span></p>
<p>我们将非确定性的策略称为<strong>随机策略</strong>（随机反映了智能体将根据<span class="math inline">\(\pi\)</span>指定的概率分布执行随机动作的事实）。</p>
</div>
<div id="markov决策过程策略-markov奖励过程" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> [Markov决策过程，策略]:= Markov奖励过程<a href="rlforfin.html#markov决策过程策略-markov奖励过程" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>本节有一个重要的见解——如果我们用固定策略<span class="math inline">\(\pi\)</span>（通常是一个固定的随机策略，注意与确定性策略区分）评估MDP，我们会得到一个由MDP和策略<span class="math inline">\(\pi\)</span>共同隐含的MRP。我们可以用符号精确地澄清这一点，但首先MDP和MRP中存在一些符号冲突。我们使用</p>
<ul>
<li><p><span class="math inline">\(\mathcal{P}_R\)</span>表示MRP转移概率函数，同时也表示MDP的状态-奖励转移概率函数；</p></li>
<li><p><span class="math inline">\(\mathcal{P}\)</span>表示MRP中隐含的Markov过程的转移概率函数，同时也表示MDP的状态转移函数；</p></li>
<li><p><span class="math inline">\(\mathcal{R}_T\)</span>表示MRP的奖励转移函数，同时也表示MDP的奖励转移函数；</p></li>
<li><p><span class="math inline">\(\mathcal{R}\)</span>表示MRP的奖励函数，同时也表示MDP的奖励函数。</p></li>
</ul>
<p>我们将在<span class="math inline">\(\pi\)</span>隐含的MRP的函数<span class="math inline">\(\mathcal{P}_R,\mathcal{P},\mathcal{R}_T,\mathcal{R}\)</span>加上上标<span class="math inline">\(\pi\)</span>以区分这些函数MDP和<span class="math inline">\(\pi\)</span>隐含的MRP中的使用。</p>
<p>假设我们给定一个固定策略<span class="math inline">\(\pi\)</span>和一个由其状态-奖励转移概率函数<span class="math inline">\(\mathcal{P}_R\)</span>指定的MDP，那么由MDP与策略<span class="math inline">\(\pi\)</span>评估隐含的MRP的转移概率函数<span class="math inline">\(\mathcal{P}_R^\pi\)</span>定义为：</p>
<p><span class="math display">\[
\mathcal{P}_R^\pi(s,r,s&#39;)=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{P}_R(s,a,r,s&#39;)
\]</span>
类似地有</p>
<p><span class="math display">\[\begin{align}
\mathcal{P}^\pi(s,s&#39;)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{P}(s,a,s&#39;)\\
\mathcal{R}_T^\pi(s,s&#39;)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}_T(s,a,s&#39;)\\
\mathcal{R}^\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)
\end{align}\]</span></p>
<p>因此，每当我们谈论用固定策略评估的 MDP时，你应该知道我们实际上是在谈论隐含的 MRP。</p>
</div>
<div id="固定策略下的mdp价值函数" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> 固定策略下的MDP价值函数<a href="rlforfin.html#固定策略下的mdp价值函数" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在我们准备讨论用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数（也称为MDP预测问题，“预测”指的是该问题涉及在智能体遵循特定策略时预测未来期望回报）。与MRP情况类似，我们定义</p>
<p><span class="math display">\[
G_t=\sum_{i=t+1}^\infty \gamma^{i-t-1}\cdot R_i=R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2\cdot R_{t+3}+....
\]</span>
其中<span class="math inline">\(\gamma\in[0,1]\)</span>是指定的贴现因子。即便对于终止序列我们也使用上述回报的定义。<br />
用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数为</p>
<p><span class="math display">\[
V^\pi:\mathcal{N}\rightarrow \mathbb{R}
\]</span>
定义为：</p>
<p><span class="math display">\[
V^\pi(s)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|S_t=s]\text{ for all }s\in\mathcal{N},\text{ for all }t=0,1,2,...
\]</span>
我们假设每当我们讨论价值函数时，折扣因子<span class="math inline">\(\gamma\)</span>是适当的，以确保每个状态的期望回报是有限的——特别是对于可能发散的连续（非终止）MDP，<span class="math inline">\(\gamma&lt;1\)</span>.<br />
我们将<span class="math inline">\(V^\pi(s)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|S_t=s]\)</span>展开如下：</p>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+1}|S_t=s]+\gamma\cdot\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+2}|S_t=s]+\gamma^2\cdot\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+3}|S_t=s]+...\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)+\gamma\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\cdot\mathcal{R}(s&#39;,a&#39;)\\
&amp;+\gamma^2\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a&#39;,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,a&#39;&#39;,s&#39;&#39;)\sum_{a&#39;&#39;\in\mathcal{A}}\pi(s&#39;&#39;,a&#39;&#39;)\cdot\mathcal{R}(s&#39;&#39;,a&#39;&#39;)\\
&amp;+...\\
=&amp; \mathcal{R}^\pi(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\cdot\mathcal{R}^\pi(s&#39;)+\gamma^2\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}^\pi(s&#39;,s&#39;&#39;)\cdot\mathcal{R}^\pi(s&#39;&#39;)+...
\end{align}\]</span></p>
<p>最后一个表达式等于<span class="math inline">\(\pi\)</span>隐含的MRP的状态<span class="math inline">\(s\)</span>的价值函数。因此，用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数<span class="math inline">\(V^\pi\)</span>与<span class="math inline">\(\pi\)</span>隐含的MRP的价值函数完全相同，因此我们可以将MRP的Bellman方程应用于<span class="math inline">\(V^\pi\)</span>，即</p>
<p><span class="math display" id="eq:4-1">\[\begin{align}
V^\pi(s)=&amp;\mathcal{R}^\pi(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\cdot V^\pi(s&#39;)\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)+\gamma\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;)\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot(\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;))\text{ for all }s\in\mathcal{N} \tag{1.2}
\end{align}\]</span></p>
<p>对于状态空间不太大的有限MDP，方程 <a href="rlforfin.html#eq:4-1">(1.2)</a> 可以通过线性代数求解<span class="math inline">\(V^\pi\)</span>。更一般地，方程 <a href="rlforfin.html#eq:4-1">(1.2)</a> 将成为本书其余部分开发各种动态规划和强化学习算法以解决MDP预测问题的关键方程。<br />
然而，另一个价值函数在开发MDP算法时也至关重要——它将（状态、动作）对映射到从该（状态，动作）对出发的期望回报，当用固定策略评估时。这被称为用固定策略评估的MDP地动作—价值函数：</p>
<p><span class="math display">\[
Q^\pi:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为：</p>
<p><span class="math display">\[
Q^\pi(s,a)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|(S_t=s,A_t=a)]\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \text{ for all }t=0,1,2,...
\]</span></p>
<p>为了避免术语混淆，我们将<span class="math inline">\(V^\pi\)</span>称为策略<span class="math inline">\(\pi\)</span>的<strong>状态-价值函数</strong>（尽管通常简称为价值函数），以区别于<strong>动作-价值函数</strong><span class="math inline">\(Q^\pi\)</span>.解释<span class="math inline">\(Q^\pi(s,a)\)</span>的方式是，它是从给定非终止状态<span class="math inline">\(s\)</span>出发，首先采取动作<span class="math inline">\(a\)</span>，然后遵循策略<span class="math inline">\(\pi\)</span>的期望回报。通过这种解释，我们可以将<span class="math inline">\(V^\pi(s)\)</span>视作<span class="math inline">\(Q^\pi(s,a)\)</span>的“加权平均”（对于所有从非终止状态<span class="math inline">\(s\)</span>出发的所有可能动作<span class="math inline">\(a\)</span>）,权重等于给定状态<span class="math inline">\(s\)</span>的动作<span class="math inline">\(a\)</span>的概率（即<span class="math inline">\(\pi(s,a)\)</span>）。具体来说：</p>
<p><span class="math display" id="eq:4-2">\[\begin{equation}
V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot Q^\pi(s,a)\text{ for all }s\in\mathcal{N} \tag{1.3}
\end{equation}\]</span></p>
<p>将<span class="math inline">\(Q^\pi(s,a)\)</span>展开后得到</p>
<p><span class="math display" id="eq:4-3">\[\begin{equation}
Q^\pi(s,a)=\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;) \text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.4}
\end{equation}\]</span></p>
<p>结合方程<a href="rlforfin.html#eq:4-2">(1.3)</a>和方程<a href="rlforfin.html#eq:4-3">(1.4)</a>我们得到</p>
<p><span class="math display" id="eq:4-4">\[\begin{equation}
Q^\pi(s,a)=\mathcal{R}(s,a)+\gamma \cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\cdot Q^\pi(s&#39;,a&#39;)\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.5}
\end{equation}\]</span></p>
<p>方程<a href="rlforfin.html#eq:4-1">(1.2)</a>被称为<strong>MDP状态-价值函数贝尔曼策略方程</strong>，方程<a href="rlforfin.html#eq:4-4">(1.5)</a>被称为<strong>MDP动作-价值函数贝尔曼策略方程</strong>。方程<a href="rlforfin.html#eq:4-1">(1.2)</a>、<a href="rlforfin.html#eq:4-2">(1.3)</a>、<a href="rlforfin.html#eq:4-3">(1.4)</a>和<a href="rlforfin.html#eq:4-4">(1.5)</a>统称为<strong>MDP贝尔曼策略方程</strong>。</p>
</div>
<div id="最优价值函数和最优策略" class="section level3 hasAnchor" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> 最优价值函数和最优策略<a href="rlforfin.html#最优价值函数和最优策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>最后，我们要达到Markov决策问题的主要目的——识别能够产生最有价值函数的策略，即从每个非终止状态出发的最佳可能期望回报。我们说，当我们识别出MDP的最优价值函数（及相关的最优策略，即产生最优价值函数的策略）时，MDP就被“解决”了。识别最优价值函数及其相关最优策略被称为<strong>MDP控制问题</strong>。“控制”指的是该问题涉及通过策略的迭代修改来引导动作，以推动价值函数向最优性发展。<br />
形式上，最优价值函数定义为</p>
<p><span class="math display">\[
V^*:\mathcal{N}\rightarrow \mathbb{R}
\]</span>
定义为</p>
<p><span class="math display">\[
V^*(s)=\max_{\pi\in\Pi} V^\pi(s) \text{ for all }s\in\mathcal{N}
\]</span>
其中<span class="math inline">\(\Pi\)</span>是<span class="math inline">\(\mathcal{N,A}\)</span>空间上的平稳随机策略集合。<br />
上述定义的解释是，对于每个非终止状态<span class="math inline">\(s\)</span>，我们考虑所有可能的随机平稳策略并在这些<span class="math inline">\(\pi\)</span>中选择最大化<span class="math inline">\(V^\pi(s).\)</span>需要注意的是，<span class="math inline">\(\pi\)</span>的选择是针对每个<span class="math inline">\(s\)</span>单独进行的，因此可以想象，不同的<span class="math inline">\(\pi\)</span>的选择可能会为不同的<span class="math inline">\(s\in\mathcal{N}\)</span>最大化<span class="math inline">\(V^\pi(s).\)</span>因此，从上述<span class="math inline">\(V^*\)</span>的定义中，我们还不能谈论“最优策略”的概念。因此，现在让我们只关注上述定义的最有价值函数。<br />
同样，最优动作-价值函数定义为</p>
<p><span class="math display">\[
Q^*:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为</p>
<p><span class="math display">\[
Q^*(s,a)=\max_{\pi\in\Pi} Q^\pi(s,a) \text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
<span class="math inline">\(V^*\)</span>通常被称为最优状态-价值函数，以区别于最优动作-价值函数<span class="math inline">\(Q^*\)</span>（尽管为了简洁，<span class="math inline">\(V^*\)</span>通常也被简称为最优价值函数）。需要明确的是，最优价值函数默认情况下值得就是最优状态-价值函数<span class="math inline">\(V^*\)</span>.</p>
<p>正如固定策略的价值函数具有递归公式一样，贝尔曼指出我们可以为最优价值函数创建递归公式。让我们从展开给定非终止状态<span class="math inline">\(s\)</span>的最优状态-价值函数<span class="math inline">\(V^*(s)\)</span>开始——我们考虑从状态<span class="math inline">\(s\)</span>出发可以采取的所有可能动作<span class="math inline">\(a\in\mathcal{A},\)</span>并选择能够产生最佳动作-价值的动作<span class="math inline">\(a,\)</span>即选择出能够产生最优<span class="math inline">\(Q^*(s,a)\)</span>的动作<span class="math inline">\(a\)</span>。形式上给出了以下方程</p>
<p><span class="math display" id="eq:4-5">\[
V^*(s)=\max_{a\in\mathcal{A}}Q^*(s,a)\text{ for all }s\in\mathcal{N} \tag{1.6}
\]</span>
同样让我们思考从给定非终止状态和动作对<span class="math inline">\((s,a)\)</span>出发的最优性意味着什么，也就是展开<span class="math inline">\(Q^*(s,a)\)</span>.首先我们获得即时的期望奖励<span class="math inline">\(\mathcal{R}(s,a)\)</span>.接下来，考虑所有可能的我们可以转到的状态<span class="math inline">\(s&#39;\in\mathcal{S}\)</span>并从每个非终止状态<span class="math inline">\(s&#39;\)</span>出发递归地采取最优动作。形式上，这给出了以下方程：</p>
<p><span class="math display" id="eq:4-6">\[
Q^*(s,a)=\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^*(s&#39;)\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.7}
\]</span>
将<a href="rlforfin.html#eq:4-6">(1.7)</a>中的<span class="math inline">\(Q^*(s,a)\)</span>代入<a href="rlforfin.html#eq:4-5">(1.6)</a>,可以得到</p>
<p><span class="math display" id="eq:4-7">\[
V^*(s)=\max_{a\in\mathcal{A}}\{ \mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^*(s&#39;) \} \text{ for all }s\in\mathcal{N} \tag{1.8}
\]</span>
方程<a href="rlforfin.html#eq:4-7">(1.8)</a>被称为<strong>MDP状态-价值函数贝尔曼最优性方程</strong>。<br />
将方程<a href="rlforfin.html#eq:4-5">(1.6)</a>代入方程<a href="rlforfin.html#eq:4-6">(1.7)</a>可以得到</p>
<p><span class="math display" id="eq:4-8">\[
Q^*(s,a)=\mathcal{R}(s,a)=\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot \max_{a&#39;\in\mathcal{A}} Q^*(s&#39;,a&#39;)\text{ for all }s\in\mathcal{A},a\in\mathcal{A} \tag{1.9}
\]</span>
方程<a href="rlforfin.html#eq:4-8">(1.9)</a>被称为<strong>MDP动作-价值函数贝尔曼最优性方程</strong>。<br />
方程<a href="rlforfin.html#eq:4-7">(1.8)</a>、<a href="rlforfin.html#eq:4-5">(1.6)</a>、<a href="rlforfin.html#eq:4-6">(1.7)</a>和<a href="rlforfin.html#eq:4-8">(1.9)</a>统称为<strong>MDP贝尔曼最优性方程</strong>。我们应该强调，当有人说 MDP贝尔曼方程或简称为贝尔曼方程时，除非他们明确说明，否则他们指的是 MDP 贝尔曼最优性方程（通常是 MDP 状态-价值函数贝尔曼最优性方程）。这是因为 MDP 贝尔曼最优性方程解决了马尔可夫决策过程的最终目的——识别最优价值函数和实现最优价值函数的相关策略（即使我们能够解决 MDP 控制问题）。<br />
我们需要强调的是，贝尔曼最优性方程并没有直接给出计算最优质函数或实现最优质函数的策略的具体方法——它们只是阐述了最优值函数的一个强大数学性质，这一性质帮助我们提出动态规划或者强化学习的算法来计算最优值函数及其相关的策略。</p>
<p>我们一直在使用“实现最优值函数的策略/策略组合”这个词，但我们还没有给出这样的策略的明确定义。事实上，正如之前提到的，从<span class="math inline">\(V^*\)</span>的定义来看并不清楚是否存在这样的策略能实现<span class="math inline">\(V^*\)</span>（因为可以设想不同的策略<span class="math inline">\(\pi\)</span>对于不同的状态<span class="math inline">\(s\in\mathcal{N}\)</span>实现<span class="math inline">\(V^\pi(s)\)</span>最大化）。因此我们定义最优策略<span class="math inline">\(\pi^*:\mathcal{N\times A}\rightarrow [0,1]\)</span>主导所有其他策略的策略，在价值函数上优于所有其他策略。形式化地说</p>
<p><span class="math display">\[
\pi^*\in\Pi\text{ is an Optimal Policy if } V^{\pi^*}(s)\geq V^\pi(s)\text{ for all }\pi\in\Pi \text{ and for all states }s\in\mathcal{N}.
\]</span></p>
<p>最优策略<span class="math inline">\(\pi^*\)</span>的定义表明，它是一个“优于或等于”所有其他静态策略的策略，且适用于所有非终止状态（注意可能存在多个最优策略）。将这个定义与最优值函数<span class="math inline">\(V^*\)</span>的定义结合，接下来的自然问题是：是否存在一个最优策略<span class="math inline">\(\pi^*\)</span>,对所有的<span class="math inline">\(s\in\mathcal{N}\)</span>最大化<span class="math inline">\(V^\pi(s)\)</span>,也就是是否存在一个<span class="math inline">\(\pi^*\)</span>使得<span class="math inline">\(V^*(s)=V^{\pi^*}(s)\)</span>对所有<span class="math inline">\(s\in\mathcal{N}.\)</span>下面的定理和证明是针对我们默认的MDP设置（离散时间、可数空间、时间齐次）的。</p>
<p><strong>定理</strong></p>
<p>对于任何（离散时间、可数空间、时间齐次）的MDP：</p>
<ul>
<li><p>存在一个最优策略<span class="math inline">\(\pi^*\in\Pi\)</span>.</p></li>
<li><p>所有最优策略都实现最优值函数。</p></li>
<li><p>所有最优策略实现最优动作-价值函数，即对于所有的<span class="math inline">\(s\in\mathcal{N},a\in\mathcal{A},Q^{\pi^*}(s,a)=Q^*(s,a)\)</span>对于所有的最优策略<span class="math inline">\(\pi^*\)</span>.</p></li>
</ul>
<p>定理的证明略去。</p>
<p>我们的确定性如下定义</p>
<p><span class="math display" id="eq:4-9">\[
\pi^*_D(s)=\mathop{\arg\max}_{a\in\mathcal{A}} Q^*(s,a)\text{ for all }s\in\mathcal{N} \tag{1.10}
\]</span></p>
<p>方程<a href="rlforfin.html#eq:4-9">(1.10)</a>是一个关键构造，它与贝尔曼最优性方程紧密结合，在涉及各种动态规划和强化学习算法以解决MDP控制问题（即求解<span class="math inline">\(V^*,Q^*,\pi^*\)</span>）时起到重要作用。最后，值得注意的是，不像预测问题在小状态空间中有一个直接的线性代数求解器，控制问题是非线性的，因此没有类似的直接线性代数求解器。控制问题的最简单解法（即使是在小状态空间中）就是我们将在下一章中讨论的动态规划算法。</p>
</div>
</div>
<div id="dynamicprogram" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> 动态规划算法<a href="rlforfin.html#dynamicprogram" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="planning-vs-learning" class="section level3 hasAnchor" number="1.3.1">
<h3><span class="header-section-number">1.3.1</span> Planning vs Learning<a href="rlforfin.html#planning-vs-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>在本书中，我们将从人工智能的角度探讨预测与控制（并且我们将特别使用人工智能的术语）。我们将区分没有马尔可夫决策过程（MDP）环境模型的算法（没有访问概率转移（<span class="math inline">\(\mathcal{P}_R\)</span>）函数）与有马尔可夫决策过程环境模型的算法（意味着我们可以通过显式的概率分布表示或仅通过采样模型访问<span class="math inline">\(\mathcal{P}_R\)</span>）。前者（没有模型访问的算法）被称为学习算法，以反映人工智能代理需要与真实世界环境互动（例如，一个机器人学习在实际森林中导航）并从其通过与环境互动获得的数据（遇到的状态、采取的行动、观察到的奖励）中学习价值函数的事实。后者（有MDP环境模型的算法）被称为规划算法，以反映人工智能代理不需要与真实世界环境互动，实际上是通过模型预测各种行动选择的未来状态/奖励的概率场景，并基于预测的结果求解所需的价值函数。在学习和规划中，贝尔曼方程是驱动算法的基本概念，但算法的细节通常会使它们看起来相当不同。本章将仅关注<strong>规划算法</strong>，实际上将只关注一种称为<em>动态规划</em>的规划算法子类。</p>
</div>
<div id="不动点理论" class="section level3 hasAnchor" number="1.3.2">
<h3><span class="header-section-number">1.3.2</span> 不动点理论<a href="rlforfin.html#不动点理论" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>定义</strong><br />
一个函数<span class="math inline">\(f:\mathcal{X\rightarrow X}\)</span>的不动点是指一个<span class="math inline">\(x\in\mathcal{X},\)</span>使得满足方程<span class="math inline">\(x=f(x).\)</span></p>
</div>
<div id="贝尔曼策略算子以及策略评估算法" class="section level3 hasAnchor" number="1.3.3">
<h3><span class="header-section-number">1.3.3</span> 贝尔曼策略算子以及策略评估算法<a href="rlforfin.html#贝尔曼策略算子以及策略评估算法" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>本节介绍第一个动态规划算法即<em>策略评估算法</em>。策略评估算法解决了在固定策略<span class="math inline">\(\pi\)</span>下计算有限Markov决策过程（MDP）的价值函数的问题（即有限MDP的预测问题）。我们知道这等价于计算<span class="math inline">\(\pi\)</span>隐式的有限Markov奖励过程（MRP）的价值函数。为了避免符号混淆，注意对符号的<span class="math inline">\(\pi\)</span>上标表示它是指<span class="math inline">\(\pi\)</span>隐式MRP的符号。预测问题的具体定义如下</p>
<p>设MDP（以及<span class="math inline">\(\pi\)</span>隐式的MRP）的状态集为<span class="math inline">\(\mathcal{S}=\{s_1,...,s_n\}\)</span>,不失一般性，设<span class="math inline">\(\mathcal{N}=(s_1,...,s_m)\)</span>为非终止状态。我们给定一个固定策略<span class="math inline">\(\pi:\mathcal{N\times A}\rightarrow [0,1]\)</span>.我们还给定<span class="math inline">\(\pi\)</span>隐式的MRP转移概率函数：</p>
<p><span class="math display">\[
\mathcal{P}_R^\pi:\mathcal{N\times D\times S}\rightarrow [0,1]
\]</span>
该函数以数据结构的形式提供（因为状态是有限的，且每个非终止状的下一状态和奖励转移对也是有限的）。预测问题就是计算在固定策略<span class="math inline">\(\pi\)</span>下评估的MDP的价值函数（等价于<span class="math inline">\(\pi\)</span>隐式MRP的价值函数），我们用<span class="math inline">\(V^\pi:\mathcal{N}\rightarrow \mathbb{R}\)</span>来表示。</p>
<p>根据前面的内容，通过从<span class="math inline">\(\mathcal{P}_R^\pi\)</span>中提取隐式Markov过程的转移概率函数<span class="math inline">\(\mathcal{P}^\pi:\mathcal{N\times S}\rightarrow [0,1]\)</span>和奖励函数<span class="math inline">\(\mathcal{R}^\pi:\mathcal{N}\rightarrow \mathbb{R},\)</span>我们可以对价值函数<span class="math inline">\(V^\pi:\mathcal{N}\rightarrow \mathbb{R}\)</span>(表示为列向量<span class="math inline">\(\boldsymbol{V}^\pi\in\mathbb{R}^m\)</span>)执行以下计算来求解这个预测问题：</p>
<p><span class="math display">\[
\boldsymbol{V}^\pi=(\boldsymbol{I}_m-\gamma \boldsymbol{\mathcal{P}}^\pi)^{-1}\cdot \boldsymbol{\mathcal{R}}^\pi
\]</span>
其中<span class="math inline">\(\boldsymbol{I}_m\)</span>是<span class="math inline">\(m\)</span>阶的单位矩阵，列向量<span class="math inline">\(\boldsymbol{\mathcal{R}}^\pi\in\mathbb{R}^m\)</span>表示<span class="math inline">\(\mathcal{R}^\pi,\boldsymbol{\mathcal{P}}^\pi\)</span>是一个<span class="math inline">\(m\)</span>阶的矩阵代表<span class="math inline">\(\mathcal{P}^\pi\)</span>（其中的行和列对应非终止状态）。然而当<span class="math inline">\(m\)</span>很大时这种计算方式不能很好地扩展，因此我们需要寻找一个数值算法来解这个MRP贝尔曼方程</p>
<p><span class="math display">\[
\boldsymbol{V}^\pi=\boldsymbol{\mathcal{R}}^\pi+\gamma \boldsymbol{\mathcal{P}}^\pi\cdot \boldsymbol{V}^\pi
\]</span>
我们定义<strong>贝尔曼策略算子</strong><span class="math inline">\(\boldsymbol{B}^\pi:\mathbb{R}^m\rightarrow \mathbb{R}^m\)</span>为</p>
<p><span class="math display" id="eq:5-1">\[
\boldsymbol{B}^\pi(\boldsymbol{V})=\boldsymbol{\mathcal{R}}^\pi+\gamma \boldsymbol{\mathcal{P}}^\pi\cdot \boldsymbol{V}^\pi
\text{ for any vector }\boldsymbol{V} \text{ in the vector space }\mathbb{R}^m \tag{1.11}
\]</span>
因此，MRP贝尔曼方程就可以表示为</p>
<p><span class="math display">\[
\boldsymbol{V}^\pi=\boldsymbol{B}^\pi(\boldsymbol{V}^\pi)
\]</span>
这意味着<span class="math inline">\(\boldsymbol{V}^\pi\)</span>是贝尔曼策略算子<span class="math inline">\(\boldsymbol{B}^\pi\)</span>地一个不动点！注意，贝尔曼策略算子可以推广到非有限MDP地情况，并且<span class="math inline">\(\boldsymbol{V}^\pi\)</span>仍然是各种感兴趣的推广的不动点。然而，由于本章重点是开发有限MDP算法因此仍然使用上述狭义的（方程<a href="rlforfin.html#eq:5-1">(1.11)</a>）定义。此外，为了证明本章基于不动点的动态规划算法的正确性，我们假设折扣因子<span class="math inline">\(\gamma&lt;1\)</span>.</p>
<p>我们希望提出一种度量使得<span class="math inline">\(\boldsymbol{B}^\pi\)</span>是一个压缩映射从而能够利用Banach不动点定理，通过反复应用贝尔曼策略算子<span class="math inline">\(\boldsymbol{B}^\pi\)</span>来解决这个预测问题。对于任何值函数<span class="math inline">\(\boldsymbol{V}\in\mathbb{R}^m\)</span>（这表示<span class="math inline">\(V:\mathcal{N}\rightarrow \mathbb{R}\)</span>），我们将表达任何状态<span class="math inline">\(s\in\mathcal{N}\)</span>的值为<span class="math inline">\(\boldsymbol{V}(s)\)</span>.</p>
<p>我们的度量$d:<sup>m</sup>m <span class="math inline">\(将是\)</span>L^$范数，定义为：</p>
<p><span class="math display">\[
d(\boldsymbol{X},\boldsymbol{Y})=\Vert\boldsymbol{X}-\boldsymbol{Y} \Vert_{\infty}=\max_{s\in\mathcal{N}}\vert(\boldsymbol{X}-\boldsymbol{Y})(s)\vert
\]</span>
<span class="math inline">\(\boldsymbol{B}^\pi\)</span>是在无穷范数下的压缩映射，这是因为对于所有的<span class="math inline">\(\boldsymbol{X},\boldsymbol{Y}\in\mathbb{R}^m,\)</span>我们有</p>
<p><span class="math display">\[
\max_{s\in\mathcal{N}}\vert \boldsymbol{B}^\pi(\boldsymbol{X})-\boldsymbol{B}^\pi(\boldsymbol{Y})(s)\vert=\gamma\cdot\max_{s\in\mathcal{N}}\vert(\boldsymbol{P}^\pi\cdot(\boldsymbol{X}-\boldsymbol{Y})(s))\vert\leq \gamma  \cdot\max_{s\in\mathcal{N}}\vert(\boldsymbol{X}-\boldsymbol{Y})(s)\vert
\]</span>
因此调用Banach不动点定理就证明了下面的定理</p>
<p><strong>定理（策略评估收敛定理）</strong>：<br />
对于一个有限的MDP，若<span class="math inline">\(\vert\mathcal{N}\vert=m,\gamma&lt;1,\)</span>如果<span class="math inline">\(\boldsymbol{V}^\pi\in\mathbb{R}^m\)</span>是在固定策略<span class="math inline">\(\pi:\mathcal{N\times A}\rightarrow [0,1]\)</span>下评估的价值函数，则<span class="math inline">\(\boldsymbol{V}^\pi\)</span>是贝尔曼策略算子<span class="math inline">\(\boldsymbol{B}^\pi\)</span>的唯一不动点，并且：</p>
<p><span class="math display">\[
\lim_{i\rightarrow \infty}(\boldsymbol{B}^\pi)^i(\boldsymbol{V}_0)\rightarrow \text{ for all starting Value Functions }\boldsymbol{V}_0\in\mathbb{R}^m
\]</span></p>
<p>这给我们提供了以下的迭代算法（称为固定策略<span class="math inline">\(\pi\)</span>下的策略评估算法）：</p>
<ul>
<li><p>从任意<span class="math inline">\(\boldsymbol{V}_0\in\mathbb{R}^m\)</span>开始</p></li>
<li><p>对于每次迭代<span class="math inline">\(i=0,1,...,\)</span>计算：<br />
<span class="math display">\[
  \boldsymbol{V}_{i+1}=\boldsymbol{B}^\pi(\boldsymbol{V}_{i})=\boldsymbol{\mathcal{R}}^\pi+\gamma \boldsymbol{\mathcal{P}}^\pi\cdot \boldsymbol{V}_i
  \]</span></p></li>
<li><p>当<span class="math inline">\(d(\boldsymbol{V}_{i},\boldsymbol{V}_{i+1})=\max_{s\in\mathcal{N}}\)</span>(<em>i-</em>{i+1})(s) $足够小时停止算法。</p></li>
</ul>
<p>请注意，尽管我们将贝尔曼策略算子<span class="math inline">\(\boldsymbol{B}^\pi\)</span>定义为作用于<span class="math inline">\(\pi\)</span>隐式的MRP值函数，但我们也可将其看作作用于MDP的值函数。为了支持MDP的视角，我们将方程<a href="rlforfin.html#eq:5-1">(1.11)</a>重新表达为MDP转移/奖励规范如下所示</p>
<p><span class="math display" id="eq:5-2">\[
\boldsymbol{B}^\pi(s)=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)+\gamma\sum_{a\in\mathcal{A}} \pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot \boldsymbol{V}(s&#39;)\text{ for all }s\in\mathcal{N} \tag{1.12}
\]</span>
如果给定MRP的非终止状态数<span class="math inline">\(m,\)</span>则每次迭代的运行时间为<span class="math inline">\(O(m^2).\)</span>注意，要从给定的MDP和给定的策略构建MRP需要<span class="math inline">\(O(m^2\cdot k)\)</span>次运算，其中<span class="math inline">\(k=|\mathcal{A}|.\)</span></p>
</div>
<div id="贪心策略" class="section level3 hasAnchor" number="1.3.4">
<h3><span class="header-section-number">1.3.4</span> 贪心策略<a href="rlforfin.html#贪心策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>我们之前提到过要展示三种动态规划算法。第一种策略评估如上一节所见。解决了MDP预测问题。接下来两节中介绍的两种将会解决MDP控制问题，本节是从预测到控制的一个过渡，在这一节中，我们定义了一个函数，该函数通过“贪心”技术来改进值函数或者策略。形式上，贪心策略函数</p>
<p><span class="math display">\[
G:\mathbb{R}^m\rightarrow (\mathcal{N\rightarrow A})
\]</span>
将一个值函数<span class="math inline">\(\boldsymbol{V}\)</span>(表示为向量)映射到一个确定性策略<span class="math inline">\(\pi&#39;:\)</span></p>
<p><span class="math display" id="eq:5-3">\[
G(\boldsymbol{V})(s):\pi_D&#39;(s)=\mathop{\arg\max}_{a\in\mathcal{A}}\{\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot\boldsymbol{V}(s&#39;)  \} \text{ for all }s\in\mathcal{N} \tag{1.13}
\]</span>
请注意，对于任何特定的<span class="math inline">\(s,\)</span>如果两个或多个动作<span class="math inline">\(a\)</span>实现了<span class="math inline">\(\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot\boldsymbol{V}(s&#39;)\)</span>的最大化,那我们将使用任意一个动作打破平局并分配一个单一的动作<span class="math inline">\(a\)</span>来作为上述<span class="math inline">\(\arg\max\)</span>操作的输出。下面使用一个等效的表达式(来指导代码)</p>
<p><span class="math display" id="eq:5-4">\[
G(\boldsymbol{V})(s)=\mathop{\arg\max}_{a\in\mathcal{A}}\left\{\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)\cdot(r+\gamma\cdot \boldsymbol{W}(s&#39;))\right\}\text{ for all }s\in\mathcal{N} \tag{1.14}
\]</span>
其中<span class="math inline">\(\boldsymbol{W}\in\mathbb{R}^n\)</span>定义为</p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{W}(s)=\begin{cases}
\boldsymbol{V}(s&#39;)&amp;\text{ if }s&#39;\in\mathcal{N}\\
0&amp;\text{ if }s&#39;\in\mathcal{T=S-N}
\end{cases}
\end{equation}\]</span></p>
<p>注意在方程<a href="rlforfin.html#eq:5-4">(1.14)</a>中必须使用<span class="math inline">\(\mathcal{P}_R\)</span>，我们需要考虑到所有状态<span class="math inline">\(s&#39;\in\mathcal{S}\)</span>的转移而不是方程<a href="rlforfin.html#eq:5-3">(1.13)</a>中<span class="math inline">\(s&#39;\in\mathcal{N}\)</span>的状态。因此，我们需要小心处理<span class="math inline">\(s&#39;\in\mathcal{T}\)</span>的转移。</p>
<p>“贪心”一次来源于“贪心算法”，意味着一种算法通过在局部做出最优选择期望能接近全局最优解。在这里，贪心策略意味着如果我们有一个策略<span class="math inline">\(\pi\)</span>及其对应的值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>(假设通过策略评估算法获得)，那么应用贪心策略函数<span class="math inline">\(G\)</span>到<span class="math inline">\(\boldsymbol{V}^\pi\)</span>将得到一个确定性策略<span class="math inline">\(\pi_D&#39;,\)</span>它预期在某种意义上比<span class="math inline">\(\pi\)</span>更好，具体而言即<span class="math inline">\(\boldsymbol{V}^{\pi&#39;_D}\)</span>要优于<span class="math inline">\(\boldsymbol{V}^\pi\)</span>.</p>
</div>
<div id="策略提升" class="section level3 hasAnchor" number="1.3.5">
<h3><span class="header-section-number">1.3.5</span> 策略提升<a href="rlforfin.html#策略提升" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>像“更好”、“提升”这样的术语指的是值函数或者策略（后者指的是评估给定策略的MDP的值函数）。那么，什么叫做值函数<span class="math inline">\(X:\mathcal{N}\rightarrow \mathbb{R}\)</span>比<span class="math inline">\(Y:\mathcal{N}\rightarrow \mathbb{R}\)</span>更好呢？下面的定义给出了答案</p>
<p><strong>定义（值函数的比较）</strong><br />
我们说对于一个有限的MDP，值函数<span class="math inline">\(X\)</span>比值函数<span class="math inline">\(Y\)</span>更好，记作<span class="math inline">\(X\geq Y,\)</span>当且仅当</p>
<p><span class="math display">\[
X(s)\geq Y(s)\quad \forall s\in\mathcal{N}
\]</span>
如果我们处理的是有限的MDP（具有<span class="math inline">\(m\)</span>个非终止状态），我们会将值函数表示为向量的形式<span class="math inline">\(\boldsymbol{X},\boldsymbol{Y}\in\mathbb{R}^m.\)</span></p>
<p>因此，每当你听到更好的值函数或者改进的值函数这样的术语时，应该理解为值函数在每个状态下都不比它所比较的值函数更差。</p>
<p>那么，什么是<span class="math inline">\(\pi_D&#39;=G(\boldsymbol{V}^\pi)\)</span>比<span class="math inline">\(\pi\)</span>更好呢？下面是Richard Bellman的一个重要定理，给出了明确的解释：</p>
<p><strong>定理（策略改进定理）</strong>：对于一个有限的MDP，对于任意策略<span class="math inline">\(\pi\)</span>,都有：</p>
<p><span class="math display">\[
\boldsymbol{V}^{\pi_D&#39;}=\boldsymbol{V}^{G(\boldsymbol{V}^{\pi})}\geq \boldsymbol{V}^{\pi}
\]</span>
这个证明基于应用贝尔曼策略算子在给定MDP的值函数上的作用（注意这种MDP视角下的贝尔曼策略算子在<a href="rlforfin.html#eq:5-2">(1.12)</a>中表示）。我们首先注意到，反复应用贝尔曼策略算子<span class="math inline">\(B^{\pi_D&#39;}\)</span>从值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>开始，最终会收敛到<span class="math inline">\(\boldsymbol{V}^{\pi_D&#39;}.\)</span>形式上，</p>
<p><span class="math display">\[
\lim_{i\rightarrow \infty}(B^{\pi_D&#39;})^i(\boldsymbol{V}^\pi)=\boldsymbol{V}^{\pi_D&#39;}
\]</span>
因此证明的关键是证明</p>
<p><span class="math display">\[
(B^{\pi_D&#39;})^{i+1}(\boldsymbol{V}^\pi)\geq (B^{\pi_D&#39;})^i(\boldsymbol{V}^\pi)\quad \forall i=0,1,2,...
\]</span>
这意味着通过反复应用贝尔曼策略算子得到一个不下降的值函数序列，随着反复应用，会不断改善直到收敛到值函数<span class="math inline">\(\boldsymbol{V}^{\pi_D&#39;}\)</span>.</p>
<p>策略改进定理为我们提供了用来解决MDP控制问题的第一个动态规划算法（称为策略迭代）。策略迭代算法是Ronald Howard（1960）提出的。</p>
</div>
<div id="策略迭代算法" class="section level3 hasAnchor" number="1.3.6">
<h3><span class="header-section-number">1.3.6</span> 策略迭代算法<a href="rlforfin.html#策略迭代算法" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>策略提升定理的证明向我们展示了怎么从一个关于策略<span class="math inline">\(\pi\)</span>的值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>出发，通过贪婪策略改进生成策略<span class="math inline">\(\pi_D&#39;=G(\boldsymbol{V})\)</span>,然后以<span class="math inline">\(\boldsymbol{V}^\pi\)</span>为起始价值函数进行策略评估（使用策略<span class="math inline">\(\pi_D&#39;\)</span>），得到改进后的价值函数<span class="math inline">\(\boldsymbol{V}^{\pi_D&#39;}\)</span>,该价值函数优于我们最初的价值函数<span class="math inline">\(\boldsymbol{V}^\pi.\)</span>现在需要注意的是，我们可以重复这一过程，从<span class="math inline">\(\pi_D&#39;,\boldsymbol{V}^{\pi_D&#39;}\)</span>出发，进一步改进策略<span class="math inline">\(\pi_D&#39;&#39;\)</span>及其相关的改进价值函数<span class="math inline">\(\boldsymbol{V}^{\pi_D&#39;&#39;}\)</span>,我们可以继续这种方式，生成进一步改进的策略及其相关的价值函数，直到无法再改进为止。这种将策略改进与使用改进策略进行策略评估相结合的方法被称为<strong>策略迭代算法</strong>。</p>
<ul>
<li><p>从任意价值函数<span class="math inline">\(\boldsymbol{V}_0\in\mathbb{R}^m\)</span>开始；</p></li>
<li><p>迭代<span class="math inline">\(j=0,1,2,...,\)</span>在每次迭代中计算：</p>
<ul>
<li>确定性策略:<span class="math inline">\(\pi_{j+1}=G(\boldsymbol{V}_j)\)</span><br />
</li>
<li>价值函数：<span class="math inline">\(\boldsymbol{V}_{j+1}=\lim_{i\rightarrow \infty}(B^{\pi_{j+1}})(\boldsymbol{V}_j)\)</span><br />
</li>
</ul></li>
<li><p>当<span class="math inline">\(d(\boldsymbol{V}_i,\boldsymbol{V}_{i+1})=\max_{s\in\mathcal{N}} |(\boldsymbol{V}_i-\boldsymbol{V}_{i+1})(s)|\)</span>足够小，停止算法。</p></li>
</ul>
<p>因此，当价值函数无法进一步改进时，算法终止，当这种情况发生时，以下等式应成立：</p>
<p><span class="math display">\[
\boldsymbol{V}_j=(B^{G(\boldsymbol{V_j})})^i(\boldsymbol{V}_j)=\boldsymbol{V}_{j+1}\text{ for all }i=0,1,2,...
\]</span>
特别地，当<span class="math inline">\(i=1\)</span>时有：</p>
<p><span class="math display">\[
\boldsymbol{V}_j(s)=B^{G(\boldsymbol{V_j})}(\boldsymbol{V}_j)(s)=\mathcal{R}(s,G(\boldsymbol{V}_j)(s))+\gamma\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,G(\boldsymbol{V_j})(s),s&#39;)\cdot \boldsymbol{V_j}(s&#39;)\text{ for all }s\in\mathcal{N}
\]</span>
由方程<a href="rlforfin.html#eq:5-3">(1.13)</a>可知，我们对于每个<span class="math inline">\(s\in\mathcal{N},\pi_{j+1}(s)=G(\boldsymbol{V_j})(s)\)</span>是最大化<span class="math inline">\(\{ \mathcal{R}(s,a)+\gamma\sum_{s&#39;}\mathcal{P}(s,a,s&#39;)\cdot \boldsymbol{V_j}(s&#39;) \}\)</span>的动作，因此</p>
<p><span class="math display">\[
\boldsymbol{V_j}(s)=\max_{a\in\mathcal{A}}\left\{ \mathcal{R}(s,a)+\gamma\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot\boldsymbol{V_j}(s&#39;) \right\}\text{ for all }s\in\mathcal{N}
\]</span>
但这实际上是MDP状态-价值函数贝尔曼最优性方程，这意味着<span class="math inline">\(\boldsymbol{V}_j=\boldsymbol{V}^*,\)</span>即当<span class="math inline">\(\boldsymbol{V}_{j+1}=\boldsymbol{V}_j\)</span>时，策略迭代算法已收敛到最优价值函数。策略迭代算法收敛时的确定性策略<span class="math inline">\(\pi_j:\mathcal{N\rightarrow A}\)</span>是一个最优策略，因为<span class="math inline">\(\boldsymbol{Y}_{\pi_j}=\boldsymbol{V}_j\approx \boldsymbol{V}^*\)</span>,这意味着用确定性策略<span class="math inline">\(\pi_j\)</span>评估MDP可以实现最优价值函数。这表明策略迭代算法解决了MDP控制问题。这证明了以下定理：</p>
<p><strong>定理（策略迭代收敛定理）：</strong> 对于具有<span class="math inline">\(|\mathcal{N}|=m,\gamma&lt;1\)</span>的有限MDP，策略迭代算法收敛到最优价值函数<span class="math inline">\(\boldsymbol{V}^*\in\mathbb{R}^m\)</span>以及一个确定性最优策略<span class="math inline">\(\pi_D^*:\mathcal{N\rightarrow A}\)</span>，无论我们从哪个价值函数<span class="math inline">\(\boldsymbol{V}_0\)</span>开始算法。</p>
</div>
<div id="贝尔曼最优性算子与值迭代算法" class="section level3 hasAnchor" number="1.3.7">
<h3><span class="header-section-number">1.3.7</span> 贝尔曼最优性算子与值迭代算法<a href="rlforfin.html#贝尔曼最优性算子与值迭代算法" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>通过对方程<a href="rlforfin.html#eq:5-3">(1.13)</a>进行微调（将<span class="math inline">\(\arg\max\)</span>改为<span class="math inline">\(\max\)</span>）,我们定义贝尔曼最优性算子：</p>
<p><span class="math display">\[
B^*:\mathbb{R}^m\rightarrow \mathbb{R}^m
\]</span>
作为向量空间<span class="math inline">\(\mathbb{R}^m\)</span>中向量（表示价值函数）的以下非线性变换：</p>
<p><span class="math display" id="eq:5-5">\[
B^*(\boldsymbol{V})(s)=\max_{a\in\mathcal{A}}\left\{ \mathcal{R}(s,a)+\gamma\sum_{s\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot \boldsymbol{V}(s&#39;)\right\}\text{ for all }s\in\mathcal{N} \tag{1.15}
\]</span>
我们将在数学推导中使用方程<a href="rlforfin.html#eq:5-5">(1.15)</a>,但我们需要一个不同但等价的表达式来指导代码，使用接口操作的是<span class="math inline">\(\mathcal{P}_R\)</span>并非<span class="math inline">\(\mathcal{P,R}\)</span>,等价表达式如下：</p>
<p><span class="math display" id="eq:5-6">\[
B^*(\boldsymbol{V})(s)=\max_{a\in\mathcal{A}}\left\{ \sum_{s\in\mathcal{N}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)\cdot
(r+\gamma\boldsymbol{W}(s&#39;))
\right\}\text{ for all }s\in\mathcal{N} \tag{1.16}
\]</span>
其中<span class="math inline">\(\boldsymbol{W}\in\mathbb{R}^n\)</span>定义为</p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{W}(s)=\begin{cases}
\boldsymbol{V}(s&#39;)&amp;\text{ if }s&#39;\in\mathcal{N}\\
0&amp;\text{ if }s&#39;\in\mathcal{T=S-N}
\end{cases}
\end{equation}\]</span></p>
<p>注意在方程<a href="rlforfin.html#eq:5-6">(1.16)</a>中，由于我们需要考虑到所有状态<span class="math inline">\(s&#39;\in\mathcal{S}\)</span>的转移而不是方程<a href="rlforfin.html#eq:5-3">(1.13)</a>中<span class="math inline">\(s&#39;\in\mathcal{N}\)</span>的状态。因此，我们需要小心处理<span class="math inline">\(s&#39;\in\mathcal{T}\)</span>的转移。</p>
<p>对于每一个<span class="math inline">\(s\in\mathcal{N},\)</span>在<a href="rlforfin.html#eq:5-5">(1.15)</a>中产生的最大化动作<span class="math inline">\(a\in\mathcal{A}\)</span>是由确定性策略<span class="math inline">\(\pi_D\)</span>在<a href="rlforfin.html#eq:5-3">(1.13)</a>中规定的动作。因此，如果我们使用贪婪策略<span class="math inline">\(G(\boldsymbol{V})\)</span>在任何价值函数上应用贝尔曼策略算子，它应该与应用贝尔曼最优性算子相同，因此：</p>
<p><span class="math display" id="eq:5-7">\[
B^{G(\boldsymbol{V})}(\boldsymbol{V})=B^*(\boldsymbol{V})\text{ for all }\boldsymbol{V}\in\mathbb{R}^m \tag{1.17}
\]</span>
特别地，通过将<span class="math inline">\(\boldsymbol{V}\)</span>转化为策略<span class="math inline">\(\pi\)</span>的价值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>,我们得到：</p>
<p><span class="math display">\[
B^{G(\boldsymbol{V}^\pi)}(\boldsymbol{V}^\pi)=B^*(\boldsymbol{V}^\pi)
\]</span>
这是策略评估第一阶段的一个简洁表示，其中使用了改进的策略<span class="math inline">\(G(\boldsymbol{V}^\pi)\)</span>（注意贝尔曼策略算子、贝尔曼最优性算子和贪婪策略函数如何在这个方程中结合在一起）。</p>
<p>正如贝尔曼策略算子<span class="math inline">\(B^\pi\)</span>是由MDP贝尔曼策略方程（等价于MRP贝尔曼方程）所驱动的，贝尔曼最优性算子<span class="math inline">\(B^*\)</span>是由MDP状态-价值函数贝尔曼最优性方程（重新陈述如下）所驱动的</p>
<p><span class="math display">\[
\boldsymbol{V}^*(s)=\max_{a\in\mathcal{A}}\left\{\mathcal{R}(s,a)+\gamma\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot\boldsymbol{V}^*(s&#39;)\right\}\text{ for all }s\in\mathcal{N}
\]</span>
因此，我们可以简洁地将MDP状态-价值函数贝尔曼最优性方程表示为：</p>
<p><span class="math display">\[
\boldsymbol{V^*}=B^*(\boldsymbol{V^*})
\]</span>
这意味着<span class="math inline">\(\boldsymbol{V^*}\)</span>是贝尔曼最优性算子<span class="math inline">\(B^*\)</span>地一个不动点。</p>
<p>需要注意的是，我们提供的贪婪策略函数和贝尔曼最优性算子的定义可以推广到非有限MDP，因此我们可以推广方程<a href="rlforfin.html#eq:5-7">(1.17)</a>，并且<span class="math inline">\(\boldsymbol{V^*}\)</span>是贝尔曼最优性算子的不动点的陈述仍然成立。然而，在本章中由于我们专注于开发有限 MDP 的算法，因此我们将坚持为有限 MDP 提供的定义。</p>
<p>正如我们证明<span class="math inline">\(B^\pi\)</span>是一个压缩函数一样，我们希望证明<span class="math inline">\(B^*\)</span>也是一个压缩函数（在<span class="math inline">\(L^\infty\)</span>范数下），以便我们可以利用Banach不动点定理，并通过迭代应用贝尔曼最优性算子<span class="math inline">\(B^*\)</span>来解决控制问题。因此，我们需要证明对于所有<span class="math inline">\(\boldsymbol{X},\boldsymbol{Y}\in\mathbb{R}^m:\)</span></p>
<p><span class="math display">\[
\max_{s\in\mathcal{N}}|(B^*(\boldsymbol{X})-B*(\boldsymbol{Y}))(s)|\leq \gamma\cdot \max_{s\in\mathcal{N}}|(\boldsymbol{X}-\boldsymbol{Y})(s)|
\]</span>
这个证明比之前为<span class="math inline">\(B^\pi\)</span>所做的证明要难一些，这里需要利用<span class="math inline">\(B^*\)</span>的两个关键性质，单调性和常数平移性：对于所有的<span class="math inline">\(\boldsymbol{X}\in\mathbb{R}^m,c\in\mathbb{R},B^*(\boldsymbol{X}+c)(s)=B^*(\boldsymbol{X})(s)+\gamma c\)</span>对于所有的<span class="math inline">\(s\in\mathcal{N}\)</span>.证明暂时略去，调用Banach不动点定理证明下面的定理：</p>
<p><strong>定理（值迭代收敛定理）：</strong>对于具有<span class="math inline">\(|\mathcal{N}|=m,\gamma&lt;1\)</span>的有限MDP，如果<span class="math inline">\(\boldsymbol{V}^*\in\mathbb{R}^m\)</span>是最优值函数，则<span class="math inline">\(\boldsymbol{V}^*\)</span>是贝尔曼最优性算子<span class="math inline">\(B^*\)</span>的唯一不动点，并且：</p>
<p><span class="math display">\[
\lim_{i\rightarrow \infty}(B^*)^i(\boldsymbol{V}_0)\rightarrow \boldsymbol{V}^*\text{ for all staring Value Function }\boldsymbol{V}_0\in\mathbb{R}^m
\]</span></p>
<p>这为我们提供了以下迭代算法，称为<strong>值迭代算法</strong>，有Richard Bellman提出：</p>
<ul>
<li><p>从任意价值函数<span class="math inline">\(\boldsymbol{V}_0\)</span>开始;</p></li>
<li><p>迭代<span class="math inline">\(i=0,1,2,...,\)</span>在每次迭代中计算：<br />
<span class="math display">\[
\boldsymbol{V}_{i+1}(s)=B^*(\boldsymbol{V}_i)(s)\text{ for all }s\in\mathcal{N}
\]</span></p></li>
<li><p>当<span class="math inline">\(d(\boldsymbol{V}_i,\boldsymbol{V}_{i+1})=\max_{s\in\mathcal{N}} |(\boldsymbol{V}_i-\boldsymbol{V}_{i+1})(s)|\)</span>足够小，停止算法。</p></li>
</ul>
</div>
<div id="从最优值函数到最优策略" class="section level3 hasAnchor" number="1.3.8">
<h3><span class="header-section-number">1.3.8</span> 从最优值函数到最优策略<a href="rlforfin.html#从最优值函数到最优策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>需要注意的是，策略迭代算法在每次迭代中都会生成一个策略及其对应的价值函数。因此，最终当我们收敛到最优值函数<span class="math inline">\(\boldsymbol{V}_j=\boldsymbol{V}^*\)</span>时，策略迭代算法总会有一个确定性策略<span class="math inline">\(\pi_j\)</span>与<span class="math inline">\(\boldsymbol{V}_j\)</span>相关联，使得：</p>
<p><span class="math display">\[
\boldsymbol{V}_j=\boldsymbol{V}^{\pi_j}=\boldsymbol{V}^*
\]</span>
我们将<span class="math inline">\(\pi_j\)</span>称为最优策略<span class="math inline">\(\pi^*,\)</span>即产生最优值函数<span class="math inline">\(\boldsymbol{V}^*\)</span>的策略，即</p>
<p><span class="math display">\[
\boldsymbol{V}^{\pi^*}=\boldsymbol{V}^*
\]</span>
然而，值迭代算法没有与之相关联的策略，因为整个算法缺乏策略表示，仅操作价值函数。因此，现在的问题是：当值迭代手来难道最优价值函数<span class="math inline">\(\boldsymbol{V}_i=\boldsymbol{V}^*\)</span>,我们如何获得一个最优策略使得：</p>
<p><span class="math display">\[
\boldsymbol{V}^{\pi^*}=\boldsymbol{V}_i=\boldsymbol{V}^*
\]</span>
答案在于贪婪策略函数<span class="math inline">\(G.\)</span>方程<a href="rlforfin.html#eq:5-7">(1.17)</a>告诉我们：</p>
<p><span class="math display">\[
B^{G(\boldsymbol{V})}(\boldsymbol{V})=B^*(\boldsymbol{V})\text{ for all }\boldsymbol{V}\in\mathbb{R}^m
\]</span>
将<span class="math inline">\(\boldsymbol{V}\)</span>特化为<span class="math inline">\(\boldsymbol{V}^*\)</span>,我们得到</p>
<p><span class="math display">\[
B^{G(\boldsymbol{V}^*)}(\boldsymbol{V}^*)=B^*(\boldsymbol{V}^*)
\]</span>
但我们又指导<span class="math inline">\(\boldsymbol{V}^*\)</span>是贝尔曼算子<span class="math inline">\(B^*\)</span>的不动点，因此</p>
<p><span class="math display">\[
B^{G(\boldsymbol{V}^*)}(\boldsymbol{V}^*)=\boldsymbol{V}^*
\]</span>
这表明，用确定性贪婪策略<span class="math inline">\(G(\boldsymbol{V}^*)\)</span>(使用贪婪策略函数从最优价值函数创建的策略)评估MDP，实际上实现了最优价值函数<span class="math inline">\(\boldsymbol{V}^*.\)</span>换句话说，<span class="math inline">\(G(\boldsymbol{V}^*)\)</span>是我们一直在寻找的确定性最优策略<span class="math inline">\(\pi^*.\)</span></p>
</div>
<div id="广义策略迭代" class="section level3 hasAnchor" number="1.3.9">
<h3><span class="header-section-number">1.3.9</span> 广义策略迭代<a href="rlforfin.html#广义策略迭代" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>本节中将深入探讨策略迭代算法的结构，并展示如何将其结构推广到更一般的情况。让我们首先从二维布局的角度来看策略迭代中价值函数从初始值函数<span class="math inline">\(\boldsymbol{V}_0\)</span>到最终价值函数<span class="math inline">\(\boldsymbol{V}^*\)</span>的演进过程。</p>
<p><strong>策略迭代的二维布局</strong></p>
<p>策略迭代的演进过程可以用以下二维布局表示：</p>
<p>$$
<span class="math display">\[\begin{align}
\pi_1=G(\boldsymbol{V}_0),\boldsymbol{V}_0\rightarrow B^{\pi_1}(\boldsymbol{V}_0)\rightarrow &amp;... (B^{\pi_1})^i(\boldsymbol{V}_0)\rightarrow... \boldsymbol{V}^{\pi_1}=\boldsymbol{V}_1\\
\pi_2=G(\boldsymbol{V}_1),\boldsymbol{V}_1\rightarrow B^{\pi_2}(\boldsymbol{V}_1)\rightarrow &amp;... (B^{\pi_2})^i(\boldsymbol{V}_1)\rightarrow... \boldsymbol{V}^{\pi_2}=\boldsymbol{V}_2\\
&amp;...\\
&amp;...\\
\pi_{j+1}=G(\boldsymbol{V}_j),\boldsymbol{V}_j\rightarrow B^{\pi_{j+1}}(\boldsymbol{V}_j)\rightarrow &amp;... (B^{\pi_{j+1}})^i(\boldsymbol{V}_j)\rightarrow... \boldsymbol{V}^{\pi_{j+1}}=\boldsymbol{V}_j\\


\end{align}\]</span>
$$
每一行代表在特定策略下价值函数的演进过程。每一行从使用贪婪策略函数<span class="math inline">\(G\)</span>创建策略开始，其余部分是通过对该策略应用贝尔曼策略算子<span class="math inline">\(B^\pi\)</span>指导收敛到该策略的价值函数。因此每一行以策略改进开始，其余部分是策略评估。注意，每一行的结束通过贪婪策略函数<span class="math inline">\(G\)</span>与下一行的开始无缝衔接！</p>
<p><strong>策略迭代的三重循环</strong></p>
<p>策略迭代算法实际上包含三重循环：</p>
<ol style="list-style-type: decimal">
<li><p>最外层循环：遍历二维布局中的每一行（每次迭代生成一个改进的策略）。</p></li>
<li><p>中间层循环：遍历每一行中的列（每次迭代应用贝尔曼策略算子，即策略评估的迭代）。</p></li>
<li><p>最内层循环：遍历所有状态<span class="math inline">\(s\in\mathcal{N},\)</span>因为在应用贝尔曼策略算子更新价值函数时需要遍历所有状态（在应用贪婪策略函数改进策略时也需要遍历所有状态）。</p></li>
</ol>
<p><strong>策略迭代的高层次视角</strong></p>
<p>从更高层次来看，策略迭代时策略评估和策略改进交替进行的过程：</p>
<ul>
<li><p><strong>策略评估</strong>：根据当前策略生成价值函数。</p></li>
<li><p><strong>策略改进</strong>：根据当前价值函数生成贪婪策略（相对于前一个策略有所改进）。</p></li>
</ul>
<p>这种交替过程使得价值函数和策略逐渐趋于一致，直到最终收敛。</p>
<p><strong>策略迭代的可视化</strong></p>
<p>下图展示了策略迭代中价值函数和策略的演进过程。图中：</p>
<ul>
<li><p>下部的线（策略线）：表示策略的演进。</p></li>
<li><p>上部的线（价值函数线）：表示价值函数的演进。</p></li>
<li><p>指向价值函数线的箭头：表示对给定策略<span class="math inline">\(\pi\)</span>的策略评估，生成价值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>.</p></li>
<li><p>指向策略线的箭头：表示从价值函数<span class="math inline">\(\boldsymbol{V}^\pi\)</span>生成贪婪策略<span class="math inline">\(\pi&#39;=G(\boldsymbol{V}^\pi)\)</span>.</p></li>
</ul>
<p>策略评估和策略改进是“竞争”的——它们“朝不同方向推动”，但最终目标是使价值函数和策略趋于一致。</p>
<p><strong>广义策略迭代</strong></p>
<p>广义策略迭代（Generalized Policy Iteration, GPI）是 Sutton 和 Barto 在其强化学习书中强调的一个重要概念，它统一了所有动态规划（DP）和强化学习（RL）算法的变体。GPI 的核心思想是：</p>
<ul>
<li><p>策略评估：可以使用任何策略评估方法</p></li>
<li><p>策略改进：可以使用任何策略改进方法（不一定是经典策略迭代算法中的方法）。</p></li>
</ul>
<p>GPI 的关键在于，策略评估和策略改进不需要完全达到它们各自追求的一致性目标。例如：</p>
<ul>
<li><p>策略评估：可以只进行几次贝尔曼策略评估，而不是完全收敛到<span class="math inline">\(V^{\pi}\)</span>.</p></li>
<li><p>策略改进：可以只更新部分状态的策略，而不是所有状态。</p></li>
</ul>
<p><strong>值迭代作为GPI的实例</strong></p>
<p>值迭代是 GPI 的一个具体实例。在值迭代中，每次迭代只应用一次贝尔曼策略算子，然后进行策略改进。其二维布局如下：</p>
<p>$$</p>
<p>$$</p>
<p>在值迭代中，策略改进步骤保持不变，但策略评估简化为仅应用一次贝尔曼策略算子。</p>
<p>广义策略迭代是强化学习中最核心的概念之一。几乎所有强化学习控制算法都可以视为 GPI 的特例。例如，在某些简单的强化学习控制算法中：</p>
<ul>
<li><p>策略评估：只对单个状态进行。</p></li>
<li><p>策略改进：也只对单个状态进行。</p></li>
</ul>
<p>这些算法本质上是单状态策略评估和单状态策略改进的交替序列。</p>
<p><strong>总结</strong></p>
<ul>
<li><p>贝尔曼方程和广义策略迭代是强化学习中最重要的两个概念。</p></li>
<li><p>GPI 的核心思想是交替进行某种形式的策略评估和策略改进。</p></li>
<li><p>GPI 统一了动态规划和强化学习的各种算法，是理解强化学习控制问题的基础。</p></li>
</ul>
</div>
</div>
<div id="dynassall" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> 动态资产配置和消费<a href="rlforfin.html#dynassall" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="个人财务的优化" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> 个人财务的优化<a href="rlforfin.html#个人财务的优化" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>个人财务对某些人来说可能非常简单（每月领取工资，花光所有工资）而对另一些人来说可能非常复杂（例如，那些在多个国家拥有多家企业并拥有复杂资产和负债的人）。在这里，我们将考虑一个相对简单但包含足够细节的情况，以便提供动态资产配置和消费问题的基本要素。假设你的个人财务包括以下几个方面：</p>
<ul>
<li><p>收入：这可能包括你的定期工资，通常在一段时间内保持不变，但如果你获得晋升或找到新工作，工资可能会发生变化。这也包括你从投资组合中变现的资金，例如如果你卖出一些股票并决定不再投资于其他资产。此外，还包括你从储蓄账户或某些债券中获得的利息。还有许多其他收入来源，有些是固定的定期支付，有些则在支付金额和时间上具有不确定性，我们不会一一列举所有不同的收入方式。我们只想强调，在生活中的各个时间点获得收入是个人财务的关键方面之一。</p></li>
<li><p>消费：这里的“消费”指的是“支出”。需要注意的是，人们需要定期消费以满足基本需求，如住房、食物和衣物。你支付的房租或按揭贷款就是一个例子——它可能是每月固定金额，但如果你的按揭利率是浮动的，它可能会有所变化。此外，如果你搬到新房子，房租或按揭可能会有所不同。你在食物和衣物上的支出也构成了消费。这通常每月相对稳定，但如果你有了新生儿，可能需要额外的支出用于婴儿的食物、衣物甚至玩具。此外，还有超出“必需品”的消费——比如周末在高级餐厅用餐、夏季度假、购买豪华汽车或昂贵的手表。人们从这种消费中获得“满足感”或“幸福感”（即效用）。这里的关键点是，我们需要定期决定每周或每月花多少钱（消费）。在动态决策中，人们面临着消费（带来消费效用）和储蓄（将钱投入投资组合，希望钱能增值，以便未来能够消费更多）之间的张力。</p></li>
<li><p>投资：假设你可以投资于多种资产——简单的储蓄账户提供少量利息、交易所交易的股票（从价值股到成长股，各自有不同的风险-收益权衡）、房地产（你购买并居住的房子确实被视为一种投资资产）、黄金等大宗商品、艺术品等。我们将投资于这些资产的资金组合称为投资组合（关于投资组合理论的简要介绍见附录B）。我们需要定期决定是否应该将大部分资金投入储蓄账户以保安全，还是应该将大部分投资资金分配于股票，或者是否应该更具投机性，投资于早期初创公司或稀有艺术品。审查投资组合的构成并可能重新分配资金（称为重新平衡投资组合）是动态资产配置的问题。还需要注意的是，我们可以将部分收入投入投资组合（意味着我们选择不立即消费这笔钱）。同样，我们可以从投资组合中提取部分资金用于消费。将资金投入或提取出投资组合的决策本质上是我们所做的动态消费决策，它与动态资产配置决策密切相关。</p></li>
</ul>
<p>以上描述希望为你提供了资产配置和消费的双重动态决策的基本概念。最终，我们的个人目标是在一生中最大化消费的期望总效用（可能还包括在你去世后为配偶和子女提供的消费效用）。由于投资组合本质上是随机的，并且我们需要定期做出资产配置和消费决策，你可以看到这具备了随机控制问题的所有要素，因此可以建模为马尔可夫决策过程（尽管通常相当复杂，因为现实生活中的财务有许多细节）。以下是该MDP的粗略和非正式草图（请记住，我们将在本章后面为简化的情况形式化MDP）：</p>
<ul>
<li><p><strong>状态</strong>：状态通常可能非常复杂，但主要包括年龄（用于跟踪达到MDP时间范围的时间）、投资于每种资产的资金数量、所投资资产的估值，以及可能还包括其他方面，如工作/职业状况（用于预测未来工资的可能性）。</p></li>
<li><p><strong>动作</strong>：动作是双重的。首先，它是每个时间步骤中选择的投资金额向量（时间步骤是我们审查投资组合以重新分配资金的时间周期）。其次，它是选择消费的灵活/可选的资金数量（即超出我们承诺支付的固定支出，如房租）。</p></li>
<li><p><strong>奖励</strong>：奖励是我们视为灵活/可选的消费效用——它对应于动作的第二部分。</p></li>
<li><p><strong>模型</strong>：模型（给定当前状态和动作的下一个状态和奖励的概率）在大多数现实生活情况中可能相当复杂。最困难的方面是预测我们生活和职业中明天可能发生的事情（我们需要这种预测，因为它决定了我们未来获得收入、消费和投资的可能性）。此外，投资资产的不确定性运动也需要由我们的模型捕捉。</p></li>
</ul>
<p>现在，我们准备采用这个MDP的一个简单特例，它去除了许多现实世界的摩擦和复杂性，但仍保留了关键特征（特别是双重动态决策方面）。这个简单的特例是默顿投资组合问题（Merton 1969）的主题，他在1969年的一篇里程碑论文中提出并解决了这个问题。他公式的一个关键特征是时间是连续的，因此状态（基于资产价格）演化为连续时间随机过程，而动作（资产配置和消费）是连续进行的。我们在下一节中介绍他论文的重要部分。</p>
</div>
<div id="merton投资组合问题及其解决" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Merton投资组合问题及其解决<a href="rlforfin.html#merton投资组合问题及其解决" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在，我们描述默顿投资组合问题并推导其解析解，这是数学经济学中最优雅的解决方案之一。该解决方案的结构将为我们提供关于资产配置和消费决策如何不仅依赖于状态变量，还依赖于问题输入的深刻直觉。</p>
<p>我们将时间记为<span class="math inline">\(t\)</span>，并假设当前时间为<span class="math inline">\(t=0\)</span>.假设你刚刚退休并且你将再活<span class="math inline">\(T\)</span>年。因此，用上一节的语言来说，你余生将不会获得任何收入，除了从投资组合中提取资金的选项。再假设你还没有固定支出，如按揭贷款、订阅费等等，这意味着你所有的消费都是灵活/可选的，即你可以在任何时间点选择消费任何非负实数。以上所有假设都是不合理的（<span style="color: orange;">但如果在养老金的资产配置中，这是合理的！</span>），但有助于保持问题的简单性，以便于解析处理。</p>
<p>我们将任何时间<span class="math inline">\(t\)</span>的财富（记为<span class="math inline">\(W_t\)</span>）定义为你的投资资产的总市场价值。请注意，由于没有外部的收入，并且所有消费都是可选的，<span class="math inline">\(W_t\)</span>就是你的净资产。假设有固定数量的<span class="math inline">\(n\)</span>个风险资产和1个无风险资产。如前所述，目标是通过在任何时间点的双重动作——资产配置和消费（消费等于在任何时间点从投资组合中提取的资金）——最大化你一生中消费的期望总效用。请注意，由于没有外部资金来源，并且所有从投资组合中提取的资金都会立即被消费，因此你永远不会向投资组合中添加资金。投资组合的增长只能来自投资组合中资产市场价值的增长。最后，我们假设消费效用函数是恒定相对风险厌恶（CRRA）的。</p>
<p>为了便于阐述，我们将问题形式化，并针对<span class="math inline">\(n=1\)</span>（即只有1个风险资产）的情况推导出Merton的优美的解析解。该解可以直观地推广到<span class="math inline">\(n&gt;1\)</span>个风险资产的情况。<br />
由于我们在连续时间中进行操作，风险资产遵循一个随机过程<span class="math inline">\(S\)</span>,具体来说是一个几何布朗运动<br />
<span class="math display">\[
dS_t=\mu S_t+\sigma S_t dZ_t,
\]</span>
其中<span class="math inline">\(\mu\in\mathbb{R},\sigma\in\mathbb{R}_+\)</span>是固定常数（注意，对于<span class="math inline">\(n\)</span>个资产则分别为向量和矩阵）。<br />
无风险资产没有不确定性，并且在连续时间内有固定的增长率，因此在时间<span class="math inline">\(t\)</span>时无风险资产<span class="math inline">\(R_t\)</span>的估值由下式给出</p>
<p><span class="math display">\[
dR_t=rR_tdt,
\]</span>
其中<span class="math inline">\(r\in\mathbb{R}\)</span>是一个固定常数。我们将单位时间内财富的消费记为<span class="math inline">\(c(t,W_t)\geq0\)</span>，以明确消费决策通常取决于<span class="math inline">\(t,W_t.\)</span>将时间<span class="math inline">\(t\)</span>时分配给风险资产的财富比例记为<span class="math inline">\(\pi(t,W_t).\)</span>注意，<span class="math inline">\(c(t,W_t),\pi(t,W_t)\)</span>共同构成了时间<span class="math inline">\(t\)</span>时的决策（MDP动作）。为了保持简洁，将<span class="math inline">\(c(t,W_t),\pi(w,W_t)\)</span>分别写为<span class="math inline">\(c_t,\pi_t\)</span>,但请在整个推导过程中认识到两者都是时间<span class="math inline">\(t\)</span>和财富<span class="math inline">\(W_t\)</span>的函数。<br />
最后，我们假设消费的效用函数为</p>
<p><span class="math display">\[
U(x)=\frac{x^{1-\gamma}}{1-\gamma},
\]</span>
其中风险厌恶参数<span class="math inline">\(\gamma\neq1\)</span>.<span class="math inline">\(\gamma\)</span>是CRRA系数，等于<span class="math inline">\(\frac{-xU&#39;&#39;(x)}{U&#39;(x)}.\)</span>我们不会讨论<span class="math inline">\(\gamma=1\)</span>时的CRRA效用函数即<span class="math inline">\(U(x)=\log x\)</span>.<br />
由于我们假设没有向投资组合中添加资金，且没有买卖任何分数量的风险和无风险资产的交易成本，财富的时间演化应被概念化为分配比例<span class="math inline">\(\pi_t\)</span>的连续调整和从投资组合中的连续提取（等于连续消费<span class="math inline">\(c_t\)</span>）.因此从时间<span class="math inline">\(t\)</span>到<span class="math inline">\(t+dt\)</span>的财富变化<span class="math inline">\(dW_t\)</span>由下式给出：</p>
<p><span class="math display" id="eq:8-1">\[\begin{equation}
dW_t=((r+\pi_t\cdot (\mu-r))\cdot W_t-c_t)dt+\pi_t\sigma W_t dZ_t. \tag{1.18}
\end{equation}\]</span></p>
<p>这是一个确定财富随机演化的伊藤过程。<br />
我们的目标是确定在任何时间<span class="math inline">\(t\)</span>时的最优<span class="math inline">\((\pi(t,W_t),c(t,W_t))\)</span>,以最大化</p>
<p><span class="math display">\[
\mathbb{E}\left[\int_t^T\frac{e^{-\rho(s-t)}\cdot c_s^{1-\gamma}}{1-\gamma} ds+\frac{e^{-\rho (T-t)}\cdot B(T)\cdot W_T^{1-\gamma}}{1-\gamma}|W_t \right],
\]</span>
其中<span class="math inline">\(\rho\geq0\)</span>是效用贴现率，用于考虑未来消费效用可能低于当前效用的事实，<span class="math inline">\(B(\cdot)\)</span>被称为遗赠函数，可以视为你在时间<span class="math inline">\(T\)</span>去世时留给家人的钱。我们可以为任意遗赠函数<span class="math inline">\(B(T)\)</span>解决这个问题，但为了简单起见，我们考虑<span class="math inline">\(B(T)=\epsilon^\gamma,0&lt;\epsilon \ll 1\)</span>,意味着无遗赠。出于技术原因我们不将其设为0，这将在后面变得明显。<br />
我们应该将这个问题视为一个连续时间的随机控制问题，其中MDP定义如下：</p>
<ul>
<li><p>时间<span class="math inline">\(t\)</span>的状态为<span class="math inline">\((t,W_t)\)</span></p></li>
<li><p>时间<span class="math inline">\(t\)</span>的动作为<span class="math inline">\((\pi_t,c_t)\)</span></p></li>
<li><p>时间<span class="math inline">\(t&lt;T\)</span>时的单位时间奖励为：<br />
<span class="math display">\[
U(c_t)=\frac{c_t^{1-\gamma}}{1-\gamma}
\]</span>
在终端时刻<span class="math inline">\(T\)</span>的奖励为<br />
<span class="math display">\[
B(T)\cdot U(W_T)=\epsilon^\gamma\cdot \frac{W_T^{1-\gamma}}{1-\gamma}
\]</span>
时间<span class="math inline">\(t\)</span>时的回报是累积贴现奖励</p>
<p><span class="math display">\[
\int_t^T\frac{e^{-\rho(s-t)\cdot c_s^{1-\gamma}}}{1-\gamma} ds+\frac{e^{-\rho (T-t)}\cdot \epsilon^\gamma\cdot W_T^{1-\gamma}}{1-\gamma}
\]</span></p></li>
</ul>
<p>我们的目标是找到策略：<span class="math inline">\((t,W_t)\rightarrow (\pi_t,c_t)\)</span>,以最大化期望汇报。<br />
我们第一步是写出Hamilton-Jacobi-Bellman(HJB)方程，这是连续时间中的Bellman最优性方程的类比。我们将最优的价值函数记为<span class="math inline">\(V^*\)</span>,使得时间<span class="math inline">\(t\)</span>时财富<span class="math inline">\(W_t\)</span>的最优价值为<span class="math inline">\(V^*(t,W_t).\)</span>这里的HJB方程可以特化为下面的式子</p>
<p><span class="math display" id="eq:8-2">\[\begin{equation}
\max_{\pi_t,c_t}\left\{\mathbb{E}_t[dV^*(t,W_t)+\frac{c_t^{1-\gamma}}{1-\gamma}] dt\right\}=\rho V^*(t,W_t)dt \tag{1.19}
\end{equation}\]</span></p>
<p>现在对于<span class="math inline">\(dV^*\)</span>使用伊藤引理，移走<span class="math inline">\(dZ_t\)</span>的部分因为这是一个鞅，并且将等式两边都除以<span class="math inline">\(dt\)</span>,以产生任意<span class="math inline">\(0\leq t&lt;T\)</span>的HJB方程的偏微分形式</p>
<p><span class="math display" id="eq:8-3">\[
\max_{\pi_t,c_t}\left\{ \frac{\partial V^*}{\partial t}+\frac{\partial V^*}{\partial W_t} \cdot ((\pi_t(\mu-r)+r)W_t-c_t)+\frac{\partial^2 V^*}{\partial W_t^2}\cdot \frac{\pi_t^2\sigma^2W_t^2}{2}+\frac{c_t^{1-\gamma}}{1-\gamma}\right\}=\rho\cdot V^*(t,W_t)  \tag{1.20}
\]</span>
这个HJB方程由下面的终端条件</p>
<p><span class="math display">\[
V^*(T,W_T)=\epsilon^\gamma \cdot\frac{W_T^{1-\gamma}}{1-\gamma}
\]</span></p>
<p>可以将<a href="rlforfin.html#eq:8-3">(1.20)</a>写得更简洁一些：</p>
<p><span class="math display" id="eq:8-4">\[
\max_{\pi_t,c_t} \Phi(t,W_t;\pi_t,c_t)=\rho\cdot V^*(t,W_t) \tag{1.21}
\]</span>
需要强调的是，我们处理的约束条件是<span class="math inline">\(W_t&gt;0,c_t\geq0,0\leq t&lt;T.\)</span><br />
为了找到最优的<span class="math inline">\(\pi_t^*,c_t^*\)</span>，我们求得<span class="math inline">\(\Phi\)</span>的一阶条件得</p>
<p><span class="math display" id="eq:8-6" id="eq:8-5">\[\begin{align}
\pi_t^*&amp;=\frac{-\frac{\partial V^*}{\partial W_t}\cdot (\mu-r)}{\frac{\partial^2 V^*}{\partial W_t^*}\cdot\sigma^2\cdot W_t} \tag{1.22}\\
c_t^*&amp;=\left( \frac{\partial V^*}{\partial W_t} \right)^{-\frac{1}{\gamma}} \tag{1.23}
\end{align}\]</span></p>
<p>下面将<a href="rlforfin.html#eq:8-5">(1.22)</a>和<a href="rlforfin.html#eq:8-6">(1.23)</a>代入到<a href="rlforfin.html#eq:8-3">(1.20)</a>中得到下面的式子，这就给了我们最优价值函数的PDE(显然下面的<span class="math inline">\(w\)</span>应为<span class="math inline">\(W_t\)</span>)：</p>
<p><span class="math display" id="eq:8-7">\[
V^*_t-\frac{(\mu-r)^2}{2\sigma^2}\cdot\frac{(V^*_{w})^2}{V^*_{ww}}+V_w^*\cdot r\cdot w+\frac{\gamma}{1-\gamma}\cdot(V_w^*)^{\frac{\gamma-1}{\gamma}}=\rho\cdot V^* \tag{1.24}
\]</span>
终值条件仍是</p>
<p><span class="math display">\[
V^*(T,W_T)=\epsilon^\gamma \cdot\frac{W_T^{1-\gamma}}{1-\gamma}
\]</span></p>
<p><span class="math inline">\(\Phi\)</span>的二阶条件在以下假设下得以满足<span class="math inline">\(c^*_t&gt;0,W_t&gt;0,\frac{\partial^2V^*}{\partial W_t^2}&lt;0\)</span>对于所有的<span class="math inline">\(0\leq t&lt;T\)</span>,稍后将会证明这些条件在我们推导的解中得到满足，并且<span class="math inline">\(U(\cdot)\)</span>是凹函数，即<span class="math inline">\(\gamma&gt;0.\)</span></p>
<p>接下来我们希望将PDE<a href="rlforfin.html#eq:8-7">(1.24)</a>简化为ODE，这样可以进行简单的求解。为此我们假设解的形式是一个关于时间的确定性函数<span class="math inline">\(f(t):\)</span></p>
<p><span class="math display">\[
V^*(t,W_t)=f(t)^\gamma\cdot\frac{W_t^{1-\gamma}}{1-\gamma}
\]</span></p>
<p>将猜解的形式代入PDE当中，可以得到</p>
<p><span class="math display">\[\begin{align}
&amp;f&#39;(t)=\nu f(t)-1,\\
\text{where }&amp;\nu=\rho-(1-\gamma)\left(\frac{(\mu-r)^2}{2\sigma^2\gamma}+r  \right)
\end{align}\]</span></p>
<p>此时我们注意到遗赠函数<span class="math inline">\(B(T)=\epsilon^\gamma\)</span>对拟合猜测解非常有用，因此该ODE的边界条件为<span class="math inline">\(f(T)=\epsilon\)</span>.由此可以解得</p>
<p><span class="math display">\[\begin{equation}
f(t)=
\begin{cases}
\frac{1+(\nu\epsilon-1)e^{-\nu(T-t)}}{\nu}&amp;\text{ for }\nu\neq 0\\
T-t+\epsilon&amp;\text{ for }\nu=0
\end{cases}
\end{equation}\]</span></p>
<p>于是最优分配策略和消费策略如下</p>
<p><span class="math display" id="eq:8-15" id="eq:8-14">\[\begin{align}
\pi^*(t,W_t)&amp;=\frac{\mu-r}{\sigma^2\gamma} \tag{1.25}\\
c^*(t,W_t)&amp;=
\begin{cases}
\frac{\nu W_t}{1+(\nu\epsilon-1)e^{-\nu(T-t)}} &amp;\text{ for }\nu\neq0\\
\frac{W_t}{T-t+\epsilon}&amp;\text{ for }\nu=0
\end{cases}
\tag{1.26}
\end{align}\]</span></p>
<p>同时有了<span class="math inline">\(f(t)\)</span>的表达式。<span class="math inline">\(V^*(t,W_t)\)</span>的表达式也就迎刃而解了。注意<span class="math inline">\(f(t)&gt;0\)</span>对于所有<span class="math inline">\(0\leq t&lt;T,\forall \nu,\)</span>确保了<span class="math inline">\(W_t&gt;0,c_t^*&gt;0,\frac{\partial^2V^*}{\partial W_t^2}&lt;0\)</span>.这确保了约束条件<span class="math inline">\(W_t&gt;0,c_t\geq0\)</span>得以满足，同时二阶条件得以满足。解决Merton的投资组合问题的一个非常重要的经验教训是HJB公式是关键，这种解法为类似的连续时间随机控制问题提供了模板。</p>
</div>
<div id="merton投资组合问题解的直觉" class="section level3 hasAnchor" number="1.4.3">
<h3><span class="header-section-number">1.4.3</span> Merton投资组合问题解的直觉<a href="rlforfin.html#merton投资组合问题解的直觉" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(\pi^*\)</span>是个常数，代表无论财富如何年龄如何，都该将相同的财富比例投资于风险资产。
而<span class="math inline">\(c^*\)</span>中风险资产的超额回报<span class="math inline">\((\mu-r)\)</span>出现在分子上，<span class="math inline">\(\sigma,\gamma\)</span>出现在坟墓上，波动率越大或者风险厌恶程度更高，自然会减少投资于风险资产，而当我们还年轻时我们希望消费的少一些，但是快死了的时候会增加消费（因为最优策略是死得一贫如洗，假设没有遗产）。</p>
<p>将最优策略<a href="rlforfin.html#eq:8-14">(1.25)</a>和<a href="rlforfin.html#eq:8-15">(1.26)</a>代入财富过程<a href="rlforfin.html#eq:8-1">(1.18)</a>，可以得到进行最优资产配置和最优消费的时候，财富过程为</p>
<p><span class="math display" id="eq:8-17">\[
dW_t^*=(r+\frac{(\mu-r)^2}{\sigma^2\gamma}-\frac{1}{f(t)})\cdot W_t^* \cdot dt+\frac{\mu-r}{\sigma\gamma}\cdot W_t^* \cdot dZ_t \tag{1.27}
\]</span>
是一个对数正态过程，对数正态波动率是常数，对数正态飘逸与财富无关但依赖于时间，我们可以解得</p>
<p><span class="math display">\[
\mathbb{E}[W_t^*]=W_0\cdot e^{(r+\frac{(\mu-r)^2}{\sigma^2\gamma})t}\cdot e^{-\int_0^t\frac{du}{f(u)}}
\]</span></p>
</div>
<div id="离散时间资产配置" class="section level3 hasAnchor" number="1.4.4">
<h3><span class="header-section-number">1.4.4</span> 离散时间资产配置<a href="rlforfin.html#离散时间资产配置" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>在这一节中，将会讨论问题的离散时间版本，这使得问题具有解析可解性。类似于Merton的连续时间投资组合问题，我们在时间<span class="math inline">\(t=0\)</span>是拥有财富<span class="math inline">\(W_0,\)</span>在每个离散时间步长<span class="math inline">\(t=0,1,...,T-1\)</span>时我们可以在没有约束没有交易成本的情况下，将财富<span class="math inline">\(W_t\)</span>分配到风险资产和无风险资产的投资组合中。风险资产的回报在每个时间步长内为常数<span class="math inline">\(r.\)</span></p>
<p>我们假设在任何<span class="math inline">\(t&lt;T\)</span>时没有消费财富并且在时间<span class="math inline">\(T\)</span>时会清算并消费财富<span class="math inline">\(W_T.\)</span>因此我们的目标是通过在每个<span class="math inline">\(t=0,1,...,T-1\)</span>时动态地分配风险资产<span class="math inline">\(x_t\in\mathbb{R}\)</span>和剩余的<span class="math inline">\(W_t-x_t\)</span>无风险资产，最大化最终时间步<span class="math inline">\(t=T\)</span>时的期望财富效用。假设单步折现因子为<span class="math inline">\(\gamma\)</span>,最终时间步<span class="math inline">\(T\)</span>的财富效用由以下CARA函数给出</p>
<p><span class="math display">\[
U(W_T)=\frac{1-e^{-aW_T}}{a}\text{ for some fixed } a\neq 0
\]</span>
因此问题变成了在每个<span class="math inline">\(t=0,1,...,T_1\)</span>时通过选择<span class="math inline">\(x_t\)</span>来最大化</p>
<p><span class="math display">\[
\mathbb{E} \left[\gamma^{T-t}\cdot \frac{1-e^{-aW_T}}{a}\mid (t,W_t)\right]
\]</span>
等价于最大化</p>
<p><span class="math display" id="eq:8-19">\[
\mathbb{E} \left[-\frac{e^{-aW_T}}{a}\mid (t,W_t)\right] \tag{1.28}
\]</span>
我们将这个问题表述为一个连续状态和连续动作的离散时间有限时域MDP，并准确指定其转移状态、奖励和折现因子，然后我们的目标是求解MDP控制问题找到最优策略。</p>
<p>有限时域MDP的终止时间为<span class="math inline">\(T\)</span>,因此所有时间<span class="math inline">\(t=T\)</span>的状态都是终止状态。时间步<span class="math inline">\(t=0,1,...,T\)</span>的状态<span class="math inline">\(s_t\in\mathcal{S}_t\)</span>包含财富<span class="math inline">\(W_t.\)</span>决策<span class="math inline">\(a_t\in\mathcal{A}_t\)</span>是对风险投资的投资量<span class="math inline">\(x_t,\)</span>因此每个时间步对无风险投资的投资量为<span class="math inline">\(W_t-x_t.\)</span>时间步<span class="math inline">\(t\)</span>的确定性策略<span class="math inline">\(\pi_t\)</span>记为<span class="math inline">\(\pi_t(W_t)=x_t,\)</span>同样，时间步<span class="math inline">\(t\)</span>时最优确定性策略<span class="math inline">\(\pi_t^*\)</span>记为<span class="math inline">\(\pi_t^*(W_t)=x^*_t.\)</span></p>
<p>我们将<span class="math inline">\(t\)</span>到<span class="math inline">\(t+1\)</span>的风险资产单步回报的随机变量记为<span class="math inline">\(Y_t\sim N(\mu,\sigma^2)\)</span>对于所有的<span class="math inline">\(t=0,1,...,T-1.\)</span>因此</p>
<p><span class="math display" id="eq:8-20">\[
W_{t+1}=x_t\cdot(1+Y_t)+(W_t-x_t)\cdot (1+r)=x_t\cdot(Y_t-r)+W_t\cdot(1+r) \tag{1.29}
\]</span></p>
<p>MDP的奖励在每个<span class="math inline">\(t=0,1,...,T-1\)</span>时为0，因此基于上述简化的目标<a href="rlforfin.html#eq:8-19">(1.28)</a>,MDP在<span class="math inline">\(t=T\)</span>时的奖励设置为随机量<span class="math inline">\(\frac{-e^{aW_T}}{a}.\)</span>我们将MDP的折现因子设置为<span class="math inline">\(\gamma=1,\)</span>在时间<span class="math inline">\(t\)</span>时的价值函数（给定策略<span class="math inline">\(\pi=(\pi_0,\pi_1,...,\pi_{T-1})\)</span>）记为</p>
<p><span class="math display">\[
V_t^\pi(W_t)=\mathbb{E}_\pi\left[ -\frac{e^{-aW_T}}{a}\mid (t,W_t) \right]
\]</span>
在时间<span class="math inline">\(t\)</span>时的最优价值函数记为</p>
<p><span class="math display">\[
V_t^*(W_t)=\max_{\pi} V_t^\pi(W_t)=\max_\pi\left\{\mathbb{E}_\pi\left[ -\frac{e^{-aW_T}}{a}\mid (t,W_t) \right] \right\}
\]</span>
贝尔曼最优方程为(当<span class="math inline">\(t=0,1,...,T-2\)</span>时)</p>
<p><span class="math display">\[
V_t^*(W_t)=\max_{x_t}Q_t^*(W_t,x_t)=\max_{x_t}\left\{\mathbb{E}_{Y_t\sim N(\mu,\sigma^2)}\left[ V_{t+1}^*(W_{t+1}) \right] \right\}
\]</span>
且</p>
<p><span class="math display">\[
V_{T-1}^*(W_{T-1})=\max_{x_{T-1}}Q_{T-1}^*(W_{T-1},x_{T-1})=\max_{x_{T-1}}\left\{\mathbb{E}_{Y_{T-1}\sim N(\mu,\sigma^2)}\left[ \frac{-e^{-aW_T}}{a} \right] \right\}
\]</span></p>
<p>其中<span class="math inline">\(Q_t^*\)</span>是时间<span class="math inline">\(t\)</span>时的最优动作价值函数。</p>
<p>我们通过对最优价值函数的形式进行合理猜测，得到</p>
<p><span class="math display" id="eq:8-21">\[
V_t^*(W_t)=-b_t e^{-c_tW_t} \tag{1.30}
\]</span>
其中<span class="math inline">\(b_t,c_t\)</span>与财富<span class="math inline">\(W_t\)</span>无关，接下来我们使用这个最优价值函数的形式来表达贝尔曼最优性方程</p>
<p><span class="math display">\[
V_t^*(W_t)=\max_{x_t}\left\{\mathbb{E}_{Y_t\sim N(\mu,\sigma^2)}\left[ -b_{t+1}e^{-c_{t+1}W_{t+1}} \right] \right\}
\]</span>
利用公式<a href="rlforfin.html#eq:8-20">(1.29)</a>可以将其改写为</p>
<p><span class="math display">\[
V_t^*(W_t)=\max_{x_t}\left\{\mathbb{E}_{Y_t\sim N(\mu,\sigma^2)}\left[ -b_{t+1}e^{-c_{t+1}(x_t(Y_t-r)+W_t(1+r))} \right] \right\}
\]</span></p>
<p>这个指数形式的期望值在正态分布下计算结果为</p>
<p><span class="math display" id="eq:8-22">\[
V_t^*(W_t)=\max_{x_t}\left\{-b_{t+1}\exp\left\{-c_{t+1}(1+r)W_t-c_{t+1}(\mu-r)x_t+c_{t+1}^2 \frac{\sigma^2}{2} x_{t}^2\right\} \right\} \tag{1.31}
\]</span></p>
<p>又由于<span class="math inline">\(V_{t}^*(W_t)=\max_{x_t}Q^*_t(W_t,x_t),\)</span>从上面的公式中可以推断出最优动作价值函数<span class="math inline">\(Q^*_t(W_t,x_t)\)</span>的函数形式为</p>
<p><span class="math display" id="eq:8-23">\[
Q^*_t(W_t,x_t)=-b_{t+1}\exp\left\{-c_{t+1}(1+r)W_t-c_{t+1}(\mu-r)x_t+c_{t+1}^2 \frac{\sigma^2}{2} x_{t}^2\right\} \tag{1.32}
\]</span></p>
<p>由于贝尔曼最优性方程<a href="rlforfin.html#eq:8-22">(1.31)</a>右侧涉及对<span class="math inline">\(x_t\)</span>的最大化操作，我们可以认为最大化操作内的项对于<span class="math inline">\(x_t\)</span>的偏导数是0，这使得我们能够将最优分配<span class="math inline">\(x_t^*\)</span>表示为<span class="math inline">\(c_{t+1}\)</span>的函数，如下所示</p>
<p><span class="math display">\[
-c_{t+1}(\mu-r)+\sigma^2c_{t+1}^2x_t^*=0
\]</span>
即</p>
<p><span class="math display" id="eq:8-24">\[
x_t^*=\frac{\mu-r}{\sigma^2 c_{t+1}} \tag{1.33}
\]</span>
代入贝尔曼最优性方程<a href="rlforfin.html#eq:8-22">(1.31)</a>可得</p>
<p><span class="math display">\[
V_{t}^*(W_t)=-b_{t+1}\exp\left\{ -c_{t+1}(1+r)W_t-\frac{(\mu-r)^2}{2\sigma^2}\right\}
\]</span>
但由于</p>
<p><span class="math display">\[
V_{t}^*(W_t)=-b_t\exp\{-c_tW_t\}
\]</span>
就可以得到如下递推方程</p>
<p><span class="math display">\[
b_t=b_{t+1}\exp\left\{-\frac{(\mu-r)^2}{2\sigma^2}\right\},\quad  c_t=c_{t+1}(1+r)
\]</span>
我们还可以通过知道在<span class="math inline">\(t+T\)</span>终端时刻的MDP的奖励<span class="math inline">\(\frac{-e^{-aW_T}}{a}\)</span>即终端时刻的财富效用来计算<span class="math inline">\(b_{T-1},c_{T-1},\)</span>然后递推得到<span class="math inline">\(b_t,c_t\)</span>的值。</p>
<p>在<span class="math inline">\(t=T-1\)</span>时：</p>
<p><span class="math display">\[
V_{T-1}^*(W_{T-1})=\max_{x_{T-1}}\left\{\mathbb{E}_{Y_{T-1}\sim N(\mu,\sigma^2)}\left[ \frac{-e^{-aW_T}}{a} \right] \right\}
\]</span>
通过公式<a href="rlforfin.html#eq:8-20">(1.29)</a>可以写为</p>
<p><span class="math display">\[
V_{T-1}^*(W_{T-1})=\max_{x_{T-1}}\left\{\mathbb{E}_{Y_{T-1}\sim N(\mu,\sigma^2)}\left[ \frac{-\exp\{-a(x_{T-1}(Y_{T-1}-r)+W_{T-1}(1+r))\}}{a} \right] \right\}
\]</span></p>
<p>利用矩母函数的相关知识可以将其改写为</p>
<p><span class="math display">\[
V_{T-1}^*(W_{T-1})=-e^{-\frac{(\mu-r)^2}{2\sigma^2}}-\frac{a(1+r)W_{T-1}}{a}
\]</span>
因此</p>
<p><span class="math display">\[
b_{T-1}=\frac{e^{-\frac{(\mu-r)^2}{2\sigma^2}}}{a},\quad c_{T-1}=a(1+r)
\]</span>
递推得到当<span class="math inline">\(t=0,1,...,T-2\)</span>时（当然<span class="math inline">\(t=T-1\)</span>也满足下面的递推式）</p>
<p><span class="math display">\[
b_t=\frac{e^{-\frac{(\mu-r)^2(T-t)}{2\sigma^2}}}{a},\quad c_t=a(1+r)^{T-t}
\]</span></p>
<p>将<span class="math inline">\(c_{t+1}\)</span>带入到<a href="rlforfin.html#eq:8-24">(1.33)</a>即可得到最优策略的解</p>
<p><span class="math display" id="eq:8-25">\[
\pi_t^*(W_t)=x_t^*=\frac{\mu-r}{\sigma^2a(1+r)^{T-t-1}},\text{ for all }t=0,1,...,T-1  \tag{1.34}
\]</span>
请注意最优策略在时间步<span class="math inline">\(t\)</span>时不依赖于状态<span class="math inline">\(W_t.\)</span>因此对于固定时间的最优策略<span class="math inline">\(\pi_t^*\)</span>是一个常数确定性的策略函数。</p>
<p>将<span class="math inline">\(b_t,c_t\)</span>的解代入公式<a href="rlforfin.html#eq:8-21">(1.30)</a>即可得到最优价值函数的解：</p>
<p><span class="math display" id="eq:8-26">\[
V_t^*(W_t)=\frac{-e^{-\frac{(\mu-r)^2(T-t)}{2\sigma^2}}}{a}\cdot e^{-a(1+r)^{T-t}W_t} \text{ for all }t=0,1,...,T-1 \tag{1.35}
\]</span>
将<span class="math inline">\(b_{t+1},c_{t+1}\)</span>代入到公式<a href="rlforfin.html#eq:8-23">(1.32)</a>可以得到最优动作价值函数的解（对于所有的<span class="math inline">\(t=0,1,...,T-1\)</span>）</p>
<p><span class="math display">\[
Q^*_t(W_t,x_t)=\\
\frac{-e^{\frac{-(\mu-r)^2(T-t-1)}{2\sigma^2}}}{a}\exp\left\{-a(1+r)^{T-t}W_t-a(\mu-r)(1+r)^{T-t-1}x_t+\frac{(a\sigma(1+r)^{T-t-1})^2}{2}x_t^2  \right\}
\]</span></p>
</div>
<div id="现实世界的应用" class="section level3 hasAnchor" number="1.4.5">
<h3><span class="header-section-number">1.4.5</span> 现实世界的应用<a href="rlforfin.html#现实世界的应用" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>上面的讨论和设置通过简化假设提供了分析的可解性。使得问题具有解析可解性的具体简化假设包括：</p>
<ul>
<li><p>资产回报的正态分布</p></li>
<li><p>CRRA/CARA假设</p></li>
<li><p>无摩擦市场交易（没有交易成本没有约束，价格/分配数量/消费都是连续的）</p></li>
</ul>
<p>但是，现实世界中的动态资产配置和消费问题并不像我们讨论的那么简单和清晰。实际的资产价格波动更加复杂，效用函数不一定符合简单的 CRRA/CARA 形式。实际上，交易通常发生在离散空间中——资产价格、分配数量和消费量往往是离散的。此外，当我们改变资产配置或清算部分投资组合以进行消费时，会产生交易成本。更重要的是，交易并不总是在连续时间内进行——通常会有特定的交易窗口或交易限制。最后，许多投资是流动性差的（如房地产），或者在特定的时间之前无法清算（如退休基金），这对从投资组合中提取资金进行消费构成了重大约束。因此，尽管价格、分配数量和消费可能接近连续变量，但上述摩擦使得我们无法在简化示例中那样利用微积分的便利。</p>
<p>考虑到上述现实世界的因素，我们需要利用动态规划——更具体地说，利用近似动态规划，因为现实世界的问题通常涉及大规模的状态空间和行动空间（即使这些空间不是连续的，它们通常接近连续）。对价值函数的适当函数逼近是解决这些问题的关键。实现一个完整的现实世界投资和消费管理系统超出了本书的范围，但我们可以实现一个具有足够理解的示例，展示如何实现一个完整的现实世界应用。我们要实现的设置包括：</p>
<ul>
<li><p>一个风险资产和一个无风险资产；</p></li>
<li><p>有限时间步数（类似于上一节的离散时间设置）；</p></li>
<li><p>在有限时间结束之前不进行消费（即不从投资组合中提取资金），因此折现因子设置为1；</p></li>
<li><p>风险资产回报的任意分布；</p></li>
<li><p>随时间变化的无风险资产回报；</p></li>
<li><p>任意效用函数；</p></li>
<li><p>在每个时间步，有限数量的风险资产投资选择；</p></li>
<li><p>初始财富的任意分布。</p></li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="关于涛哥.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["CBook.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection",
"toc_collapsed": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
