<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK</title>
  <meta name="description" content="这是用R的bookdown功能制作的课堂笔记。" />
  <meta name="generator" content="bookdown 0.42 and GitBook 2.6.7" />

  <meta property="og:title" content="1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="这是用R的bookdown功能制作的课堂笔记。" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 强化学习在金融中的应用 | GOGENTLE’s NOTEBOOK" />
  
  <meta name="twitter:description" content="这是用R的bookdown功能制作的课堂笔记。" />
  

<meta name="author" content="gogentle" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="关于涛哥.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  jax: ["input/TeX","output/SVG"],
  extensions: ["tex2jax.js","MathMenu.js","MathZoom.js"],
  TeX: {
    extensions: ["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]
  }
});
</script>
<script type="text/javascript"
   src="../../../MathJax/MathJax.js">
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">GZT FOR REAL</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>写在前面</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#为什么要写课程笔记"><i class="fa fa-check"></i>为什么要写课程笔记</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="关于涛哥.html"><a href="关于涛哥.html"><i class="fa fa-check"></i>关于涛哥</a></li>
<li class="part"><span><b>I 课程笔记</b></span></li>
<li class="chapter" data-level="1" data-path="rlforfin.html"><a href="rlforfin.html"><i class="fa fa-check"></i><b>1</b> 强化学习在金融中的应用</a>
<ul>
<li class="chapter" data-level="1.1" data-path="rlforfin.html"><a href="rlforfin.html#Markov"><i class="fa fa-check"></i><b>1.1</b> Markov过程</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="rlforfin.html"><a href="rlforfin.html#过程中的状态概念"><i class="fa fa-check"></i><b>1.1.1</b> 过程中的状态概念</a></li>
<li class="chapter" data-level="1.1.2" data-path="rlforfin.html"><a href="rlforfin.html#通过股票价格的例子理解markov性"><i class="fa fa-check"></i><b>1.1.2</b> 通过股票价格的例子理解Markov性</a></li>
<li class="chapter" data-level="1.1.3" data-path="rlforfin.html"><a href="rlforfin.html#markov过程的正式定义"><i class="fa fa-check"></i><b>1.1.3</b> Markov过程的正式定义</a></li>
<li class="chapter" data-level="1.1.4" data-path="rlforfin.html"><a href="rlforfin.html#markov过程的稳态分布"><i class="fa fa-check"></i><b>1.1.4</b> Markov过程的稳态分布</a></li>
<li class="chapter" data-level="1.1.5" data-path="rlforfin.html"><a href="rlforfin.html#markov奖励过程的形式主义"><i class="fa fa-check"></i><b>1.1.5</b> Markov奖励过程的形式主义</a></li>
<li class="chapter" data-level="1.1.6" data-path="rlforfin.html"><a href="rlforfin.html#markov奖励过程的价值函数"><i class="fa fa-check"></i><b>1.1.6</b> Markov奖励过程的价值函数</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="rlforfin.html"><a href="rlforfin.html#MDP"><i class="fa fa-check"></i><b>1.2</b> Markov决策过程</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="rlforfin.html"><a href="rlforfin.html#不确定性下的序列决策难题"><i class="fa fa-check"></i><b>1.2.1</b> 不确定性下的序列决策难题</a></li>
<li class="chapter" data-level="1.2.2" data-path="rlforfin.html"><a href="rlforfin.html#markov决策过程的正式定义"><i class="fa fa-check"></i><b>1.2.2</b> Markov决策过程的正式定义</a></li>
<li class="chapter" data-level="1.2.3" data-path="rlforfin.html"><a href="rlforfin.html#策略"><i class="fa fa-check"></i><b>1.2.3</b> 策略</a></li>
<li class="chapter" data-level="1.2.4" data-path="rlforfin.html"><a href="rlforfin.html#markov决策过程策略-markov奖励过程"><i class="fa fa-check"></i><b>1.2.4</b> [Markov决策过程，策略]:= Markov奖励过程</a></li>
<li class="chapter" data-level="1.2.5" data-path="rlforfin.html"><a href="rlforfin.html#固定策略下的mdp价值函数"><i class="fa fa-check"></i><b>1.2.5</b> 固定策略下的MDP价值函数</a></li>
<li class="chapter" data-level="1.2.6" data-path="rlforfin.html"><a href="rlforfin.html#最优价值函数和最优策略"><i class="fa fa-check"></i><b>1.2.6</b> 最优价值函数和最优策略</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="rlforfin.html"><a href="rlforfin.html#dynamicprogram"><i class="fa fa-check"></i><b>1.3</b> 动态规划算法</a></li>
<li class="chapter" data-level="1.4" data-path="rlforfin.html"><a href="rlforfin.html#dynassall"><i class="fa fa-check"></i><b>1.4</b> 动态资产配置和消费</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="rlforfin.html"><a href="rlforfin.html#个人财务的优化"><i class="fa fa-check"></i><b>1.4.1</b> 个人财务的优化</a></li>
<li class="chapter" data-level="1.4.2" data-path="rlforfin.html"><a href="rlforfin.html#merton投资组合问题及其解决"><i class="fa fa-check"></i><b>1.4.2</b> Merton投资组合问题及其解决</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>2</b> 广义线性模型</a>
<ul>
<li class="chapter" data-level="2.1" data-path="glm.html"><a href="glm.html#glmexp"><i class="fa fa-check"></i><b>2.1</b> 广义线性模型的指数族分布</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="RFWLFR.html"><a href="RFWLFR.html"><i class="fa fa-check"></i><b>3</b> Random forest weighted local Frechet regression with random objects</a>
<ul>
<li class="chapter" data-level="3.1" data-path="RFWLFR.html"><a href="RFWLFR.html#rfwlfr2"><i class="fa fa-check"></i><b>3.1</b> 提出的方法</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="RFWLFR.html"><a href="RFWLFR.html#rfwlfr21"><i class="fa fa-check"></i><b>3.1.1</b> 预备知识</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="qrm.html"><a href="qrm.html"><i class="fa fa-check"></i><b>4</b> 量化风险管理</a>
<ul>
<li class="chapter" data-level="4.1" data-path="qrm.html"><a href="qrm.html#rmconcept"><i class="fa fa-check"></i><b>4.1</b> 风险管理的基本概念</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="qrm.html"><a href="qrm.html#建模价值和价值变动"><i class="fa fa-check"></i><b>4.1.1</b> 建模价值和价值变动</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="stochastic.html"><a href="stochastic.html"><i class="fa fa-check"></i><b>5</b> 随机分析</a>
<ul>
<li class="chapter" data-level="5.1" data-path="stochastic.html"><a href="stochastic.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="stochastic.html"><a href="stochastic.html#最优停时optimal-stopping"><i class="fa fa-check"></i><b>5.1.1</b> 1.4 最优停时（Optimal Stopping）</a></li>
<li class="chapter" data-level="5.1.2" data-path="stochastic.html"><a href="stochastic.html#随机控制stochastic-control"><i class="fa fa-check"></i><b>5.1.2</b> 1.5 随机控制（Stochastic Control）</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="stochastic.html"><a href="stochastic.html#预备知识"><i class="fa fa-check"></i><b>5.2</b> 预备知识</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="stochastic.html"><a href="stochastic.html#概率空间-随机变量-随机过程"><i class="fa fa-check"></i><b>5.2.1</b> 概率空间 随机变量 随机过程</a></li>
<li class="chapter" data-level="5.2.2" data-path="stochastic.html"><a href="stochastic.html#布朗运动brownian-motion"><i class="fa fa-check"></i><b>5.2.2</b> 布朗运动（Brownian Motion）</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II 语言基础</b></span></li>
<li class="chapter" data-level="6" data-path="english.html"><a href="english.html"><i class="fa fa-check"></i><b>6</b> 英语</a>
<ul>
<li class="chapter" data-level="6.1" data-path="english.html"><a href="english.html#week2"><i class="fa fa-check"></i><b>6.1</b> week2</a></li>
</ul></li>
<li class="part"><span><b>III 讨论班报告</b></span></li>
<li class="chapter" data-level="7" data-path="cdc.html"><a href="cdc.html"><i class="fa fa-check"></i><b>7</b> Optimal strategies for collective defined contribution plans when the stock and labor markets are co-integrated（股票和劳动力市场协同整合时集体确定缴款计划的最优策略）</a>
<ul>
<li class="chapter" data-level="7.1" data-path="cdc.html"><a href="cdc.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="cdc.html"><a href="cdc.html#相关工作"><i class="fa fa-check"></i><b>7.1.1</b> 相关工作</a></li>
<li class="chapter" data-level="7.1.2" data-path="cdc.html"><a href="cdc.html#主要区别"><i class="fa fa-check"></i><b>7.1.2</b> 主要区别</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="cdc.html"><a href="cdc.html#模型的公式化"><i class="fa fa-check"></i><b>7.2</b> 模型的公式化</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="cdc.html"><a href="cdc.html#金融市场"><i class="fa fa-check"></i><b>7.2.1</b> 金融市场</a></li>
<li class="chapter" data-level="7.2.2" data-path="cdc.html"><a href="cdc.html#劳动收入"><i class="fa fa-check"></i><b>7.2.2</b> 劳动收入</a></li>
<li class="chapter" data-level="7.2.3" data-path="cdc.html"><a href="cdc.html#养老金系统"><i class="fa fa-check"></i><b>7.2.3</b> 养老金系统</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="cdc.html"><a href="cdc.html#养老金基金计划的最优策略"><i class="fa fa-check"></i><b>7.3</b> 养老金基金计划的最优策略</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="cdc.html"><a href="cdc.html#具有特质性冲击的最优策略"><i class="fa fa-check"></i><b>7.3.1</b> 具有特质性冲击的最优策略</a></li>
<li class="chapter" data-level="7.3.2" data-path="cdc.html"><a href="cdc.html#无特质性冲击时的最优策略"><i class="fa fa-check"></i><b>7.3.2</b> 无特质性冲击时的最优策略</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="cdc.html"><a href="cdc.html#数值分析"><i class="fa fa-check"></i><b>7.4</b> 数值分析</a></li>
<li class="chapter" data-level="7.5" data-path="cdc.html"><a href="cdc.html#总结"><i class="fa fa-check"></i><b>7.5</b> 总结</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="mulreinsurance.html"><a href="mulreinsurance.html"><i class="fa fa-check"></i><b>8</b> 最优多维再保险与多元风险厌恶效用</a>
<ul>
<li class="chapter" data-level="8.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#OMR"><i class="fa fa-check"></i><b>8.1</b> 共同冲击依赖结构下的最优多维再保险政策</a>
<ul>
<li class="chapter" data-level="8.1.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#common-shock-model"><i class="fa fa-check"></i><b>8.1.1</b> Common shock model</a></li>
<li class="chapter" data-level="8.1.2" data-path="mulreinsurance.html"><a href="mulreinsurance.html#hjb-equation"><i class="fa fa-check"></i><b>8.1.2</b> HJB equation</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="mulreinsurance.html"><a href="mulreinsurance.html#mrau"><i class="fa fa-check"></i><b>8.2</b> 应用于ESG投资的多元风险厌恶效用</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="mulreinsurance.html"><a href="mulreinsurance.html#setting-and-theoretical-results"><i class="fa fa-check"></i><b>8.2.1</b> Setting and theoretical results</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>IV 课外学习笔记</b></span></li>
<li class="chapter" data-level="9" data-path="datawhale.html"><a href="datawhale.html"><i class="fa fa-check"></i><b>9</b> Datawhale Quant</a>
<ul>
<li class="chapter" data-level="9.1" data-path="datawhale.html"><a href="datawhale.html#investquant"><i class="fa fa-check"></i><b>9.1</b> 投资与量化投资</a></li>
<li class="chapter" data-level="9.2" data-path="datawhale.html"><a href="datawhale.html#finmarket"><i class="fa fa-check"></i><b>9.2</b> 金融市场的基本概念</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="datawhale.html"><a href="datawhale.html#monetaryfin"><i class="fa fa-check"></i><b>9.2.1</b> 货币金融学</a></li>
<li class="chapter" data-level="9.2.2" data-path="datawhale.html"><a href="datawhale.html#investfin"><i class="fa fa-check"></i><b>9.2.2</b> 投资学</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="datawhale.html"><a href="datawhale.html#stockdataget"><i class="fa fa-check"></i><b>9.3</b> 股票数据获取</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="datawhale.html"><a href="datawhale.html#股票数据常见指标介绍"><i class="fa fa-check"></i><b>9.3.1</b> 股票数据常见指标介绍</a></li>
<li class="chapter" data-level="9.3.2" data-path="datawhale.html"><a href="datawhale.html#baostock的基础数据获取"><i class="fa fa-check"></i><b>9.3.2</b> Baostock的基础数据获取</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="datawhale.html"><a href="datawhale.html#quantselect"><i class="fa fa-check"></i><b>9.4</b> 量化选股策略</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="datawhale.html"><a href="datawhale.html#fctsctm"><i class="fa fa-check"></i><b>9.4.1</b> 因子选股模型</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="causal.html"><a href="causal.html"><i class="fa fa-check"></i><b>10</b> 格兰格因果性</a>
<ul>
<li class="chapter" data-level="10.1" data-path="causal.html"><a href="causal.html#causal-intro"><i class="fa fa-check"></i><b>10.1</b> 介绍</a></li>
<li class="chapter" data-level="10.2" data-path="causal.html"><a href="causal.html#causal-def"><i class="fa fa-check"></i><b>10.2</b> 格兰格因果性的定义</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="usage.html"><a href="usage.html"><i class="fa fa-check"></i><b>11</b> 中文图书Bookdown模板的基本用法</a>
<ul>
<li class="chapter" data-level="11.1" data-path="usage.html"><a href="usage.html#usage-ins"><i class="fa fa-check"></i><b>11.1</b> 安装设置</a></li>
<li class="chapter" data-level="11.2" data-path="usage.html"><a href="usage.html#usage-writing"><i class="fa fa-check"></i><b>11.2</b> 编写自己的内容</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="usage.html"><a href="usage.html#usage-writing-struct"><i class="fa fa-check"></i><b>11.2.1</b> 文档结构</a></li>
<li class="chapter" data-level="11.2.2" data-path="usage.html"><a href="usage.html#usage-writing-fig"><i class="fa fa-check"></i><b>11.2.2</b> 图形自动编号</a></li>
<li class="chapter" data-level="11.2.3" data-path="usage.html"><a href="usage.html#usage-writing-tab"><i class="fa fa-check"></i><b>11.2.3</b> 表格自动编号</a></li>
<li class="chapter" data-level="11.2.4" data-path="usage.html"><a href="usage.html#usage-writing-math"><i class="fa fa-check"></i><b>11.2.4</b> 数学公式编号</a></li>
<li class="chapter" data-level="11.2.5" data-path="usage.html"><a href="usage.html#文献引用与文献列表"><i class="fa fa-check"></i><b>11.2.5</b> 文献引用与文献列表</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="usage.html"><a href="usage.html#usage-output"><i class="fa fa-check"></i><b>11.3</b> 转换</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="usage.html"><a href="usage.html#usage-gitbook"><i class="fa fa-check"></i><b>11.3.1</b> 转换为网页</a></li>
<li class="chapter" data-level="11.3.2" data-path="usage.html"><a href="usage.html#usage-pdfbook"><i class="fa fa-check"></i><b>11.3.2</b> 生成PDF</a></li>
<li class="chapter" data-level="11.3.3" data-path="usage.html"><a href="usage.html#usage-website"><i class="fa fa-check"></i><b>11.3.3</b> 上传到网站</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://go9entle.github.io" target="_blank">Proudly Presented by GZT</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">GOGENTLE’s NOTEBOOK</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rlforfin" class="section level1 hasAnchor" number="1">
<h1><span class="header-section-number">1</span> 强化学习在金融中的应用<a href="rlforfin.html#rlforfin" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="Markov" class="section level2 hasAnchor" number="1.1">
<h2><span class="header-section-number">1.1</span> Markov过程<a href="rlforfin.html#Markov" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>本书的主题是“序列不确定下的序列决策”，在本章中将暂时忽略“序列决策”方面而只关注”序列不确定性“。</p>
<div id="过程中的状态概念" class="section level3 hasAnchor" number="1.1.1">
<h3><span class="header-section-number">1.1.1</span> 过程中的状态概念<a href="rlforfin.html#过程中的状态概念" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(S_t\)</span>是过程在时间<span class="math inline">\(t\)</span>时的状态。特别地，我们对于下一时刻的状态<span class="math inline">\(S_{t+1}\)</span>的概率感兴趣，如果已知现在的状态<span class="math inline">\(S_t\)</span>和过去的状态<span class="math inline">\(S_0,S_1,...,S_{t-1}\)</span>，我们对<span class="math inline">\(P\{S_{t+1}|S_t,S_{t-1},...,S_0\}\)</span>感兴趣。</p>
</div>
<div id="通过股票价格的例子理解markov性" class="section level3 hasAnchor" number="1.1.2">
<h3><span class="header-section-number">1.1.2</span> 通过股票价格的例子理解Markov性<a href="rlforfin.html#通过股票价格的例子理解markov性" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>为了帮助理解，我们假设股票价格只取整数值，并且零或负股票价格是可以接受的。我们将时间<span class="math inline">\(t\)</span>的股票价格表示为<span class="math inline">\(X_t\)</span>.假设从时间<span class="math inline">\(t\)</span> 到下一个时间步骤 <span class="math inline">\(t + 1\)</span>,股票价格可以上涨<span class="math inline">\(1\)</span>或下跌<span class="math inline">\(1\)</span>,即<span class="math inline">\(X_{t+1}\)</span>的唯一两个结果是<span class="math inline">\(X_t + 1\)</span>或<span class="math inline">\(X_t − 1\)</span>.要了解股票价格随时间的随机演变，我们只需要量化上涨的概率 <span class="math inline">\(P[X_{t+1} =X_t+ 1]\)</span>.我们将考虑股票价格演变的 3 个不同过程。</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(P[X_{t+1}=X_t+1]=\frac{1}{1+e^{-\alpha_1(L-X_t)}}\)</span>.</p>
<p>这意味着股票的价格倾向于均值回归(mean-reverting),均值即为参考水平<span class="math inline">\(L,\)</span>拉力系数为<span class="math inline">\(\alpha.\)</span></p>
<p>我们不妨设<span class="math inline">\(S_t=X_t,\)</span>且可以看到下一时刻的状态<span class="math inline">\(S_{t+1}\)</span>只与<span class="math inline">\(S_t\)</span>有关而与<span class="math inline">\(S_0,S_1,...,S_{t-1}\)</span>无关。即可写作
<span class="math display">\[
P[S_{t+1}|S_t,S_{t-1},...,S_0]=P[S_{t+1}|S_t]\text{ for all }t\geq 0.
\]</span>
这就被成为Markov性。</p>
<p>书中还给出了相应的代码</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="rlforfin.html#cb3-1" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb3-2"><a href="rlforfin.html#cb3-2" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-3"><a href="rlforfin.html#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a href="rlforfin.html#cb3-4" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb3-5"><a href="rlforfin.html#cb3-5" tabindex="-1"></a><span class="kw">class</span> Process1:</span>
<span id="cb3-6"><a href="rlforfin.html#cb3-6" tabindex="-1"></a>    <span class="at">@dataclass</span></span>
<span id="cb3-7"><a href="rlforfin.html#cb3-7" tabindex="-1"></a>    <span class="kw">class</span> State:</span>
<span id="cb3-8"><a href="rlforfin.html#cb3-8" tabindex="-1"></a>        price: <span class="bu">int</span></span>
<span id="cb3-9"><a href="rlforfin.html#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a href="rlforfin.html#cb3-10" tabindex="-1"></a>    level_param: <span class="bu">int</span>  <span class="co"># level to which price mean-reverts</span></span>
<span id="cb3-11"><a href="rlforfin.html#cb3-11" tabindex="-1"></a>    alpha1: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.25</span>  <span class="co"># strength of mean-reversion (non-negative value)</span></span>
<span id="cb3-12"><a href="rlforfin.html#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a href="rlforfin.html#cb3-13" tabindex="-1"></a>    <span class="kw">def</span> up_prob(<span class="va">self</span>, state: State) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb3-14"><a href="rlforfin.html#cb3-14" tabindex="-1"></a>        <span class="cf">return</span> <span class="fl">1.</span><span class="op">/</span>(<span class="dv">1</span><span class="op">+</span>np.exp(<span class="op">-</span><span class="va">self</span>.alpha1<span class="op">*</span>(<span class="va">self</span>.level_param<span class="op">-</span>state.price)))</span>
<span id="cb3-15"><a href="rlforfin.html#cb3-15" tabindex="-1"></a>    <span class="kw">def</span> next_state(<span class="va">self</span>, state:State) <span class="op">-&gt;</span> State:</span>
<span id="cb3-16"><a href="rlforfin.html#cb3-16" tabindex="-1"></a>        up_move: <span class="bu">int</span> <span class="op">=</span> np.random.binomial(<span class="dv">1</span>, <span class="va">self</span>.up_prob(state),<span class="dv">1</span>)[<span class="dv">0</span>] <span class="co">#生成随机移动 up_move = 0 or 1</span></span>
<span id="cb3-17"><a href="rlforfin.html#cb3-17" tabindex="-1"></a>        <span class="cf">return</span> Process1.State(price<span class="op">=</span>state.price <span class="op">+</span> up_move <span class="op">*</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>) <span class="co"># 若up_move = 1, 则价格上升1，若为0价格下降1</span></span></code></pre></div>
<p>接下来，我们使用 Python 的生成器功能（使用<code>yield</code>）编写一个简单的模拟器，如下所示：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="rlforfin.html#cb4-1" tabindex="-1"></a><span class="kw">def</span> simulation(process, start_state):</span>
<span id="cb4-2"><a href="rlforfin.html#cb4-2" tabindex="-1"></a>    state <span class="op">=</span> start_state</span>
<span id="cb4-3"><a href="rlforfin.html#cb4-3" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb4-4"><a href="rlforfin.html#cb4-4" tabindex="-1"></a>        <span class="cf">yield</span> state</span>
<span id="cb4-5"><a href="rlforfin.html#cb4-5" tabindex="-1"></a>        state <span class="op">=</span> process.next_state(state)</span></code></pre></div>
<p>现在我们可以使用此模拟器函数生成采样轨迹。在下面的代码中，我们从 <code>start_price</code>的价格<span class="math inline">\(X_0\)</span>​开始，在<code>time_steps</code>时间步长内生成<code>num_traces</code>个采样轨迹。使用 Python 的生成器功能，我们可以使用<code>itertools.islice</code>函数“懒惰地”执行此操作。</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="rlforfin.html#cb5-1" tabindex="-1"></a><span class="im">import</span> itertools</span>
<span id="cb5-2"><a href="rlforfin.html#cb5-2" tabindex="-1"></a><span class="kw">def</span> process1_price_traces(</span>
<span id="cb5-3"><a href="rlforfin.html#cb5-3" tabindex="-1"></a> start_price: <span class="bu">int</span>,</span>
<span id="cb5-4"><a href="rlforfin.html#cb5-4" tabindex="-1"></a>    level_param: <span class="bu">int</span>,</span>
<span id="cb5-5"><a href="rlforfin.html#cb5-5" tabindex="-1"></a>    alpha1: <span class="bu">float</span>,</span>
<span id="cb5-6"><a href="rlforfin.html#cb5-6" tabindex="-1"></a>    time_steps: <span class="bu">int</span>,</span>
<span id="cb5-7"><a href="rlforfin.html#cb5-7" tabindex="-1"></a>    num_traces: <span class="bu">int</span></span>
<span id="cb5-8"><a href="rlforfin.html#cb5-8" tabindex="-1"></a>) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb5-9"><a href="rlforfin.html#cb5-9" tabindex="-1"></a>    process <span class="op">=</span> Process1(level_param<span class="op">=</span>level_param, alpha1<span class="op">=</span>alpha1)</span>
<span id="cb5-10"><a href="rlforfin.html#cb5-10" tabindex="-1"></a>    start_state <span class="op">=</span> Process1.State(price<span class="op">=</span>start.price)</span>
<span id="cb5-11"><a href="rlforfin.html#cb5-11" tabindex="-1"></a>    <span class="cf">return</span> np.vstack([</span>
<span id="cb5-12"><a href="rlforfin.html#cb5-12" tabindex="-1"></a>        np.fromiter((s.price <span class="cf">for</span> s <span class="kw">in</span> itertools.islice(</span>
<span id="cb5-13"><a href="rlforfin.html#cb5-13" tabindex="-1"></a>         simulation(process, start_state),</span>
<span id="cb5-14"><a href="rlforfin.html#cb5-14" tabindex="-1"></a>            time_steps <span class="op">+</span> <span class="dv">1</span></span>
<span id="cb5-15"><a href="rlforfin.html#cb5-15" tabindex="-1"></a>     )), <span class="bu">float</span>) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_traces)])</span></code></pre></div></li>
<li><p><span class="math display">\[
P[X_{t+1}=X_t+1]=\begin{cases}
0.5(1-\alpha_2(X_t-X_{t-1}))&amp;\text{  if }t&gt;0\\
0.5&amp;\text{ if }t=0
\end{cases}
\]</span></p>
<p>其中<span class="math inline">\(\alpha_2\)</span>是一个“拉力强度”参数，取值在<span class="math inline">\([0,1]\)</span>之间。这里的直觉时下一步的移动的方向偏向于前一次移动的反方向。我们注意到如果依然按照前文建模则无法满足Markov性质，因为<span class="math inline">\(X_{t+1}\)</span>取值的概率不仅依赖于<span class="math inline">\(X_t,\)</span>还依赖于<span class="math inline">\(X_{t-1}.\)</span>不过我们可以在这里做一个小技巧，即创建一个扩展状态<span class="math inline">\(S_t\)</span>由一对<span class="math inline">\((X_t,X_{t-1})\)</span>组成。当<span class="math inline">\(t=0\)</span>时状态<span class="math inline">\(S_0\)</span>可以取值<span class="math inline">\((X_0,null)\)</span>,这里的null只是一个符号。通过将状态<span class="math inline">\(S_t\)</span>视为<span class="math inline">\((X_t,X_t-X_{t-1})\)</span>建模可以发现Markov性质得到了满足。
<span class="math display">\[
\begin{aligned}
&amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1}),...,(X_0,null)]\\
=&amp;P[(X_{t+1},X_{t+1}-X_t)|(X_t,X_t-X_{t-1})]\\
=&amp;0.5(1-\alpha_2(X_{t+1}-X_t)(X_t-X_{t-1}))
\end{aligned}
\]</span>
关于上面的式子deepseek给出了证明。</p>
<p>人们自然会想知道，为什么状态不单单由<span class="math inline">\(X_t - X_{t-1}\)</span> 组成——换句话说，为什么 <span class="math inline">\(X_t\)</span> 也需要作为状态的一部分。确实，单独知道<span class="math inline">\(X_t - X_{t-1}\)</span>可以完全确定 <span class="math inline">\(X_{t+1}-X_t\)</span>的概率。因此，如果我们将状态设定为在任意时间步 <span class="math inline">\(t\)</span>仅为 <span class="math inline">\(X_t - X_{t-1}\)</span>，那么我们确实会得到一个只有两个状态 +1 和 -1 的马尔可夫过程（它们之间的概率转移）。然而，这个简单的马尔可夫过程并不能通过查看时间<span class="math inline">\(t\)</span>的状态 <span class="math inline">\(X_t - X_{t-1}\)</span> 来告诉我们股票价格 <span class="math inline">\(X_t\)</span> 的值。在这个应用中，我们不仅关心马尔可夫状态转移概率，还关心从时间 <span class="math inline">\(t\)</span> 的状态中获取任意时间 <span class="math inline">\(t\)</span> 的股票价格信息。因此，我们将状态建模为对 <span class="math inline">\(( X_t, X_{t-1} )\)</span>。</p>
<p>请注意，如果我们将状态 <span class="math inline">\(S_t\)</span> 建模为整个股票价格历史 <span class="math inline">\(( X_0, X_1,..., X_t )，\)</span>那么马尔可夫性质将显然得到满足，将<span class="math inline">\(S_t\)</span>建模为对<span class="math inline">\((X_t,X_{t-1})\)</span>Markov性质也会得到满足。然而，我们选择 <span class="math inline">\(S_t := (X_t, X_t - X_{t-1})\)</span> 是“最简单”的内部表示。实际上，在整本书中，我们对各种过程建模状态的努力是确保马尔可夫性质，同时使用“最简单/最小”的状态表示。</p></li>
<li><p>Process3是Process2的扩展，其中下一个移动的概率不仅依赖于上一时刻的移动还依赖于过去所有的移动。具体来说，它依赖于过去上涨次数的数量记为<span class="math inline">\(U_t=\sum_{i=1}^t\max(X_i-X_{i-1},0)\)</span>,与过去下跌次数的数量，记为<span class="math inline">\(D_t=\sum_{i=1}^t\max(X_{i-1}-X_i,0)\)</span>之间的关系。表示为
<span class="math display">\[
P[X_{t+1}=X_t+1]=\begin{cases}
\frac{1}{1+(\frac{U_t+D_t}{D_t}-1)^{\alpha_3}}&amp;\text{ if }t&gt;0\\
0.5&amp;\text{ if }t=0
\end{cases}
\]</span>
其中<span class="math inline">\(\alpha_3\in\mathbb{R}_{\geq0}\)</span>是一个拉力强度参数，将上述概率表达式视为<span class="math inline">\(f(\frac{D_t}{U_t+D_t};\alpha_3)\)</span>其中<span class="math inline">\(f:[0,1]\rightarrow[0,1]\)</span>是一个sigmoid型函数
<span class="math display">\[
f(x;\alpha)=\frac{1}{1+(\frac{1}{x}-1)^{\alpha}}.
\]</span>
下一个上涨移动的概率基本依赖<span class="math inline">\(\frac{U_t}{U_t+D_t}\)</span>即过去时间步中下跌次数的比例。因此，如果历史上的下跌次数大于上涨次数，那么下一个价格移动<span class="math inline">\(X_{t+1}-X_t\)</span>将会有更多的向上拉力，反之亦然。</p>
<p>我们将<span class="math inline">\(S_t\)</span>建模为由对<span class="math inline">\((U_t,D_t)\)</span>组成，这样<span class="math inline">\(S_t\)</span>的Markov性质可以得到满足
<span class="math display">\[
\begin{aligned}
&amp;P[(U_{t+1},D_{t+1})|(U_t,D_t),...,(U_0,D_0)]=P[(U_{t+1},D_{t+1})|(U_t,D_t)]\\
&amp;=\begin{cases}
f(\frac{D_t}{U_t+D_t};\alpha_3)&amp;\text{ if }U_{t+1}=U_t+1,D_{t+1}=D_t\\
f(\frac{U_t}{U_t+D_t};\alpha_3)&amp;\text{ if }U_{t+1}=U_t,D_{t+1}=D_t+1
\end{cases}
\end{aligned}
\]</span>
重要的是与前面两个过程不同，股票价格<span class="math inline">\(X_t\)</span>实际上并不是过程3中状态<span class="math inline">\(S_t\)</span>的一部分，这是因为<span class="math inline">\(U_t,D_t\)</span>共同包含了捕捉<span class="math inline">\(X_t\)</span>的足够信息，因为<span class="math inline">\(X_t=X_0+U_t-D_t.\)</span></p></li>
</ol>
</div>
<div id="markov过程的正式定义" class="section level3 hasAnchor" number="1.1.3">
<h3><span class="header-section-number">1.1.3</span> Markov过程的正式定义<a href="rlforfin.html#markov过程的正式定义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>书中的定义和定理将由限制在离散时间和可数状态集合。</p>
<blockquote>
<p><strong>Def 3.3.1</strong></p>
<p>Markov过程由以下组成</p>
<ul>
<li>一个可数状态集合<span class="math inline">\(\mathcal{S}\)</span>（称为状态空间）和一个子集<span class="math inline">\(\mathcal{T}\subset \mathcal{S}\)</span>​（称为终止状态集合）。</li>
<li>一个时间索引的随即状态序列<span class="math inline">\(S_t\in S,\)</span>时间步为<span class="math inline">\(t=0,1,2,...\)</span>,每个状态转移都满足Markov性质:<span class="math inline">\(P[S_{t+1}|S_t,...,S_0]=P[S_{t+1}|S_t],\text{for all }t\geq0.\)</span></li>
<li>终止：如果某个时间步<span class="math inline">\(T\)</span>的结果<span class="math inline">\(S_T\)</span>是集合<span class="math inline">\(\mathcal{T}\)</span>中的一个状态，则该序列的结果在时间步<span class="math inline">\(T\)</span>终止。</li>
</ul>
<p>将<span class="math inline">\(P[S_{t+1}|S_t]\)</span>称为时间<span class="math inline">\(t\)</span>的转移概率。</p>
<p><strong>Def 3.3.2</strong></p>
<p>一个时间齐次Markov过程是一个Markov过程且<span class="math inline">\(P[S_{t+1}|S_t]\)</span>与<span class="math inline">\(t\)</span>无关。</p>
</blockquote>
<p>这意味着时间齐次Markov过程的动态可以通过下面的函数完全指定：
<span class="math display">\[
P:(\mathcal{S}-\mathcal{T})\times\mathcal{S}\rightarrow[0,1]
\]</span>
定义为<span class="math inline">\(P(s&#39;,s)=P[S_{t+1}=s&#39;|S_t=s]\)</span>使得<span class="math inline">\(\sum_{s&#39;\in S}P(s,s&#39;)=1,\text{for all}s\in\mathcal{S-T}.\)</span>​</p>
<p>注意上述规范中<span class="math inline">\(P\)</span>的参数没有时间索引<span class="math inline">\(t\)</span>（因此称为时间齐次）。此外注意到一个非时间齐次的Markov过程可以通过将所有状态和时间索引<span class="math inline">\(t\)</span>来结合转换为齐次Markov过程。这意味着如果一个非时间齐次的Markov过程的原始状态空间是<span class="math inline">\(\mathcal{S}\)</span>，那么对应的时间齐次Markov过程的状态空间是<span class="math inline">\(\mathbb{Z}_{\geq0}\times\mathcal{S}.\)</span></p>
</div>
<div id="markov过程的稳态分布" class="section level3 hasAnchor" number="1.1.4">
<h3><span class="header-section-number">1.1.4</span> Markov过程的稳态分布<a href="rlforfin.html#markov过程的稳态分布" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<blockquote>
<p><strong>Def 3.7.1</strong>
对于状态空间<span class="math inline">\(\mathcal{S}=\mathbb{N}\)</span>的离散、时间齐次的Markov过程及其转移概率函数<span class="math inline">\(P:\mathbb{N}\times\mathbb{N}\rightarrow [0,1]\)</span>,稳态分布是一个概率分布函数<span class="math inline">\(\pi:\mathbb{N}\rightarrow [0,1]\)</span>,满足
<span class="math display">\[
\pi(s&#39;)=\sum_{s\in\mathbb{N}}\pi(s)\cdot P(s,s&#39;),\text{ for all }s&#39;\in\mathbb{N}
\]</span></p>
</blockquote>
<p>稳态分布<span class="math inline">\(\pi\)</span>的直观理解是，在特定条件下如果我们让Markov过程无限运行，那么在长期内，状态在特定步出现频率（概率）由分布<span class="math inline">\(\pi\)</span>给出，该分布与时间步无关。</p>
<p>如果将稳态分布的定义专门化为有限状态、离散时间、时间齐次的Markov过程，状态空间为<span class="math inline">\(S=\{s_1,...,s_2\}=\mathbb{N},\)</span>那么我们可以将稳态分布<span class="math inline">\(\pi\)</span>​表示为
<span class="math display">\[
\pi(s_j)=\sum_{i=1}^n\pi(s_i)\cdot P(s_i,s_j),\text{ for al }j=1,2,...,n
\]</span>
下面使用粗体符号表示向量和矩阵。故<span class="math inline">\(\boldsymbol{\pi}\)</span>是一个长度为<span class="math inline">\(n\)</span>的列向量，<span class="math inline">\(\boldsymbol{\mathcal{P}}\)</span>是<span class="math inline">\(n\times n\)</span>的转移概率矩阵，其中行是原状态，列为目标状态，每行的和为1。那么上述定义的表述就可以简洁地表示为：
<span class="math display">\[
\boldsymbol{\pi}^T=\boldsymbol{\pi}^T\cdot\boldsymbol{\mathcal{P}},\text{ or }\boldsymbol{\mathcal{P}}^T\cdot\boldsymbol{\pi}=\boldsymbol{\pi}
\]</span>
后一个式子可以说明<span class="math inline">\(\boldsymbol{\pi}\)</span>是矩阵<span class="math inline">\(\boldsymbol{\mathcal{P}}\)</span>​的特征值为1对应的特征向量。</p>
</div>
<div id="markov奖励过程的形式主义" class="section level3 hasAnchor" number="1.1.5">
<h3><span class="header-section-number">1.1.5</span> Markov奖励过程的形式主义<a href="rlforfin.html#markov奖励过程的形式主义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>我们之所以讲述Markov过程是因为希望通过为Markov过程添加增量特性来逐步进入Markov决策过程，也就是强化学习的算法框架。现在开始讲述介于二者之间的中间框架即Markov奖励过程。基本上我们只是为每次从一个状态转移到下一个状态时引入一个数值奖励的概念。这些奖励是随机的，我们需要做的就是在进行状态转移时指定这些奖励的概率分布。</p>
<p>Markov奖励过程的主要目的是计算如果让过程无限运行（期望从每个非终止状态获得的奖励总和）我们将累积多少奖励，考虑到未来的奖励需要适当地折现。</p>
<blockquote>
<p><strong>Def 3.8.1</strong></p>
<p>Markov奖励过程是一个Markov过程以及一个时间索引序列的奖励随机变量<span class="math inline">\(R_t\in\mathcal{D},\mathcal{D}\)</span>是<span class="math inline">\(\mathbb{R}\)</span>中一个可数子集，<span class="math inline">\(t=1,2,...,\)</span>满足Markov性质：
<span class="math display">\[
P[(R_{t+1},S_{t+1})|S_{t},S_{t-1},...,S_0]=P[(R_{t+1},S_{t+1})|S_t]\text{ for all }t\geq0
\]</span></p>
</blockquote>
<p>我们将<span class="math inline">\(P[(R_{t+1},S_{t+1})|S_t]\)</span>称为Markov Reward Process在时间<span class="math inline">\(t\)</span>地转移概率。由于我们通常假设Markov的时间齐次性，我们将假设MRP具有时间齐次性，即<span class="math inline">\(P[(R_{t+1},S_{t+1})|S_t]\)</span>与<span class="math inline">\(t\)</span>​无关。</p>
由时间齐次性的假设，MRP的转移概率可以表示为转移概率函数
<span class="math display">\[
\mathcal{P}_R:\mathcal{N\times D\times S}\rightarrow[0,1]
\]</span>
定义为<br />
$$
<span class="math display">\[\begin{aligned}
&amp;\mathcal{P}_R(s,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|S_t=s]\text{ for }t=0,1,2,...,\\
&amp;\text{for all }s\in\mathcal{N},r\in\mathcal{D},s&#39;\in\mathcal{S},\text{ s.t. }\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)=1,\text{ for all }s\in \mathcal{N}

\end{aligned}\]</span>
<p>$$
当涉及模拟时我们需要单独指定起始状态的概率分布。</p>
<p>现在可以扩展更多理论。给定奖励转移函数<span class="math inline">\(\mathcal{P}_R\)</span>，我们可以得到</p>
<ul>
<li><p>隐式Markov过程的概率转移函数<span class="math inline">\(P:\mathbb{N}\times S\rightarrow [0,1]\)</span>可以定义为<br />
<span class="math display">\[
\mathcal{P}(s,s&#39;)=\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)
\]</span></p></li>
<li><p>奖励转移函数<span class="math inline">\(\mathcal{R}_T:\mathcal{N\times S}\rightarrow \mathbb{R}\)</span>定义为<br />
<span class="math display">\[
\mathcal{R}_T(s,s&#39;)=\mathbb{E}[R_{t+1}|S_{t+1}=s&#39;,S_t=s]=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,r,s&#39;)}{\mathcal{P}(s,s&#39;)}=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,r,s&#39;)}{\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)}\cdot r
\]</span></p></li>
</ul>
<p>我们在实践中遇到的大多数MRP奖励规范可以直接表示为奖励转移函数<span class="math inline">\(\mathcal{R}_T\)</span>.最后我们想强调的是，可以将<span class="math inline">\(\mathcal{P}_R\)</span>或<span class="math inline">\(\mathcal{R}_T\)</span>转换为一种更紧凑的奖励函数。该函数足以执行涉及MRP的关键计算，这个奖励函数<span class="math inline">\(\mathcal{R}:\mathcal{N}\rightarrow \mathbb{R}\)</span>定义为<br />
<span class="math display">\[
\mathcal{R}(s)=\mathbb{E}[R_{t+1}|S_t=s]=\sum_{s&#39;\in\mathcal{S}}\mathcal{P}(s,s&#39;)\cdot\mathcal{R}_T(s,s&#39;)=\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,r,s&#39;)\cdot r
\]</span></p>
</div>
<div id="markov奖励过程的价值函数" class="section level3 hasAnchor" number="1.1.6">
<h3><span class="header-section-number">1.1.6</span> Markov奖励过程的价值函数<a href="rlforfin.html#markov奖励过程的价值函数" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在，我们准备正式定义涉及MRP的主要问题，我们希望计算从任何非终止状态出发的“期望累积奖励”。允许在奖励累积时使用贴现因子<span class="math inline">\(\gamma\in[0,1]\)</span>,我们将回报<span class="math inline">\(G_t\)</span>定义为时间<span class="math inline">\(t\)</span>之后的“未来奖励的贴现累积”。形式上：<br />
<span class="math display">\[
G_t=\sum_{i=t+1}^\infty \gamma^{i-t-1}\cdot R_i=R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2\cdot R_{t+3}+....
\]</span>
即使对于终止序列（例如<span class="math inline">\(t=T\)</span>时终止，即<span class="math inline">\(S_T\in\mathcal{T}\)</span>）我们只需将<span class="math inline">\(i&gt;T\)</span>的<span class="math inline">\(R_i=0\)</span>.<br />
我们希望识别具有较大期望回报的非终止状态和具有较小期望回报的非终止状态。事实上，这是涉及MRP的主要问题——计算MRP中每个非终止状态的期望回报。形式上，我们感兴趣的是计算价值函数：<br />
<span class="math display">\[
V:\mathcal{N}\rightarrow\mathbb{R}
\]</span>
定义为<br />
<span class="math display">\[
V(s)=\mathbb{E}[G_t|S_t=s]\text{ for all }s\in\mathcal{N},\text{ for all }t=0,1,2,...
\]</span>
贝尔曼指出价值函数具有递归结构，具体来说</p>
<p><span class="math display" id="eq:bellman">\[\begin{align}
V(s)=&amp;\mathbb{E}[R_{t+1}|S_t=s]+\gamma\cdot\mathbb{E}[R_{t+2}|S_t=s]+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\cdot\mathbb{E}[R_{t+2}|S_{t+1}=s&#39;]\\
&amp;+\gamma^2\sum_{s&#39;\in\mathcal{N}}P[S_{t+1}=s&#39;|S_t=s]\sum_{s&#39;&#39;\in\mathcal{N}}P[S_{t+2}=s&#39;&#39;|S_{t+1}=s&#39;]\cdot\mathbb{E}[R_{t+3}|S_{t+2}=s&#39;&#39;]\\
&amp;+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot\mathcal{R}(s&#39;)+\gamma^2\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,s&#39;&#39;)\mathcal{R}(s&#39;&#39;)+...\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot(R(s&#39;)+\gamma\cdot\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,s&#39;&#39;)\cdot\mathcal{R}(s&#39;&#39;)+...)\\
=&amp;\mathcal{R}(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,s&#39;)\cdot V(s&#39;) \text{ for all }s\in\mathcal{N} \tag{1.1}
\end{align}\]</span></p>
<p><a href="mailto:我们将这个价值函数的递归方程@ref" class="email">我们将这个价值函数的递归方程@ref</a>(eq:bellman)称为<strong>Markov奖励过程的Bellman方程</strong>。</p>
</div>
</div>
<div id="MDP" class="section level2 hasAnchor" number="1.2">
<h2><span class="header-section-number">1.2</span> Markov决策过程<a href="rlforfin.html#MDP" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="不确定性下的序列决策难题" class="section level3 hasAnchor" number="1.2.1">
<h3><span class="header-section-number">1.2.1</span> 不确定性下的序列决策难题<a href="rlforfin.html#不确定性下的序列决策难题" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>通常，MDP具有两个截然不同且相互依赖的高级特征<br />
1. 在每个时间步<span class="math inline">\(t,\)</span>观察到状态<span class="math inline">\(S_t\)</span>后，从指定的动作集合中选择一个动作<span class="math inline">\(A_t.\)</span>
2. 给定观察到的状态<span class="math inline">\(S_t\)</span>和执行的动作<span class="math inline">\(A_t,\)</span>下一个时间步的状态<span class="math inline">\(S_{t+1}\)</span>和奖励<span class="math inline">\(R_{t+1}\)</span>的概率通常不仅取决于状态<span class="math inline">\(S_t\)</span>还取决于动作<span class="math inline">\(A_t.\)</span><br />
我们的任务是最大化每个状态的期望回报（即最大化价值函数）。在一般情况下，这似乎是一个非常困难的问题，因为存在循环的相互作用。一方面，动作依赖于状态；另一方面，下一个状态/奖励的概率依赖于动作和状态。此外，动作可能会对奖励产生延迟影响，如何区分不同时间步的动作对未来奖励的影响也是一个挑战。如果没有动作和奖励之间的直接对应关系，我们如何控制动作以最大化期望累积奖励？为了回答这个问题，我们需要建立一些符号和理论。在我们正式定义马尔可夫决策过程框架及其相关（优雅的）理论之前，让我们先设定一些术语。</p>
<p><strong>人工智能视角下的MDP</strong><br />
使用人工智能的语言，我们说在每个时间步<span class="math inline">\(t,\)</span><strong>智能体（Agent）</strong>（我们设计的算法）观察到状态<span class="math inline">\(S_t,\)</span>然后智能体执行动作<span class="math inline">\(A_t,\)</span>之后环境（Environment）（在看到<span class="math inline">\(S_t,A_t\)</span>后）生成一个随机对<span class="math inline">\((S_{t+1},R_{t+1})\)</span>。接着，智能体观察到下一个状态<span class="math inline">\(S_{t+1}\)</span>，循环重复直到达到终止状态。这种循环相互作用如图<a href="rlforfin.html#fig:mdp">1.1</a>所示</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="rlforfin.html#cb6-1" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">include_graphics</span>(<span class="st">&quot;https://Go9entle.github.io/picx-images-hosting/image.3yekzoaheh.webp&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:mdp"></span>
<img src="https://Go9entle.github.io/picx-images-hosting/image.3yekzoaheh.webp" alt="Markov Decision Process"  />
<p class="caption">
图1.1: Markov Decision Process
</p>
</div>
<div class="lemma">
<p><span id="lem:chf-pdf" class="lemma"><strong>Lemma 1.1  </strong></span>For any two random variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, they both have the same probability distribution if and only if</p>
<p><span class="math display">\[\varphi _{X_1}(t)=\varphi _{X_2}(t)\]</span></p>
</div>
</div>
<div id="markov决策过程的正式定义" class="section level3 hasAnchor" number="1.2.2">
<h3><span class="header-section-number">1.2.2</span> Markov决策过程的正式定义<a href="rlforfin.html#markov决策过程的正式定义" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>与Markov过程和Markov奖励过程的定义类似，为了便于阐述，以下Markov决策过程的定义和理论将针对离散时间、可数状态空间和可数的下一个状态与奖励转移对。</p>
<div class="theorem">
<p><span id="thm:unlabeled-div-1" class="theorem"><strong>定理1.1  </strong></span>Markov决策过程包括以下内容：<br />
- 可数状态集合<span class="math inline">\(\mathcal{S}\)</span>（称为状态空间），终止状态集合<span class="math inline">\(\mathcal{T}\subset \mathcal{S}\)</span>,以及可数动作集合<span class="math inline">\(\mathcal{A}\)</span>（称为动作空间）。
- 时间索引的环境生成随机状态序列<span class="math inline">\(S_t\in\mathcal{S}\)</span>（时间步<span class="math inline">\(t=0,1,2,...\)</span>），时间索引的环境生成奖励随机变量序列<span class="math inline">\(R_t\in\mathcal{D}\)</span>（<span class="math inline">\(\mathcal{D}\)</span>是<span class="math inline">\(\mathbb{R}\)</span>的可数子集），以及时间索引的智能体可控动作序列<span class="math inline">\(A_t\in\mathcal{A}\)</span><br />
- Markov性<br />
<span class="math display">\[\begin{equation*}
  P[(R_{t+1},S_{t+1})|(S_t,A_t,S_{t-1},A_{t-1},...,S_0,A_0)]=P[   (R_{t+1},S_{t+1})|(S_t,A_t)]\text{ for all }t\geq 0
  \end{equation*}\]</span>
- 终止：如果某个时间步<span class="math inline">\(T\)</span>的状态<span class="math inline">\(S_T\in\mathcal{T}\)</span>,则该序列结果在时间步<span class="math inline">\(T\)</span>终止。</p>
</div>
<p>在更一般的情况下，如果状态或奖励是不可数的，相同的概念仍然适用，只是数学形式需要更加详细和谨慎。具体来说，我们将使用积分代替求和，使用概率密度函数（用于连续概率分布）代替概率质量函数（用于离散概率分布）。为了符号的简洁性，更重要的是为了核心概念的理解（而不被繁重的数学形式分散注意力），我们选择默认使用离散时间、可数<span class="math inline">\(\mathcal{S}\)</span>、可数<span class="math inline">\(\mathcal{A}\)</span>和可数<span class="math inline">\(\mathcal{D}\)</span>.</p>
<p>我们将<span class="math inline">\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\)</span>称为Markov决策过程在时间<span class="math inline">\(t\)</span>的转移概率。</p>
<p>与Markov过程和MRP一样，我们默认Markov决策过程是时间其次的，即<span class="math inline">\(P[(R_{t+1},S_{t+1})|(S_t,A_t)]\)</span>与<span class="math inline">\(t\)</span>无关。这意味着Markov决策过程的转移概率在最一般情况下可以表示为<strong>状态-奖励转移概率函数</strong>：<br />
<span class="math display">\[
\mathcal{P}_R:\mathcal{N\times A\times D\times S}\rightarrow [0,1]
\]</span>
定义为<br />
<span class="math display">\[
\mathcal{P}_R(s,a,r,s&#39;)=P[(R_{t+1}=r,S_{t+1}=s&#39;)|(S_t=s,A_t=a)]
\]</span>
对于时间步<span class="math inline">\(t=0,1,2,...\)</span>，对于所有的<span class="math inline">\(s,s&#39;\in\mathcal{N},a\in\mathcal{A},r\in\mathcal{D}\)</span>满足<br />
<span class="math display">\[
\sum_{s&#39;\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)=1\text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
这又可以通过状态-奖励转移概率函数<span class="math inline">\(\mathcal{P}_R\)</span>来表征，给定<span class="math inline">\(\mathcal{P}_R\)</span>的规范，我们可以构造<br />
- 状态转移概率函数：<br />
<span class="math display">\[
    \mathcal{P}:\mathcal{N\times A\times S}\rightarrow [0,1]
  \]</span>
定义为<br />
<span class="math display">\[
  \mathcal{P}(s,a,s&#39;)=\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)
  \]</span></p>
<ul>
<li><p>奖励转移函数：<br />
<span class="math display">\[
\mathcal{R}_T:\mathcal{N\times A\times S}\rightarrow \mathbb{R}
\]</span>
定义为</p>
<p><span class="math display">\[\begin{align}
\mathcal{R}_T(s,a,s&#39;)&amp;=\mathbb{E}[R_{t+1}|(S_{t+1}=s&#39;,S_t=s,A_t=a)]\\
&amp;=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,a,r,s&#39;)}{\mathcal{P}(s,a,s&#39;)}\cdot r\\
&amp;=\sum_{r\in\mathcal{D}}\frac{\mathcal{P}_R(s,a,r,s&#39;)}{\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)}\cdot r

\end{align}\]</span></p></li>
</ul>
<p>在实践中，我们遇到的大多数Markov决策过程的奖励规范可以直接表示为奖励转移函数<span class="math inline">\(\mathcal{R}_T\)</span>而不是更一般的<span class="math inline">\(\mathcal{P}_R\)</span>.最后我们想强调的是可以将<span class="math inline">\(\mathcal{P}_R\)</span>或<span class="math inline">\(\mathcal{R}_T\)</span>转换为“更紧凑”的奖励函数，该函数足以执行设计MDP的关键计算，这个奖励函数为：<br />
<span class="math display">\[
\mathcal{R}:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为：<br />
<span class="math display">\[
\begin{aligned}
\mathcal{R}(s,a)&amp;=\mathbb{E}[R_{t+1}|(S_t=s,A_t=a)]\\
&amp;=\sum_{s\in\mathcal{S}}\mathcal{P}(s,a,s&#39;)\cdot\mathcal{R}_T(s,a,s&#39;)\\
&amp;=\sum_{s\in\mathcal{S}}\sum_{r\in\mathcal{D}}\mathcal{P}_R(s,a,r,s&#39;)\cdot r
\end{aligned}
\]</span></p>
</div>
<div id="策略" class="section level3 hasAnchor" number="1.2.3">
<h3><span class="header-section-number">1.2.3</span> 策略<a href="rlforfin.html#策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>理解了MDP的动态后，我们现在转向智能体动作的规范，即作为当前状态函数的动作选择。在一般情况下，我们假设智能体将根据当i请安状态<span class="math inline">\(S_t\)</span>的概率分布执行动作<span class="math inline">\(A_t\)</span>,我们将此函数称为策略（Policy）。<br />
形式上，策略是一个函数：<br />
<span class="math display">\[
\pi:\mathcal{N\times A}\rightarrow [0,1]
\]</span>
定义为:<br />
<span class="math display">\[
\pi(s,a)=P[A_t=a|S_t=s]\text{ for }t=0,1,2,...\text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
使得<br />
<span class="math display">\[
\sum_{a\in\mathcal{A}}\pi(s,a)=1\text{ for all }s\in\mathcal{N}
\]</span>
需要注意的是，上述定义假设策略是Markov的，即动作概率仅依赖于当前状态，而不依赖于历史状态。上述定义还假设策略是平稳的，即<span class="math inline">\(P[A_t=a|S_t=s]\)</span>在时间<span class="math inline">\(t\)</span>上是不变的。如果我们遇到策略需要依赖于时间<span class="math inline">\(t\)</span>的情况，我们可以简单地将<span class="math inline">\(t\)</span>包含在状态中，从而使策略变得平稳（尽管这会增加状态空间的规模，从而导致计算成本的增加）。<br />
当策略对每个状态的动作概率集中在单个动作上（即只要到达一个状态，动作是确定的）时，我们称之为<strong>确定性策略</strong>。形式上，确定性策略<span class="math inline">\(\pi_D:\mathcal{N}\rightarrow \mathcal{A}\)</span>具有以下性质：对所有的<span class="math inline">\(s\in\mathcal{N},\)</span></p>
<p><span class="math display">\[
\pi(s,\pi_D(s))=1\text{ and }\pi(s,a)=0, \text{ for all }a\ne \pi_D(s)
\]</span></p>
<p>我们将非确定性的策略称为<strong>随机策略</strong>（随机反映了智能体将根据<span class="math inline">\(\pi\)</span>指定的概率分布执行随机动作的事实）。</p>
</div>
<div id="markov决策过程策略-markov奖励过程" class="section level3 hasAnchor" number="1.2.4">
<h3><span class="header-section-number">1.2.4</span> [Markov决策过程，策略]:= Markov奖励过程<a href="rlforfin.html#markov决策过程策略-markov奖励过程" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>本节有一个重要的见解——如果我们用固定策略<span class="math inline">\(\pi\)</span>（通常是一个固定的随机策略，注意与确定性策略区分）评估MDP，我们会得到一个由MDP和策略<span class="math inline">\(\pi\)</span>共同隐含的MRP。我们可以用符号精确地澄清这一点，但首先MDP和MRP中存在一些符号冲突。我们使用<br />
- <span class="math inline">\(\mathcal{P}_R\)</span>表示MRP转移概率函数，同时也表示MDP的状态-奖励转移概率函数；
- <span class="math inline">\(\mathcal{P}\)</span>表示MRP中隐含的Markov过程的转移概率函数，同时也表示MDP的状态转移函数；
- <span class="math inline">\(\mathcal{R}_T\)</span>表示MRP的奖励转移函数，同时也表示MDP的奖励转移函数；
- <span class="math inline">\(\mathcal{R}\)</span>表示MRP的奖励函数，同时也表示MDP的奖励函数。</p>
<p>我们将在<span class="math inline">\(\pi\)</span>隐含的MRP的函数<span class="math inline">\(\mathcal{P}_R,\mathcal{P},\mathcal{R}_T,\mathcal{R}\)</span>加上上标<span class="math inline">\(\pi\)</span>以区分这些函数MDP和<span class="math inline">\(\pi\)</span>隐含的MRP中的使用。</p>
<p>假设我们给定一个固定策略<span class="math inline">\(\pi\)</span>和一个由其状态-奖励转移概率函数<span class="math inline">\(\mathcal{P}_R\)</span>指定的MDP，那么由MDP与策略<span class="math inline">\(\pi\)</span>评估隐含的MRP的转移概率函数<span class="math inline">\(\mathcal{P}_R^\pi\)</span>定义为：<br />
<span class="math display">\[
\mathcal{P}_R^\pi(s,r,s&#39;)=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{P}_R(s,a,r,s&#39;)
\]</span>
类似地有</p>
<p><span class="math display">\[\begin{align}
\mathcal{P}^\pi(s,s&#39;)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{P}(s,a,s&#39;)\\
\mathcal{R}_T^\pi(s,s&#39;)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}_T(s,a,s&#39;)\\
\mathcal{R}^\pi(s)&amp;=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)
\end{align}\]</span></p>
<p>因此，每当我们谈论用固定策略评估的 MDP时，你应该知道我们实际上是在谈论隐含的 MRP。</p>
</div>
<div id="固定策略下的mdp价值函数" class="section level3 hasAnchor" number="1.2.5">
<h3><span class="header-section-number">1.2.5</span> 固定策略下的MDP价值函数<a href="rlforfin.html#固定策略下的mdp价值函数" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在我们准备讨论用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数（也称为MDP预测问题，“预测”指的是该问题涉及在智能体遵循特定策略时预测未来期望回报）。与MRP情况类似，我们定义<br />
<span class="math display">\[
G_t=\sum_{i=t+1}^\infty \gamma^{i-t-1}\cdot R_i=R_{t+1}+\gamma \cdot R_{t+2}+\gamma^2\cdot R_{t+3}+....
\]</span>
其中<span class="math inline">\(\gamma\in[0,1]\)</span>是指定的贴现因子。即便对于终止序列我们也使用上述回报的定义。<br />
用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数为<br />
<span class="math display">\[
V^\pi:\mathcal{N}\rightarrow \mathbb{R}
\]</span>
定义为：<br />
<span class="math display">\[
V^\pi(s)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|S_t=s]\text{ for all }s\in\mathcal{N},\text{ for all }t=0,1,2,...
\]</span>
我们假设每当我们讨论价值函数时，折扣因子<span class="math inline">\(\gamma\)</span>是适当的，以确保每个状态的期望回报是有限的——特别是对于可能发散的连续（非终止）MDP，<span class="math inline">\(\gamma&lt;1\)</span>.<br />
我们将<span class="math inline">\(V^\pi(s)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|S_t=s]\)</span>展开如下：</p>
<p><span class="math display">\[\begin{align}
&amp;\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+1}|S_t=s]+\gamma\cdot\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+2}|S_t=s]+\gamma^2\cdot\mathbb{E}_{\pi,\mathcal{P}_R}[R_{t+3}|S_t=s]+...\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)+\gamma\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\cdot\mathcal{R}(s&#39;,a&#39;)\\
&amp;+\gamma^2\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a&#39;,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}(s&#39;,a&#39;&#39;,s&#39;&#39;)\sum_{a&#39;&#39;\in\mathcal{A}}\pi(s&#39;&#39;,a&#39;&#39;)\cdot\mathcal{R}(s&#39;&#39;,a&#39;&#39;)\\
&amp;+...\\
=&amp; \mathcal{R}^\pi(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\cdot\mathcal{R}^\pi(s&#39;)+\gamma^2\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\sum_{s&#39;&#39;\in\mathcal{N}}\mathcal{P}^\pi(s&#39;,s&#39;&#39;)\cdot\mathcal{R}^\pi(s&#39;&#39;)+...
\end{align}\]</span></p>
<p>最后一个表达式等于<span class="math inline">\(\pi\)</span>隐含的MRP的状态<span class="math inline">\(s\)</span>的价值函数。因此，用固定策略<span class="math inline">\(\pi\)</span>评估的MDP的价值函数<span class="math inline">\(V^\pi\)</span>与<span class="math inline">\(\pi\)</span>隐含的MRP的价值函数完全相同，因此我们可以将MRP的Bellman方程应用于<span class="math inline">\(V^\pi\)</span>，即</p>
<p><span class="math display" id="eq:4-1">\[\begin{align}
V^\pi(s)=&amp;\mathcal{R}^\pi(s)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}^\pi(s,s&#39;)\cdot V^\pi(s&#39;)\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot\mathcal{R}(s,a)+\gamma\cdot\sum_{a\in\mathcal{A}}\pi(s,a)\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;)\\
=&amp;\sum_{a\in\mathcal{A}}\pi(s,a)\cdot(\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;))\text{ for all }s\in\mathcal{N} \tag{1.2}
\end{align}\]</span></p>
<p>对于状态空间不太大的有限MDP，方程 <a href="rlforfin.html#eq:4-1">(1.2)</a> 可以通过线性代数求解<span class="math inline">\(V^\pi\)</span>。更一般地，方程 <a href="rlforfin.html#eq:4-1">(1.2)</a> 将成为本书其余部分开发各种动态规划和强化学习算法以解决MDP预测问题的关键方程。<br />
然而，另一个价值函数在开发MDP算法时也至关重要——它将（状态、动作）对映射到从该（状态，动作）对出发的期望回报，当用固定策略评估时。这被称为用固定策略评估的MDP地动作—价值函数：<br />
<span class="math display">\[
Q^\pi:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为：<br />
<span class="math display">\[
Q^\pi(s,a)=\mathbb{E}_{\pi,\mathcal{P}_R}[G_t|(S_t=s,A_t=a)]\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \text{ for all }t=0,1,2,...
\]</span>
为了避免术语混淆，我们将<span class="math inline">\(V^\pi\)</span>称为策略<span class="math inline">\(\pi\)</span>地<strong>状态-价值函数</strong>（尽管通常简称为价值函数），以区别于<strong>动作-价值函数</strong><span class="math inline">\(Q^\pi\)</span>.解释<span class="math inline">\(Q^\pi(s,a)\)</span>的方式是，它是从给定非终止状态<span class="math inline">\(s\)</span>出发，首先采取动作<span class="math inline">\(a\)</span>，然后遵循策略<span class="math inline">\(\pi\)</span>的期望回报。通过这种解释，我们可以将<span class="math inline">\(V^\pi(s)\)</span>视作<span class="math inline">\(Q^\pi(s,a)\)</span>的“加权平均”（对于所有从非终止状态<span class="math inline">\(s\)</span>出发的所有可能动作<span class="math inline">\(a\)</span>）,权重等于给定状态<span class="math inline">\(s\)</span>的动作<span class="math inline">\(a\)</span>的概率（即<span class="math inline">\(\pi(s,a)\)</span>）。具体来说：</p>
<p><span class="math display" id="eq:4-2">\[\begin{equation}
V^\pi(s)=\sum_{a\in\mathcal{A}}\pi(s,a)\cdot Q^\pi(s,a)\text{ for all }s\in\mathcal{N} \tag{1.3}
\end{equation}\]</span></p>
<p>将<span class="math inline">\(Q^\pi(s,a)\)</span>展开后得到</p>
<p><span class="math display" id="eq:4-3">\[\begin{equation}
Q^\pi(s,a)=\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^\pi(s&#39;) \text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.4}
\end{equation}\]</span></p>
<p>结合方程<a href="rlforfin.html#eq:4-2">(1.3)</a>和方程<a href="rlforfin.html#eq:4-3">(1.4)</a>我们得到</p>
<p><span class="math display" id="eq:4-4">\[\begin{equation}
Q^\pi(s,a)=\mathcal{R}(s,a)+\gamma \cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\sum_{a&#39;\in\mathcal{A}}\pi(s&#39;,a&#39;)\cdot Q^\pi(s&#39;,a&#39;)\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.5}
\end{equation}\]</span></p>
<p>方程<a href="rlforfin.html#eq:4-1">(1.2)</a>被称为<strong>MDP状态-价值函数贝尔曼策略方程</strong>，方程<a href="rlforfin.html#eq:4-4">(1.5)</a>被称为<strong>MDP动作-价值函数贝尔曼策略方程</strong>。方程<a href="rlforfin.html#eq:4-1">(1.2)</a>、<a href="rlforfin.html#eq:4-2">(1.3)</a>、<a href="rlforfin.html#eq:4-3">(1.4)</a>和<a href="rlforfin.html#eq:4-4">(1.5)</a>统称为<strong>MDP贝尔曼策略方程</strong>。</p>
</div>
<div id="最优价值函数和最优策略" class="section level3 hasAnchor" number="1.2.6">
<h3><span class="header-section-number">1.2.6</span> 最优价值函数和最优策略<a href="rlforfin.html#最优价值函数和最优策略" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>最后，我们要达到Markov决策问题的主要目的——识别能够产生最有价值函数的策略，即从每个非终止状态出发的最佳可能期望回报。我们说，当我们识别出MDP的最优价值函数（及相关的最优策略，即产生最优价值函数的策略）时，MDP就被“解决”了。识别最优价值函数及其相关最优策略被称为<strong>MDP控制问题</strong>。“控制”指的是该问题涉及通过策略的迭代修改来引导动作，以推动价值函数向最优性发展。<br />
形式上，最优价值函数定义为<br />
<span class="math display">\[
V^*:\mathcal{N}\rightarrow \mathbb{R}
\]</span>
定义为<br />
<span class="math display">\[
V^*(s)=\max_{\pi\in\Pi} V^\pi(s) \text{ for all }s\in\mathcal{N}
\]</span>
其中<span class="math inline">\(\Pi\)</span>是<span class="math inline">\(\mathcal{N,A}\)</span>空间上的平稳随机策略集合。<br />
上述定义的解释是，对于每个非终止状态<span class="math inline">\(s\)</span>，我们考虑所有可能的随机平稳策略并在这些<span class="math inline">\(\pi\)</span>中选择最大化<span class="math inline">\(V^\pi(s).\)</span>需要注意的是，<span class="math inline">\(\pi\)</span>的选择是针对每个<span class="math inline">\(s\)</span>单独进行的，因此可以想象，不同的<span class="math inline">\(\pi\)</span>的选择可能会为不同的<span class="math inline">\(s\in\mathcal{N}\)</span>最大化<span class="math inline">\(V^\pi(s).\)</span>因此，从上述<span class="math inline">\(V^*\)</span>的定义中，我们还不能谈论“最优策略”的概念。因此，现在让我们只关注上述定义的最有价值函数。<br />
同样，最优动作-价值函数定义为<br />
<span class="math display">\[
Q^*:\mathcal{N\times A}\rightarrow \mathbb{R}
\]</span>
定义为<br />
<span class="math display">\[
Q^*(s,a)=\max_{\pi\in\Pi} Q^\pi(s,a) \text{ for all }s\in\mathcal{N},a\in\mathcal{A}
\]</span>
<span class="math inline">\(V^*\)</span>通常被称为最优状态-价值函数，以区别于最优动作-价值函数<span class="math inline">\(Q^*\)</span>（尽管为了简洁，<span class="math inline">\(V^*\)</span>通常也被简称为最优价值函数）。需要明确的是，最优价值函数默认情况下值得就是最优状态-价值函数<span class="math inline">\(V^*\)</span>.</p>
<p>正如固定策略的价值函数具有递归公式一样，贝尔曼指出我们可以为最优价值函数创建递归公式。让我们从展开给定非终止状态<span class="math inline">\(s\)</span>的最优状态-价值函数<span class="math inline">\(V^*(s)\)</span>开始——我们考虑从状态<span class="math inline">\(s\)</span>出发可以采取的所有可能动作<span class="math inline">\(a\in\mathcal{A},\)</span>并选择能够产生最佳动作-价值的动作<span class="math inline">\(a,\)</span>即选择出能够产生最优<span class="math inline">\(Q^*(s,a)\)</span>的动作<span class="math inline">\(a\)</span>。形式上给出了以下方程<br />
<span class="math display" id="eq:4-5">\[
V^*(s)=\max_{a\in\mathcal{A}}Q^*(s,a)\text{ for all }s\in\mathcal{N} \tag{1.6}
\]</span>
同样让我们思考从给定非终止状态和动作对<span class="math inline">\((s,a)\)</span>出发的最优性意味着什么，也就是展开<span class="math inline">\(Q^*(s,a)\)</span>.首先我们获得即时的期望奖励<span class="math inline">\(\mathcal{R}(s,a)\)</span>.接下来，考虑所有可能的我们可以转到的状态<span class="math inline">\(s&#39;\in\mathcal{S}\)</span>并从每个非终止状态<span class="math inline">\(s&#39;\)</span>出发递归地采取最优动作。形式上，这给出了以下方程：<br />
<span class="math display" id="eq:4-6">\[
Q^*(s,a)=\mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^*(s&#39;)\text{ for all }s\in\mathcal{N},a\in\mathcal{A} \tag{1.7}
\]</span>
将<a href="rlforfin.html#eq:4-6">(1.7)</a>中的<span class="math inline">\(Q^*(s,a)\)</span>代入<a href="rlforfin.html#eq:4-5">(1.6)</a>,可以得到<br />
<span class="math display" id="eq:4-7">\[
V^*(s)=\max_{a\in\mathcal{A}}\{ \mathcal{R}(s,a)+\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot V^*(s&#39;) \} \text{ for all }s\in\mathcal{N} \tag{1.8}
\]</span>
方程<a href="rlforfin.html#eq:4-7">(1.8)</a>被称为<strong>MDP状态-价值函数贝尔曼最优性方程</strong>。<br />
将方程<a href="rlforfin.html#eq:4-5">(1.6)</a>代入方程<a href="rlforfin.html#eq:4-6">(1.7)</a>可以得到<br />
<span class="math display" id="eq:4-8">\[
Q^*(s,a)=\mathcal{R}(s,a)=\gamma\cdot\sum_{s&#39;\in\mathcal{N}}\mathcal{P}(s,a,s&#39;)\cdot \max_{a&#39;\in\mathcal{A}} Q^*(s&#39;,a&#39;)\text{ for all }s\in\mathcal{A},a\in\mathcal{A} \tag{1.9}
\]</span>
方程<a href="rlforfin.html#eq:4-8">(1.9)</a>被称为<strong>MDP动作-价值函数贝尔曼最优性方程</strong>。<br />
方程<a href="rlforfin.html#eq:4-7">(1.8)</a>、<a href="rlforfin.html#eq:4-5">(1.6)</a>、<a href="rlforfin.html#eq:4-6">(1.7)</a>和<a href="rlforfin.html#eq:4-8">(1.9)</a>统称为<strong>MDP贝尔曼最优性方程</strong>。我们应该强调，当有人说 MDP贝尔曼方程或简称为贝尔曼方程时，除非他们明确说明，否则他们指的是 MDP 贝尔曼最优性方程（通常是 MDP 状态-价值函数贝尔曼最优性方程）。这是因为 MDP 贝尔曼最优性方程解决了马尔可夫决策过程的最终目的——识别最优价值函数和实现最优价值函数的相关策略（即使我们能够解决 MDP 控制问题）。<br />
我们需要强调的是，贝尔曼最优性方程并没有直接给出计算最优质函数或实现最优质函数的策略的具体方法——它们只是阐述了最优值函数的一个强大数学性质，这一性质帮助我们提出动态规划或者强化学习的算法来计算最优值函数及其相关的策略。<br />
我们一直在使用“实现最优值函数的策略/策略组合”这个词，但我们还没有给出这样的策略的明确定义。事实上，正如之前提到的，从<span class="math inline">\(V^*\)</span>的定义来看并不清楚是否存在这样的策略能实现<span class="math inline">\(V^*\)</span>（因为可以设想不同的策略<span class="math inline">\(\pi\)</span>对于不同的状态<span class="math inline">\(s\in\mathcal{N}\)</span>实现<span class="math inline">\(V^\pi(s)\)</span>最大化）。因此我们定义最优策略<span class="math inline">\(\pi^*:\mathcal{N\times A}\rightarrow [0,1]\)</span>主导所有其他策略的策略，在价值函数上优于所有其他策略。形式化地说<br />
<span class="math display">\[
\pi^*\in\Pi\text{ is an Optimal Policy if } V^{\pi^*}(s)\geq V^\pi(s)\text{ for all }\pi\in\Pi \text{ and for all states }s\in\mathcal{N}.
\]</span>
最优策略<span class="math inline">\(\pi^*\)</span>的定义表明，它是一个“优于或等于”所有其他静态策略的策略，且适用于所有非终止状态（注意可能存在多个最优策略）。将这个定义与最优值函数<span class="math inline">\(V^*\)</span>的定义结合，接下来的自然问题是：是否存在一个最优策略<span class="math inline">\(\pi^*\)</span>,对所有的<span class="math inline">\(s\in\mathcal{N}\)</span>最大化<span class="math inline">\(V^\pi(s)\)</span>,也就是是否存在一个<span class="math inline">\(\pi^*\)</span>使得<span class="math inline">\(V^*(s)=V^{\pi^*}(s)\)</span>对所有<span class="math inline">\(s\in\mathcal{N}.\)</span>下面的定理和证明是针对我们默认的MDP设置（离散时间、可数空间、时间齐次）的。</p>
<div class="theorem">
<p><span id="thm:zuiyoucelue" class="theorem"><strong>定理1.2  </strong></span>对于任何（离散时间、可数空间、时间齐次）的MDP：</p>
<ul>
<li><p>存在一个最优策略<span class="math inline">\(\pi^*\in\Pi\)</span>.</p></li>
<li><p>所有最优策略都实现最优值函数。</p></li>
<li><p>所有最优策略实现最优动作-价值函数，即对于所有的<span class="math inline">\(s\in\mathcal{N},a\in\mathcal{A},Q^{\pi^*}(s,a)=Q^*(s,a)\)</span>对于所有的最优策略<span class="math inline">\(\pi^*\)</span>.</p></li>
</ul>
</div>
<p>定理的证明略去。</p>
<p>我们的确定性如下定义<br />
<span class="math display" id="eq:4-9">\[
\pi^*_D(s)=\mathop{\arg\max}_{a\in\mathcal{A}} Q^*(s,a)\text{ for all }s\in\mathcal{N} \tag{1.10}
\]</span>
方程<a href="rlforfin.html#eq:4-9">(1.10)</a>是一个关键构造，它与贝尔曼最优性方程紧密结合，在涉及各种动态规划和强化学习算法以解决MDP控制问题（即求解<span class="math inline">\(V^*,Q^*,\pi^*\)</span>）时起到重要作用。最后，值得注意的是，不像预测问题在小状态空间中有一个直接的线性代数求解器，控制问题是非线性的，因此没有类似的直接线性代数求解器。控制问题的最简单解法（即使是在小状态空间中）就是我们将在下一章中讨论的动态规划算法。</p>
</div>
</div>
<div id="dynamicprogram" class="section level2 hasAnchor" number="1.3">
<h2><span class="header-section-number">1.3</span> 动态规划算法<a href="rlforfin.html#dynamicprogram" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="dynassall" class="section level2 hasAnchor" number="1.4">
<h2><span class="header-section-number">1.4</span> 动态资产配置和消费<a href="rlforfin.html#dynassall" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="个人财务的优化" class="section level3 hasAnchor" number="1.4.1">
<h3><span class="header-section-number">1.4.1</span> 个人财务的优化<a href="rlforfin.html#个人财务的优化" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>个人财务对某些人来说可能非常简单（每月领取工资，花光所有工资）而对另一些人来说可能非常复杂（例如，那些在多个国家拥有多家企业并拥有复杂资产和负债的人）。在这里，我们将考虑一个相对简单但包含足够细节的情况，以便提供动态资产配置和消费问题的基本要素。假设你的个人财务包括以下几个方面：</p>
<ul>
<li><p>收入：这可能包括你的定期工资，通常在一段时间内保持不变，但如果你获得晋升或找到新工作，工资可能会发生变化。这也包括你从投资组合中变现的资金，例如如果你卖出一些股票并决定不再投资于其他资产。此外，还包括你从储蓄账户或某些债券中获得的利息。还有许多其他收入来源，有些是固定的定期支付，有些则在支付金额和时间上具有不确定性，我们不会一一列举所有不同的收入方式。我们只想强调，在生活中的各个时间点获得收入是个人财务的关键方面之一。</p></li>
<li><p>消费：这里的“消费”指的是“支出”。需要注意的是，人们需要定期消费以满足基本需求，如住房、食物和衣物。你支付的房租或按揭贷款就是一个例子——它可能是每月固定金额，但如果你的按揭利率是浮动的，它可能会有所变化。此外，如果你搬到新房子，房租或按揭可能会有所不同。你在食物和衣物上的支出也构成了消费。这通常每月相对稳定，但如果你有了新生儿，可能需要额外的支出用于婴儿的食物、衣物甚至玩具。此外，还有超出“必需品”的消费——比如周末在高级餐厅用餐、夏季度假、购买豪华汽车或昂贵的手表。人们从这种消费中获得“满足感”或“幸福感”（即效用）。这里的关键点是，我们需要定期决定每周或每月花多少钱（消费）。在动态决策中，人们面临着消费（带来消费效用）和储蓄（将钱投入投资组合，希望钱能增值，以便未来能够消费更多）之间的张力。</p></li>
<li><p>投资：假设你可以投资于多种资产——简单的储蓄账户提供少量利息、交易所交易的股票（从价值股到成长股，各自有不同的风险-收益权衡）、房地产（你购买并居住的房子确实被视为一种投资资产）、黄金等大宗商品、艺术品等。我们将投资于这些资产的资金组合称为投资组合（关于投资组合理论的简要介绍见附录B）。我们需要定期决定是否应该将大部分资金投入储蓄账户以保安全，还是应该将大部分投资资金分配于股票，或者是否应该更具投机性，投资于早期初创公司或稀有艺术品。审查投资组合的构成并可能重新分配资金（称为重新平衡投资组合）是动态资产配置的问题。还需要注意的是，我们可以将部分收入投入投资组合（意味着我们选择不立即消费这笔钱）。同样，我们可以从投资组合中提取部分资金用于消费。将资金投入或提取出投资组合的决策本质上是我们所做的动态消费决策，它与动态资产配置决策密切相关。</p></li>
</ul>
<p>以上描述希望为你提供了资产配置和消费的双重动态决策的基本概念。最终，我们的个人目标是在一生中最大化消费的期望总效用（可能还包括在你去世后为配偶和子女提供的消费效用）。由于投资组合本质上是随机的，并且我们需要定期做出资产配置和消费决策，你可以看到这具备了随机控制问题的所有要素，因此可以建模为马尔可夫决策过程（尽管通常相当复杂，因为现实生活中的财务有许多细节）。以下是该MDP的粗略和非正式草图（请记住，我们将在本章后面为简化的情况形式化MDP）：</p>
<ul>
<li><p><strong>状态</strong>：状态通常可能非常复杂，但主要包括年龄（用于跟踪达到MDP时间范围的时间）、投资于每种资产的资金数量、所投资资产的估值，以及可能还包括其他方面，如工作/职业状况（用于预测未来工资的可能性）。</p></li>
<li><p><strong>动作</strong>：动作是双重的。首先，它是每个时间步骤中选择的投资金额向量（时间步骤是我们审查投资组合以重新分配资金的时间周期）。其次，它是选择消费的灵活/可选的资金数量（即超出我们承诺支付的固定支出，如房租）。</p></li>
<li><p><strong>奖励</strong>：奖励是我们视为灵活/可选的消费效用——它对应于动作的第二部分。</p></li>
<li><p><strong>模型</strong>：模型（给定当前状态和动作的下一个状态和奖励的概率）在大多数现实生活情况中可能相当复杂。最困难的方面是预测我们生活和职业中明天可能发生的事情（我们需要这种预测，因为它决定了我们未来获得收入、消费和投资的可能性）。此外，投资资产的不确定性运动也需要由我们的模型捕捉。</p></li>
</ul>
<p>现在，我们准备采用这个MDP的一个简单特例，它去除了许多现实世界的摩擦和复杂性，但仍保留了关键特征（特别是双重动态决策方面）。这个简单的特例是默顿投资组合问题（Merton 1969）的主题，他在1969年的一篇里程碑论文中提出并解决了这个问题。他公式的一个关键特征是时间是连续的，因此状态（基于资产价格）演化为连续时间随机过程，而动作（资产配置和消费）是连续进行的。我们在下一节中介绍他论文的重要部分。</p>
</div>
<div id="merton投资组合问题及其解决" class="section level3 hasAnchor" number="1.4.2">
<h3><span class="header-section-number">1.4.2</span> Merton投资组合问题及其解决<a href="rlforfin.html#merton投资组合问题及其解决" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>现在，我们描述默顿投资组合问题并推导其解析解，这是数学经济学中最优雅的解决方案之一。该解决方案的结构将为我们提供关于资产配置和消费决策如何不仅依赖于状态变量，还依赖于问题输入的深刻直觉。</p>
<p>我们将时间记为<span class="math inline">\(t\)</span>，并假设当前时间为<span class="math inline">\(t=0\)</span>.假设你刚刚退休并且你将再活<span class="math inline">\(T\)</span>年。因此，用上一节的语言来说，你余生将不会获得任何收入，除了从投资组合中提取资金的选项。再假设你还没有固定支出，如按揭贷款、订阅费等等，这意味着你所有的消费都是灵活/可选的，即你可以在任何时间点选择消费任何非负实数。以上所有假设都是不合理的（<span style="color: orange;">但如果在养老金的资产配置中，这是合理的！</span>），但有助于保持问题的简单性，以便于解析处理。<br />
我们将任何时间<span class="math inline">\(t\)</span>的财富（记为<span class="math inline">\(W_t\)</span>）定义为你的投资资产的总市场价值。请注意，由于没有外部的收入，并且所有消费都是可选的，<span class="math inline">\(W_t\)</span>就是你的净资产。假设有固定数量的<span class="math inline">\(n\)</span>个风险资产和1个无风险资产。如前所述，目标是通过在任何时间点的双重动作——资产配置和消费（消费等于在任何时间点从投资组合中提取的资金）——最大化你一生中消费的期望总效用。请注意，由于没有外部资金来源，并且所有从投资组合中提取的资金都会立即被消费，因此你永远不会向投资组合中添加资金。投资组合的增长只能来自投资组合中资产市场价值的增长。最后，我们假设消费效用函数是恒定相对风险厌恶（CRRA）的。<br />
为了便于阐述，我们将问题形式化，并针对<span class="math inline">\(n=1\)</span>（即只有1个风险资产）的情况推导出Merton的优美的解析解。该解可以直观地推广到<span class="math inline">\(n&gt;1\)</span>个风险资产的情况。<br />
由于我们在连续时间中进行操作，风险资产遵循一个随机过程<span class="math inline">\(S\)</span>,具体来说是一个几何布朗运动<br />
<span class="math display">\[
dS_t=\mu S_t+\sigma S_t dZ_t,
\]</span>
其中<span class="math inline">\(\mu\in\mathbb{R},\sigma\in\mathbb{R}_+\)</span>是固定常数（注意，对于<span class="math inline">\(n\)</span>个资产则分别为向量和矩阵）。<br />
无风险资产没有不确定性，并且在连续时间内有固定的增长率，因此在时间<span class="math inline">\(t\)</span>时无风险资产<span class="math inline">\(R_t\)</span>的估值由下式给出<br />
<span class="math display">\[
dR_t=rR_tdt,
\]</span>
其中<span class="math inline">\(r\in\mathbb{R}\)</span>是一个固定常数。我们将单位时间内财富的消费记为<span class="math inline">\(c(t,W_t)\geq0\)</span>，以明确消费决策通常取决于<span class="math inline">\(t,W_t.\)</span>将时间<span class="math inline">\(t\)</span>时分配给风险资产的财富比例记为<span class="math inline">\(\pi(t,W_t).\)</span>注意，<span class="math inline">\(c(t,W_t),\pi(t,W_t)\)</span>共同构成了时间<span class="math inline">\(t\)</span>时的决策（MDP动作）。为了保持简洁，将<span class="math inline">\(c(t,W_t),\pi(w,W_t)\)</span>分别写为<span class="math inline">\(c_t,\pi_t\)</span>,但请在整个推导过程中认识到两者都是时间<span class="math inline">\(t\)</span>和财富<span class="math inline">\(W_t\)</span>的函数。<br />
最后，我们假设消费的效用函数为<br />
<span class="math display">\[
U(x)=\frac{x^{1-\gamma}}{1-\gamma},
\]</span>
其中风险厌恶参数<span class="math inline">\(\gamma\neq1\)</span>.<span class="math inline">\(\gamma\)</span>是CRRA系数，等于<span class="math inline">\(\frac{-xU&#39;&#39;(x)}{U&#39;(x)}.\)</span>我们不会讨论<span class="math inline">\(\gamma=1\)</span>时的CRRA效用函数即<span class="math inline">\(U(x)=\log x\)</span>.<br />
由于我们假设没有向投资组合中添加资金，且没有买卖任何分数量的风险和无风险资产的交易成本，财富的时间演化应被概念化为分配比例<span class="math inline">\(\pi_t\)</span>的连续调整和从投资组合中的连续提取（等于连续消费<span class="math inline">\(c_t\)</span>）.因此从时间<span class="math inline">\(t\)</span>到<span class="math inline">\(t+dt\)</span>的财富变化<span class="math inline">\(dW_t\)</span>由下式给出：</p>
<p><span class="math display" id="eq:8-1">\[\begin{equation}
dW_t=((r+\pi_t\cdot (\mu-r))\cdot W_t-c_t)dt+\pi_t\sigma W_t dZ_t. \tag{1.11}
\end{equation}\]</span></p>
<p>这是一个确定财富随机演化的伊藤过程。<br />
我们的目标是确定在任何时间<span class="math inline">\(t\)</span>时的最优<span class="math inline">\((\pi(t,W_t),c(t,W_t))\)</span>,以最大化<br />
<span class="math display">\[
\mathbb{E}\left[\int_t^T\frac{e^{-\rho(s-t)}\cdot c_s^{1-\gamma}}{1-\gamma} ds+\frac{e^{-\rho (T-t)}\cdot B(T)\cdot W_T^{1-\gamma}}{1-\gamma}|W_t \right],
\]</span>
其中<span class="math inline">\(\rho\geq0\)</span>是效用贴现率，用于考虑未来消费效用可能低于当前效用的事实，<span class="math inline">\(B(\cdot)\)</span>被称为遗赠函数，可以视为你在时间<span class="math inline">\(T\)</span>去世时留给家人的钱。我们可以为任意遗赠函数<span class="math inline">\(B(T)\)</span>解决这个问题，但为了简单起见，我们考虑<span class="math inline">\(B(T)=\epsilon^\gamma,0&lt;\epsilon \ll 1\)</span>,意味着无遗赠。出于技术原因我们不将其设为0，这将在后面变得明显。<br />
我们应该将这个问题视为一个连续时间的随机控制问题，其中MDP定义如下：</p>
<ul>
<li>时间<span class="math inline">\(t\)</span>的状态为<span class="math inline">\((t,W_t)\)</span><br />
</li>
<li>时间<span class="math inline">\(t\)</span>的动作为<span class="math inline">\((\pi_t,c_t)\)</span><br />
</li>
<li>时间<span class="math inline">\(t&lt;T\)</span>时的单位时间奖励为：<br />
<span class="math display">\[
U(c_t)=\frac{c_t^{1-\gamma}}{1-\gamma}
\]</span>
在终端时刻<span class="math inline">\(T\)</span>的奖励为<br />
<span class="math display">\[
B(T)\cdot U(W_T)=\epsilon^\gamma\cdot \frac{W_T^{1-\gamma}}{1-\gamma}
\]</span>
时间<span class="math inline">\(t\)</span>时的回报是累积贴现奖励<br />
<span class="math display">\[
\int_t^T\frac{e^{-\rho(s-t)\cdot c_s^{1-\gamma}}}{1-\gamma} ds+\frac{e^{-\rho (T-t)}\cdot \epsilon^\gamma\cdot W_T^{1-\gamma}}{1-\gamma}
\]</span></li>
</ul>
<p>我们的目标是找到策略：<span class="math inline">\((t,W_t)\rightarrow (\pi_t,c_t)\)</span>,以最大化期望汇报。<br />
我们第一步是写出Hamilton-Jacobi-Bellman(HJB)方程，这是连续时间中的Bellman最优性方程的类比。我们将最优的价值函数记为<span class="math inline">\(V^*\)</span>,使得时间<span class="math inline">\(t\)</span>时财富<span class="math inline">\(W_t\)</span>的最优价值为<span class="math inline">\(V^*(t,W_t).\)</span>这里的HJB方程可以特化为下面的式子</p>
<p><span class="math display" id="eq:8-2">\[\begin{equation}
\max_{\pi_t,c_t}\left\{\mathbb{E}_t[dV^*(t,W_t)+\frac{c_t^{1-\gamma}}{1-\gamma}] dt\right\}=\rho V^*(t,W_t)dt \tag{1.12}
\end{equation}\]</span></p>
<p>现在对于<span class="math inline">\(dV^*\)</span>使用伊藤引理，移走<span class="math inline">\(dZ_t\)</span>的部分因为这是一个鞅，并且将等式两边都除以<span class="math inline">\(dt\)</span>,以产生任意<span class="math inline">\(0\leq t&lt;T\)</span>的HJB方程的偏微分形式<br />
<span class="math display" id="eq:8-3">\[
\max_{\pi_t,c_t}\left\{ \frac{\partial V^*}{\partial t}+\frac{\partial V^*}{\partial W_t} \cdot ((\pi_t(\mu-r)+r)W_t-c_t)+\frac{\partial^2 V^*}{\partial W_t^2}\cdot \frac{\pi_t^2\sigma^2W_t^2}{2}+\frac{c_t^{1-\gamma}}{1-\gamma}\right\}=\rho\cdot V^*(t,W_t)  \tag{1.13}
\]</span>
这个HJB方程由下面的终端条件<br />
<span class="math display">\[
V^*(T,W_T)=\epsilon^\gamma \cdot\frac{W_T^{1-\gamma}}{1-\gamma}
\]</span>-
可以将<a href="rlforfin.html#eq:8-3">(1.13)</a>写得更简洁一些：<br />
<span class="math display" id="eq:8-4">\[
\max_{\pi_t,c_t} \Phi(t,W_t;\pi_t,c_t)=\rho\cdot V^*(t,W_t) \tag{1.14}
\]</span>
需要强调的是，我们处理的约束条件是<span class="math inline">\(W_t&gt;0,c_t\geq0,0\leq t&lt;T.\)</span><br />
为了找到最优的<span class="math inline">\(\pi_t^*,c_t^*\)</span>，我们求得<span class="math inline">\(\Phi\)</span>的一阶条件得</p>
<p><span class="math display" id="eq:8-6" id="eq:8-5">\[\begin{equation}
\begin{split}
\pi_t^*&amp;=\frac{-\frac{\partial V^*}{\partial W_t}\cdot (\mu-r)}{\frac{\partial^2 V^*}{\partial W_t^*}\cdot\sigma^2\cdot W_t} \tag{1.15}\\
c_t^*&amp;=\left( \frac{\partial V^*}{\partial W_t} \right)^{-\frac{1}{\gamma}} \tag{1.16}
\end{split}
\end{equation}\]</span></p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="关于涛哥.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["CBook.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
